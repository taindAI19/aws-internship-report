[{"uri":"https://taindAI19.github.io/aws-internship-report/vi/5-workshop/5.3-architecture/5.3.1-agentcore-memory/","title":"Agent core memory","tags":[],"description":"","content":"Cấu hình Memory trong AgentCore Để tạo bộ nhớ cho AgentCore, thực hiện theo các bước dưới đây.\n1. Tạo Memory trong Bedrock Vào Bedrock → chọn AgentCore. Chuyển sang tab Memory. Nhấn Create memory. Trong giao diện tạo Memory, bạn sẽ thấy các phần sau:\nMemory name Đặt tên bộ nhớ mà AgentCore sẽ sử dụng.\nShort-term memory (raw event) expiration Số ngày hệ thống lưu lại lịch sử hội thoại chi tiết.\nDemo này có thể để mặc định 90 ngày.\n2. Các loại Memory trong AgentCore 1. Summarization – Tóm tắt hội thoại Chức năng: Tóm tắt nội dung hội thoại sau khi kết thúc hoặc theo chu kỳ.\nTác dụng: Giữ lại ngữ cảnh dài hạn mà không chiếm nhiều bộ nhớ.\nVí dụ:\nBạn chat 100 câu về lỗi AWS CLI → lần sau Agent nhớ:\n“Người dùng đang gặp lỗi kết nối AWS CLI.”\n2. Semantic Memory – Bộ nhớ ngữ nghĩa Chức năng: Lưu trữ các facts/kiến thức quan trọng, độc lập với ngữ cảnh.\nTác dụng: Dùng để trả lời các câu hỏi liên quan đến thông tin đã nói trước đó.\nVí dụ:\n“Dự án A dùng Python 3.9.”\nLần sau hỏi lại → Agent trả lời ngay Python 3.9.\n3. User Preferences – Sở thích người dùng Chức năng: Tự nhận biết thói quen và phong cách của người dùng.\nTác dụng: Cá nhân hóa cách phản hồi.\nVí dụ:\nBạn hay yêu cầu:\n“Hãy trả lời ngắn gọn.”\nAgent sẽ tự động trả lời đúng ý mỗi lần.\n4. Episodes – Bộ nhớ tình huống Chức năng: Lưu chuỗi sự kiện, phân tích thành công/thất bại qua Reflections.\nTác dụng: Giúp Agent học từ kinh nghiệm.\nVí dụ:\nLần trước đặt vé bị lỗi vì thiếu ngày tháng → Agent nhớ.\nLần sau nó sẽ hỏi ngày tháng trước.\n3. Loại Memory dùng trong Demo Trong phần demo, chỉ cần dùng loại:\nSummarization Chọn Summarization rồi nhấn Create để hoàn tất.\n4. Cập nhật Memory ID trong Python Sau khi tạo Memory, bạn sẽ nhận được Memory ID.\nThêm vào file Python như sau:\n# AgentCore Memory Configuration REGION = \u0026#34;ap-southeast-1\u0026#34; MEMORY_ID = \u0026#34;memory_j98zj-4LFDxqB2o1\u0026#34; GROQ_API_KEY = os.getenv(\u0026#34;GROQ_API_KEY\u0026#34;) lưu ý thay đổi Id Memory và Region theo đúng cái bạn đã tạo\n"},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/","title":"Báo cáo thực tập","tags":[],"description":"","content":"Báo cáo thực tập Thông tin sinh viên: Họ và tên: Nguyễn Đức Tài\nSố điện thoại: 0961287204\nEmail: taindse180365@fpt.edu.vn\nTrường: Đại Học FPT HCM\nNgành: Trí Tuệ Nhân Tạo\nLớp: AWS092025\nCông ty thực tập: Công ty TNHH Amazon Web Services Vietnam\nVị trí thực tập: GenAI Intern\nThời gian thực tập: Từ ngày 08/09/2025 đến ngày 09/12/2026\nNội dung báo cáo Worklog Proposal Các bài blogs đã dịch Các events đã tham gia Workshop Tự đánh giá Chia sẻ, đóng góp ý kiến "},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"AWS được xếp hạng Leader trong Gartner Magic Quadrant 2025 dành cho Nền tảng Ứng dụng Cloud-Native vàQuản lý Container Trong bối cảnh chuyển đổi số đang diễn ra mạnh mẽ, các nền tảng cloud-native và hệ sinh thái container trở thành hạ tầng lõi cho những doanh nghiệp muốn xây dựng và vận hành ứng dụng linh hoạt, hiện đại. Mới đây, theo báo cáo Gartner Magic Quadrant 2025, Amazon Web Services (AWS) tiếp tục giữ vị trí Leader trong hai lĩnh vực quan trọng:\nCloud-Native Application Platforms Container Management Điều này không chỉ phản ánh sự trưởng thành của hệ sinh thái dịch vụ AWS mà còn cho thấy hướng đi chiến lược trong việc hỗ trợ khách hàng xây dựng kiến trúc ứng dụng quy mô lớn, bảo mật và hiệu quả.\nBài viết này phân tích cách AWS đạt được vị trí Leader, những thay đổi đáng chú chú ý trong nền tảng dịch vụ, và cách chúng ảnh hưởng đến kiến trúc ứng dụng hiện đại.\nTổng quan kiến trúc và lý do AWS được xếp hạng Leader Từ góc nhìn kiến trúc cloud-native, điểm mạnh của AWS nằm ở độ phủ dịch vụ sâu rộng, mức độ tích hợp cao và hệ sinh thái hỗ trợ toàn diện cho vòng đời ứng dụng: xây dựng → triển khai → vận hành → tối ưu.\nGartner đánh giá AWS rất cao ở 2 tiêu chí:\nAbility to Execute (khả năng thực thi) Completeness of Vision (đầy đủ tầm nhìn) AWS đáp ứng tốt bởi vì:\nHệ sinh thái compute đa dạng: Lambda, App Runner, Amplify, Elastic Beanstalk Khả năng mở rộng tự động và ổn định Tích hợp AI/ML bản địa: Amazon Bedrock, SageMaker Triển khai container serverless linh hoạt: ECS, EKS với Fargate Mô hình hybrid \u0026amp; edge ngày càng hoàn thiện: EKS Hybrid Nodes, ECS Anywhere Hướng tiếp cận của AWS đối với cloud-native platforms Nền tảng cloud-native của AWS được xây dựng dựa trên các nguyên tắc:\nCung cấp managed runtime environments để tối ưu vận hành Hỗ trợ lifecycle đầy đủ cho ứng dụng Tích hợp sâu với AI/ML và serverless Triển khai đa dạng: từ monolith đến microservices và event-driven Cloud-native application platforms giúp doanh nghiệp rút ngắn thời gian đưa sản phẩm ra thị trường và giảm chi phí vận hành nhờ tự động hóa.\nDanh mục dịch vụ cốt lõi bao gồm: AWS Lambda – serverless compute AWS App Runner – triển khai web/app backend tự động AWS Amplify – fullstack cloud-native dành cho web/mobile AWS Elastic Beanstalk – PaaS truyền thống nhưng mạnh mẽ Các dịch vụ này có thể kết hợp linh hoạt dựa trên yêu cầu của doanh nghiệp.\nLựa chọn các dịch vụ compute: Phân tầng giao tiếp Kịch bản triển khai ứng dụng Dịch vụ tương ứng / mô hình cần xem xét Serverless toàn phần AWS Lambda, Amazon API Gateway, DynamoDB Web app đơn giản – cần tự động scaling AWS App Runner, AWS Amplify Ứng dụng đa tầng truyền thống AWS Elastic Beanstalk, Amazon EC2 Microservices Amazon ECS, Amazon EKS, AWS Fargate, Amazon EventBridge AI/ML inference \u0026amp; fine-tuning Amazon Bedrock, Amazon SageMaker Nhờ tính modul hóa, doanh nghiệp có thể bắt đầu nhỏ và mở rộng dần theo nhu cầu.\nContainer Management: Tại sao AWS vượt trội? AWS tiếp tục giữ vị trí Leader 3 năm liên tiếp trong nhóm Container Management nhờ:\n1. Đa dạng orchestration Amazon ECS – orchestrator bản địa, đơn giản, ổn định Amazon EKS – Kubernetes fully managed, hỗ trợ multi-AZ, multi-region 2. Serverless container với Fargate Không cần quản lý node, scaling tự động, tối ưu chi phí.\n3. Hybrid \u0026amp; Edge EKS Hybrid Nodes ECS Anywhere EKS Anywhere Cho phép chạy workloads tại on-prem, edge hoặc hybrid cloud thống nhất.\n4. Khả năng quan sát và quản trị Tích hợp sâu với:\nCloudWatch X-Ray AWS OpenSearch Managed Prometheus \u0026amp; Grafana Pub/Sub và event-driven trong môi trường AWS Tương tự kiến trúc event-driven trong form mẫu, AWS cung cấp các dịch vụ hỗ trợ mô hình pub/sub:\nAmazon SNS – pub/sub đơn giản Amazon EventBridge – event bus mạnh mẽ, routing thông minh SQS – hàng đợi decoupling Step Functions – orchestration workflow Ưu điểm:\nGiảm tải synchronous calls Hạn chế coupling giữa các microservices Đảm bảo khả năng mở rộng và độ ổn định Nhược điểm:\nCần giám sát event flow \u0026amp; xử lý message thất bại Phải đảm bảo idempotency khi có trùng message Tác động tới doanh nghiệp và đội ngũ kỹ thuật AWS giữ vị trí Leader không chỉ vì công nghệ mà còn bởi khả năng hỗ trợ khách hàng:\nVới doanh nghiệp: Rút ngắn thời gian ra mắt sản phẩm Giảm chi phí vận hành hạ tầng Linh hoạt mở rộng theo nhu cầu thực tế Tích hợp AI/ML dễ dàng Với đội kỹ thuật: Giảm cognitive load nhờ dịch vụ managed Tăng tốc độ thử nghiệm (experimentation) Dễ triển khai microservices \u0026amp; event-driven Hỗ trợ CI/CD toàn diện Kết luận Việc AWS tiếp tục dẫn đầu trong Gartner Magic Quadrant 2025 là minh chứng rõ ràng cho chiến lược tập trung vào đổi mới liên tục, đa dạng hóa dịch vụ, và đảm bảo trải nghiệm nhà phát triển tối ưu.\nDù bạn là startup hay doanh nghiệp lớn, việc áp dụng AWS cloud-native platforms và container services đều mang lại lợi thế cạnh tranh dài hạn, đặc biệt trong bối cảnh AI, hybrid cloud và serverless đang trở thành tiêu chuẩn mới.\nAWS không chỉ cung cấp công nghệ, mà còn là hệ sinh thái hoàn thiện giúp doanh nghiệp tăng tốc hành trình hiện đại hóa ứng dụng.\n"},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Tự động hóa xử lý dữ liệu với Amazon EMR: Điều phối bằng Step Functions \u0026amp; EventBridge Khi các pipeline dữ liệu ngày càng mở rộng, nhu cầu tự động hóa quy trình, giảm thao tác thủ công, và tối ưu chi phí compute trở nên quan trọng hơn bao giờ hết. Amazon EMR cung cấp môi trường mạnh mẽ để chạy các workload như Apache Spark, ETL, batch analytics và data enrichment. Tuy nhiên, trong nhiều trường hợp – đặc biệt trong các ngành đòi hỏi kiểm soát hạ tầng chặt chẽ như tài chính, y tế, chính phủ – việc vận hành các cụm EMR tạm thời (transient clusters) nếu không tự động hóa sẽ gây tốn kém và rủi ro vận hành.\nTrong blog này, tôi chia sẻ cách xây dựng pipeline Spark hoàn toàn tự động sử dụng EMR on EC2, được điều phối bởi AWS Step Functions và kích hoạt bởi Amazon EventBridge. Kiến trúc này minh họa cách chạy các job xử lý dữ liệu theo lịch, tối ưu chi phí và dễ quản trị.\nHướng dẫn kiến trúc Giải pháp sử dụng dataset COVID-19 public để mô phỏng pipeline xử lý dữ liệu định kỳ. Toàn bộ workflow được tách thành các thành phần độc lập, dễ thay thế, và dễ mở rộng.\nLuồng xử lý dữ liệu gồm 7 bước chính:\nDữ liệu CSV được lưu trong S3 (raw input). EventBridge trigger theo lịch, kích hoạt state machine. Step Functions tạo cụm EMR tạm thời trên EC2. PySpark job được submit để tính toán các chỉ số COVID-19 theo bang. Kết quả được ghi vào output S3. Cụm EMR tự động xóa sau khi job hoàn thành. Toàn bộ log được lưu vào S3 để theo dõi \u0026amp; troubleshooting. Kiến trúc này đặc biệt phù hợp với các workload Spark theo batch, yêu cầu kiểm soát chi tiết hạ tầng nhưng vẫn muốn tối ưu chi phí.\nĐặc điểm kiến trúc tự động hóa Khi làm việc với transient EMR clusters, ba yếu tố quan trọng được xem xét:\nNội tại: hiệu năng cụm Spark, kiểm soát tài nguyên, tối ưu chi phí EC2 Bên ngoài: trigger theo lịch, điều phối nhiều job pipeline Con người: giảm gánh nặng vận hành, hạn chế lỗi thủ công, audit rõ ràng Mô hình orchestration sử dụng Step Functions giúp theo dõi lifecycle rõ ràng – từ provisioning → spark submit → cleanup.\nLựa chọn công nghệ và phạm vi giao tiếp Thành phần Công nghệ sử dụng Provision \u0026amp; teardown cluster AWS Step Functions, Amazon EC2, Amazon EMR Scheduling Amazon EventBridge Scheduler Data storage \u0026amp; logs Amazon S3 Observability Amazon CloudWatch, S3 logs, EMR Application UIs IaC (triển khai hạ tầng) AWS CloudFormation The orchestration pipeline Mô hình kết nối các micro-components được xây dựng theo hướng event-driven:\nEventBridge: kích hoạt workflow. Step Functions: quản lý tuần tự các bước. EMR: thực thi workload Spark. S3: lưu input, output và log. Ưu điểm mô hình này:\nTách biệt compute (EMR) và logic điều phối (Step Functions) Chỉ trả tiền compute khi cluster đang chạy Quá trình xử lý minh bạch theo từng state Nhược điểm: cần quan sát chặt chẽ state machine \u0026amp; error handling.\nCore workflow components 1. Provisioning EMR cluster Step Functions khởi tạo cụm EMR on EC2 với cấu hình tối ưu cho workload. IAM roles được set theo nguyên tắc least privilege.\n2. Spark job execution Job PySpark được upload lên S3, và Step Functions gửi step submit đến EMR cluster.\nKhi chạy, script thực hiện:\nChuẩn hóa format dữ liệu Loại bỏ bản ghi lỗi Tính trung bình hàng tháng của: giường nội trú giường ICU tỷ lệ bệnh nhân COVID-19 Xuất file CSV theo timestamp vào S3 output 3. Automated teardown Sau khi job hoàn thành, cluster được tự động xóa để tiết kiệm chi phí.\nKhông còn lo chi phí EC2 phát sinh khi quên tắt cluster.\nThiết lập \u0026amp; vận hành pipeline CloudFormation Một template duy nhất triển khai toàn bộ hạ tầng:\nEMR roles S3 buckets Log bucket Step Functions state machine EventBridge schedule Triển khai thường hoàn tất trong 5 phút.\nTải và chuẩn bị dữ liệu Dataset từ healthdata.gov được đổi tên và upload vào raw/.\nPySpark script Script được upload vào thư mục scripts/ trên S3 và sử dụng khi submit job.\nScheduling với EventBridge Workflow có thể chạy:\nMột lần Theo chu kỳ (cron hoặc rate-based) Theo trigger nghiệp vụ Điều này biến pipeline Spark thành một phần của hệ thống ETL định kỳ, ví dụ:\nxử lý báo cáo cuối ngày tổng hợp dữ liệu theo tuần tính toán chỉ số y tế định kỳ Monitoring \u0026amp; observability Giám sát Step Functions Theo dõi từng state Bắt lỗi nhanh, xem input/output Thao tác trực quan Giám sát EMR Theo dõi step Spark Xem chi tiết job qua Spark UI, YARN Timeline Quan sát cluster-level metrics CloudWatch logs Nếu bật logging:\ndriver logs executor logs system metrics Đều được stream realtime.\nQuan sát đa lớp giúp rút ngắn thời gian troubleshooting, đặc biệt với workload Spark phức tạp.\nKết quả xử lý \u0026amp; kiểm tra Output file CSV được lưu vào thư mục\ns3:///processed//\nKết quả có thể dùng cho:\ndashboard analytics báo cáo tuân thủ data enrichment downstream Dọn dẹp tài nguyên Để tránh phát sinh chi phí:\nXóa S3 buckets (input/output/log) Xóa CloudFormation stack Kiểm tra lại EventBridge schedule để tránh trigger không mong muốn Kết luận Giải pháp sử dụng Amazon EMR + Step Functions + EventBridge cung cấp một cách tiếp cận hoàn toàn tự động, tối ưu chi phí, và giàu tính quan sát cho việc xử lý Spark on-demand.\nĐây là lựa chọn lý tưởng cho các doanh nghiệp muốn:\nTự động hóa pipeline ETL/batch analytics Giảm chi phí compute bằng mô hình cluster tạm thời Duy trì kiểm soát chi tiết về bảo mật và hạ tầng Đảm bảo audit, logging, và scheduling theo chuẩn enterprise Bạn có thể mở rộng giải pháp này bằng:\nthêm nhiều Spark jobs thêm workflow phức tạp hơn trong Step Functions tích hợp partitioned output với Glue Data Catalog kết hợp với Lake Formation để kiểm soát dữ liệu "},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Tăng tốc phát triển ứng dụng Spark trên Amazon EMR Với Data Solutions Framework (DSF) on AWS Trong bối cảnh doanh nghiệp ngày càng phụ thuộc vào xử lý dữ liệu lớn, việc phát triển và vận hành ứng dụng Apache Spark đòi hỏi một quy trình thống nhất, có thể lặp lại và dễ bảo trì. Tuy nhiên, nhiều nhóm kỹ thuật vẫn gặp khó khăn: cấu hình Spark không đồng nhất, mã nguồn phân tán, pipeline thiếu chuẩn hóa, hoặc workflow triển khai thiếu tự động.\nĐể giải quyết vấn đề này, AWS giới thiệu Data Solutions Framework (DSF) – một khung phát triển chuẩn hóa giúp đơn giản hóa quá trình xây dựng, kiểm thử, đóng gói và triển khai ứng dụng Spark trên Amazon EMR. Trong blog này, tôi trình bày cách DSF tăng tốc development lifecycle, chuẩn hóa pipeline Spark, và mang lại lợi ích rõ rệt cho đội ngũ data engineering.\nDSF là gì? Data Solutions Framework on AWS là một bộ công cụ chuẩn hóa cho phép bạn:\nPhát triển ứng dụng Spark theo mô hình modular Chuẩn hóa cấu trúc dự án Đóng gói dễ dàng để triển khai trên EMR Tích hợp với các dịch vụ như AWS Glue, EMR, S3 hỗ trợ CI/CD pipelines (GitHub Actions, CodePipeline) Hạn chế lỗi cấu hình Spark không đồng nhất DSF giúp các nhóm kỹ thuật không cần tự xây dựng từ đầu một khung chuẩn cho Spark, đồng thời tránh sự phân mảnh giữa các dự án.\nKiến trúc giải pháp với DSF DSF định nghĩa ba lớp chính:\nFramework Layer – cung cấp abstraction để đọc, ghi, xử lý, logging, config. Application Layer – nơi viết business logic Spark (ETL, transform…). Pipeline \u0026amp; Deployment Layer – CI/CD, packaging, orchestration và job submission. Luồng hoạt động tổng quát:\nDeveloper viết job Spark theo tiêu chuẩn DSF DSF CLI dùng để chạy local, test logic, và đóng gói Artifact được upload lên S3 EMR cluster thực thi job theo chuẩn DSF thông qua bootstrap hoặc step submit Monitoring \u0026amp; logging tích hợp qua CloudWatch và S3 Điểm mạnh: tách biệt hoàn toàn code nhiệm vụ (business logic) khỏi hạ tầng (EMR config).\nLợi ích khi sử dụng DSF 1. Chuẩn hóa cấu trúc dự án Các dự án Spark sẽ có một layout thống nhất:\nsrc/ main/ python/ resources/ conf/ tests/ scripts/\nĐiều này giúp onboarding developer dễ dàng hơn, giảm nhầm lẫn giữa các project.\n2. Quản lý cấu hình tập trung DSF hỗ trợ YAML-based config, bao gồm:\ninput/output paths desired Spark settings environment-specific overrides → Không còn tình trạng hardcode config trong code Spark.\n3. Debug \u0026amp; chạy local đơn giản Thông qua DSF CLI, bạn có thể chạy pipeline local – mô phỏng giống khi chạy trên EMR.\n4. Packaging tự động DSF tự động đóng gói application cùng dependencies → deploy thẳng lên EMR.\n5. Hỗ trợ orchestration DSF hoạt động tốt với:\nAWS Step Functions Amazon MWAA / Airflow Amazon EMR on EC2 hoặc EMR Serverless Công cụ hỗ trợ: DSF CLI DSF CLI cung cấp các lệnh:\ndsf init – tạo project Spark chuẩn hóa dsf run – chạy ứng dụng local/test dsf package – đóng gói \u0026amp; build dsf deploy – upload artifact lên S3 dsf generate – tạo file config hoặc template module CLI giúp đơn giản hóa workflow: Build → Test → Deploy.\nQuy trình phát triển với DSF Quy trình chuẩn khi phát triển ứng dụng Spark với DSF:\n1. Khởi tạo dự án Developer dùng DSF CLI để tạo project với đầy đủ module, cấu trúc chuẩn.\n2. Viết logic xử lý Spark Logic ETL hoặc transform được tách biệt theo module, dễ bảo trì.\n3. Thiết lập config YAML Bao gồm:\ndataset inputs schema Spark settings môi trường chạy (dev, staging, prod) 4. Chạy local để unit test DSF cho phép chạy local mà không cần EMR → tăng tốc development.\n5. Build \u0026amp; package Artifact chứa code + configs được build tự động.\n6. Deploy \u0026amp; orchestrate Artifact được đẩy lên S3, sau đó chạy trên EMR thông qua Step Functions/Airflow.\nTích hợp với Amazon EMR DSF hỗ trợ nhiều mô hình chạy Spark:\n1. EMR on EC2 Tính tùy biến cao Phù hợp workload nặng Chạy theo mô hình job flow hoặc step submit 2. EMR Serverless Không cần quản lý cluster Tự động scale Phù hợp workloads linh hoạt hoặc ad-hoc Cách DSF giúp EMR hiệu quả hơn: Unified config → dễ chạy nhiều môi trường Packaging nhất quán → giảm lỗi deployment Logging định chuẩn → debugging dễ hơn Dễ automation bằng Step Functions hoặc EventBridge CI/CD cho ứng dụng Spark dùng DSF DSF tích hợp sẵn cho pipeline:\nGitHub Actions AWS CodePipeline + CodeBuild Pipeline mẫu:\nDeveloper push code CodeBuild chạy unit test DSF package application Upload artifact lên S3 Trigger run trên EMR (tùy chọn) → Dễ dàng đảm bảo chất lượng \u0026amp; nhất quán khi deploy.\nLogging \u0026amp; Observability Khi chạy job Spark với DSF:\nLogs được gửi về Amazon CloudWatch Logs chi tiết driver/executor được lưu vào S3 Framework hỗ trợ standardized logging Điều này giúp forensic, audit và debug thuận tiện cho team dữ liệu.\nMở rộng kiến trúc Bạn có thể mở rộng DSF cho:\nPipeline nhiều bước: extract → transform → enrich → publish Lakehouse kết hợp Glue Catalog + Iceberg/Hudi Real-time ingestion với Kinesis/Managed Kafka Kết hợp với data governance của Lake Formation DSF tạo nền tảng thống nhất để mở rộng theo nhu cầu doanh nghiệp.\nKết luận Data Solutions Framework on AWS giúp chuẩn hóa toàn bộ vòng đời phát triển ứng dụng Spark, từ local development đến deployment production.\nTrong môi trường mà workloads dữ liệu ngày càng phức tạp, DSF giúp đội ngũ data engineering:\nGiảm chi phí phát triển Tránh sai sót do thiếu chuẩn hóa Nâng cao năng suất và tốc độ triển khai Đảm bảo tính nhất quán giữa các pipeline Spark Tích hợp dễ dàng với EMR, Step Functions, MWAA, và CI/CD DSF không chỉ đơn giản là một công cụ, mà là một best-practice framework giúp doanh nghiệp xây dựng nền tảng xử lý dữ liệu mạnh mẽ, linh hoạt và có thể mở rộng.\n"},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/3-blogstranslated/3.4-blog4/","title":"Blog 4","tags":[],"description":"","content":" ⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nBắt đầu với healthcare data lakes: Sử dụng microservices Các data lake có thể giúp các bệnh viện và cơ sở y tế chuyển dữ liệu thành những thông tin chi tiết về doanh nghiệp và duy trì hoạt động kinh doanh liên tục, đồng thời bảo vệ quyền riêng tư của bệnh nhân. Data lake là một kho lưu trữ tập trung, được quản lý và bảo mật để lưu trữ tất cả dữ liệu của bạn, cả ở dạng ban đầu và đã xử lý để phân tích. data lake cho phép bạn chia nhỏ các kho chứa dữ liệu và kết hợp các loại phân tích khác nhau để có được thông tin chi tiết và đưa ra các quyết định kinh doanh tốt hơn.\nBài đăng trên blog này là một phần của loạt bài lớn hơn về việc bắt đầu cài đặt data lake dành cho lĩnh vực y tế. Trong bài đăng blog cuối cùng của tôi trong loạt bài, “Bắt đầu với data lake dành cho lĩnh vực y tế: Đào sâu vào Amazon Cognito”, tôi tập trung vào các chi tiết cụ thể của việc sử dụng Amazon Cognito và Attribute Based Access Control (ABAC) để xác thực và ủy quyền người dùng trong giải pháp data lake y tế. Trong blog này, tôi trình bày chi tiết cách giải pháp đã phát triển ở cấp độ cơ bản, bao gồm các quyết định thiết kế mà tôi đã đưa ra và các tính năng bổ sung được sử dụng. Bạn có thể truy cập các code samples cho giải pháp tại Git repo này để tham khảo.\nHướng dẫn kiến trúc Thay đổi chính kể từ lần trình bày cuối cùng của kiến trúc tổng thể là việc tách dịch vụ đơn lẻ thành một tập hợp các dịch vụ nhỏ để cải thiện khả năng bảo trì và tính linh hoạt. Việc tích hợp một lượng lớn dữ liệu y tế khác nhau thường yêu cầu các trình kết nối chuyên biệt cho từng định dạng; bằng cách giữ chúng được đóng gói riêng biệt với microservices, chúng ta có thể thêm, xóa và sửa đổi từng trình kết nối mà không ảnh hưởng đến những kết nối khác. Các microservices được kết nối rời thông qua tin nhắn publish/subscribe tập trung trong cái mà tôi gọi là “pub/sub hub”.\nGiải pháp này đại diện cho những gì tôi sẽ coi là một lần lặp nước rút hợp lý khác từ last post của tôi. Phạm vi vẫn được giới hạn trong việc nhập và phân tích cú pháp đơn giản của các HL7v2 messages được định dạng theo Quy tắc mã hóa 7 (ER7) thông qua giao diện REST.\nKiến trúc giải pháp bây giờ như sau:\nHình 1. Kiến trúc tổng thể; những ô màu thể hiện những dịch vụ riêng biệt.\nMặc dù thuật ngữ microservices có một số sự mơ hồ cố hữu, một số đặc điểm là chung:\nChúng nhỏ, tự chủ, kết hợp rời rạc Có thể tái sử dụng, giao tiếp thông qua giao diện được xác định rõ Chuyên biệt để giải quyết một việc Thường được triển khai trong event-driven architecture Khi xác định vị trí tạo ranh giới giữa các microservices, cần cân nhắc:\nNội tại: công nghệ được sử dụng, hiệu suất, độ tin cậy, khả năng mở rộng Bên ngoài: chức năng phụ thuộc, tần suất thay đổi, khả năng tái sử dụng Con người: quyền sở hữu nhóm, quản lý cognitive load Lựa chọn công nghệ và phạm vi giao tiếp Phạm vi giao tiếp Các công nghệ / mô hình cần xem xét Trong một microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Giữa các microservices trong một dịch vụ AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Giữa các dịch vụ Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The pub/sub hub Việc sử dụng kiến trúc hub-and-spoke (hay message broker) hoạt động tốt với một số lượng nhỏ các microservices liên quan chặt chẽ.\nMỗi microservice chỉ phụ thuộc vào hub Kết nối giữa các microservice chỉ giới hạn ở nội dung của message được xuất Giảm số lượng synchronous calls vì pub/sub là push không đồng bộ một chiều Nhược điểm: cần phối hợp và giám sát để tránh microservice xử lý nhầm message.\nCore microservice Cung cấp dữ liệu nền tảng và lớp truyền thông, gồm:\nAmazon S3 bucket cho dữ liệu Amazon DynamoDB cho danh mục dữ liệu AWS Lambda để ghi message vào data lake và danh mục Amazon SNS topic làm hub Amazon S3 bucket cho artifacts như mã Lambda Chỉ cho phép truy cập ghi gián tiếp vào data lake qua hàm Lambda → đảm bảo nhất quán.\nFront door microservice Cung cấp API Gateway để tương tác REST bên ngoài Xác thực \u0026amp; ủy quyền dựa trên OIDC thông qua Amazon Cognito Cơ chế deduplication tự quản lý bằng DynamoDB thay vì SNS FIFO vì: SNS deduplication TTL chỉ 5 phút SNS FIFO yêu cầu SQS FIFO Chủ động báo cho sender biết message là bản sao Staging ER7 microservice Lambda “trigger” đăng ký với pub/sub hub, lọc message theo attribute Step Functions Express Workflow để chuyển ER7 → JSON Hai Lambda: Sửa format ER7 (newline, carriage return) Parsing logic Kết quả hoặc lỗi được đẩy lại vào pub/sub hub Tính năng mới trong giải pháp 1. AWS CloudFormation cross-stack references Ví dụ outputs trong core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/3-blogstranslated/3.5-blog5/","title":"Blog 5","tags":[],"description":"","content":" ⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nBắt đầu với healthcare data lakes: Sử dụng microservices Các data lake có thể giúp các bệnh viện và cơ sở y tế chuyển dữ liệu thành những thông tin chi tiết về doanh nghiệp và duy trì hoạt động kinh doanh liên tục, đồng thời bảo vệ quyền riêng tư của bệnh nhân. Data lake là một kho lưu trữ tập trung, được quản lý và bảo mật để lưu trữ tất cả dữ liệu của bạn, cả ở dạng ban đầu và đã xử lý để phân tích. data lake cho phép bạn chia nhỏ các kho chứa dữ liệu và kết hợp các loại phân tích khác nhau để có được thông tin chi tiết và đưa ra các quyết định kinh doanh tốt hơn.\nBài đăng trên blog này là một phần của loạt bài lớn hơn về việc bắt đầu cài đặt data lake dành cho lĩnh vực y tế. Trong bài đăng blog cuối cùng của tôi trong loạt bài, “Bắt đầu với data lake dành cho lĩnh vực y tế: Đào sâu vào Amazon Cognito”, tôi tập trung vào các chi tiết cụ thể của việc sử dụng Amazon Cognito và Attribute Based Access Control (ABAC) để xác thực và ủy quyền người dùng trong giải pháp data lake y tế. Trong blog này, tôi trình bày chi tiết cách giải pháp đã phát triển ở cấp độ cơ bản, bao gồm các quyết định thiết kế mà tôi đã đưa ra và các tính năng bổ sung được sử dụng. Bạn có thể truy cập các code samples cho giải pháp tại Git repo này để tham khảo.\nHướng dẫn kiến trúc Thay đổi chính kể từ lần trình bày cuối cùng của kiến trúc tổng thể là việc tách dịch vụ đơn lẻ thành một tập hợp các dịch vụ nhỏ để cải thiện khả năng bảo trì và tính linh hoạt. Việc tích hợp một lượng lớn dữ liệu y tế khác nhau thường yêu cầu các trình kết nối chuyên biệt cho từng định dạng; bằng cách giữ chúng được đóng gói riêng biệt với microservices, chúng ta có thể thêm, xóa và sửa đổi từng trình kết nối mà không ảnh hưởng đến những kết nối khác. Các microservices được kết nối rời thông qua tin nhắn publish/subscribe tập trung trong cái mà tôi gọi là “pub/sub hub”.\nGiải pháp này đại diện cho những gì tôi sẽ coi là một lần lặp nước rút hợp lý khác từ last post của tôi. Phạm vi vẫn được giới hạn trong việc nhập và phân tích cú pháp đơn giản của các HL7v2 messages được định dạng theo Quy tắc mã hóa 7 (ER7) thông qua giao diện REST.\nKiến trúc giải pháp bây giờ như sau:\nHình 1. Kiến trúc tổng thể; những ô màu thể hiện những dịch vụ riêng biệt.\nMặc dù thuật ngữ microservices có một số sự mơ hồ cố hữu, một số đặc điểm là chung:\nChúng nhỏ, tự chủ, kết hợp rời rạc Có thể tái sử dụng, giao tiếp thông qua giao diện được xác định rõ Chuyên biệt để giải quyết một việc Thường được triển khai trong event-driven architecture Khi xác định vị trí tạo ranh giới giữa các microservices, cần cân nhắc:\nNội tại: công nghệ được sử dụng, hiệu suất, độ tin cậy, khả năng mở rộng Bên ngoài: chức năng phụ thuộc, tần suất thay đổi, khả năng tái sử dụng Con người: quyền sở hữu nhóm, quản lý cognitive load Lựa chọn công nghệ và phạm vi giao tiếp Phạm vi giao tiếp Các công nghệ / mô hình cần xem xét Trong một microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Giữa các microservices trong một dịch vụ AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Giữa các dịch vụ Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The pub/sub hub Việc sử dụng kiến trúc hub-and-spoke (hay message broker) hoạt động tốt với một số lượng nhỏ các microservices liên quan chặt chẽ.\nMỗi microservice chỉ phụ thuộc vào hub Kết nối giữa các microservice chỉ giới hạn ở nội dung của message được xuất Giảm số lượng synchronous calls vì pub/sub là push không đồng bộ một chiều Nhược điểm: cần phối hợp và giám sát để tránh microservice xử lý nhầm message.\nCore microservice Cung cấp dữ liệu nền tảng và lớp truyền thông, gồm:\nAmazon S3 bucket cho dữ liệu Amazon DynamoDB cho danh mục dữ liệu AWS Lambda để ghi message vào data lake và danh mục Amazon SNS topic làm hub Amazon S3 bucket cho artifacts như mã Lambda Chỉ cho phép truy cập ghi gián tiếp vào data lake qua hàm Lambda → đảm bảo nhất quán.\nFront door microservice Cung cấp API Gateway để tương tác REST bên ngoài Xác thực \u0026amp; ủy quyền dựa trên OIDC thông qua Amazon Cognito Cơ chế deduplication tự quản lý bằng DynamoDB thay vì SNS FIFO vì: SNS deduplication TTL chỉ 5 phút SNS FIFO yêu cầu SQS FIFO Chủ động báo cho sender biết message là bản sao Staging ER7 microservice Lambda “trigger” đăng ký với pub/sub hub, lọc message theo attribute Step Functions Express Workflow để chuyển ER7 → JSON Hai Lambda: Sửa format ER7 (newline, carriage return) Parsing logic Kết quả hoặc lỗi được đẩy lại vào pub/sub hub Tính năng mới trong giải pháp 1. AWS CloudFormation cross-stack references Ví dụ outputs trong core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/3-blogstranslated/3.6-blog6/","title":"Blog 6","tags":[],"description":"","content":" ⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nBắt đầu với healthcare data lakes: Sử dụng microservices Các data lake có thể giúp các bệnh viện và cơ sở y tế chuyển dữ liệu thành những thông tin chi tiết về doanh nghiệp và duy trì hoạt động kinh doanh liên tục, đồng thời bảo vệ quyền riêng tư của bệnh nhân. Data lake là một kho lưu trữ tập trung, được quản lý và bảo mật để lưu trữ tất cả dữ liệu của bạn, cả ở dạng ban đầu và đã xử lý để phân tích. data lake cho phép bạn chia nhỏ các kho chứa dữ liệu và kết hợp các loại phân tích khác nhau để có được thông tin chi tiết và đưa ra các quyết định kinh doanh tốt hơn.\nBài đăng trên blog này là một phần của loạt bài lớn hơn về việc bắt đầu cài đặt data lake dành cho lĩnh vực y tế. Trong bài đăng blog cuối cùng của tôi trong loạt bài, “Bắt đầu với data lake dành cho lĩnh vực y tế: Đào sâu vào Amazon Cognito”, tôi tập trung vào các chi tiết cụ thể của việc sử dụng Amazon Cognito và Attribute Based Access Control (ABAC) để xác thực và ủy quyền người dùng trong giải pháp data lake y tế. Trong blog này, tôi trình bày chi tiết cách giải pháp đã phát triển ở cấp độ cơ bản, bao gồm các quyết định thiết kế mà tôi đã đưa ra và các tính năng bổ sung được sử dụng. Bạn có thể truy cập các code samples cho giải pháp tại Git repo này để tham khảo.\nHướng dẫn kiến trúc Thay đổi chính kể từ lần trình bày cuối cùng của kiến trúc tổng thể là việc tách dịch vụ đơn lẻ thành một tập hợp các dịch vụ nhỏ để cải thiện khả năng bảo trì và tính linh hoạt. Việc tích hợp một lượng lớn dữ liệu y tế khác nhau thường yêu cầu các trình kết nối chuyên biệt cho từng định dạng; bằng cách giữ chúng được đóng gói riêng biệt với microservices, chúng ta có thể thêm, xóa và sửa đổi từng trình kết nối mà không ảnh hưởng đến những kết nối khác. Các microservices được kết nối rời thông qua tin nhắn publish/subscribe tập trung trong cái mà tôi gọi là “pub/sub hub”.\nGiải pháp này đại diện cho những gì tôi sẽ coi là một lần lặp nước rút hợp lý khác từ last post của tôi. Phạm vi vẫn được giới hạn trong việc nhập và phân tích cú pháp đơn giản của các HL7v2 messages được định dạng theo Quy tắc mã hóa 7 (ER7) thông qua giao diện REST.\nKiến trúc giải pháp bây giờ như sau:\nHình 1. Kiến trúc tổng thể; những ô màu thể hiện những dịch vụ riêng biệt.\nMặc dù thuật ngữ microservices có một số sự mơ hồ cố hữu, một số đặc điểm là chung:\nChúng nhỏ, tự chủ, kết hợp rời rạc Có thể tái sử dụng, giao tiếp thông qua giao diện được xác định rõ Chuyên biệt để giải quyết một việc Thường được triển khai trong event-driven architecture Khi xác định vị trí tạo ranh giới giữa các microservices, cần cân nhắc:\nNội tại: công nghệ được sử dụng, hiệu suất, độ tin cậy, khả năng mở rộng Bên ngoài: chức năng phụ thuộc, tần suất thay đổi, khả năng tái sử dụng Con người: quyền sở hữu nhóm, quản lý cognitive load Lựa chọn công nghệ và phạm vi giao tiếp Phạm vi giao tiếp Các công nghệ / mô hình cần xem xét Trong một microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Giữa các microservices trong một dịch vụ AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Giữa các dịch vụ Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The pub/sub hub Việc sử dụng kiến trúc hub-and-spoke (hay message broker) hoạt động tốt với một số lượng nhỏ các microservices liên quan chặt chẽ.\nMỗi microservice chỉ phụ thuộc vào hub Kết nối giữa các microservice chỉ giới hạn ở nội dung của message được xuất Giảm số lượng synchronous calls vì pub/sub là push không đồng bộ một chiều Nhược điểm: cần phối hợp và giám sát để tránh microservice xử lý nhầm message.\nCore microservice Cung cấp dữ liệu nền tảng và lớp truyền thông, gồm:\nAmazon S3 bucket cho dữ liệu Amazon DynamoDB cho danh mục dữ liệu AWS Lambda để ghi message vào data lake và danh mục Amazon SNS topic làm hub Amazon S3 bucket cho artifacts như mã Lambda Chỉ cho phép truy cập ghi gián tiếp vào data lake qua hàm Lambda → đảm bảo nhất quán.\nFront door microservice Cung cấp API Gateway để tương tác REST bên ngoài Xác thực \u0026amp; ủy quyền dựa trên OIDC thông qua Amazon Cognito Cơ chế deduplication tự quản lý bằng DynamoDB thay vì SNS FIFO vì: SNS deduplication TTL chỉ 5 phút SNS FIFO yêu cầu SQS FIFO Chủ động báo cho sender biết message là bản sao Staging ER7 microservice Lambda “trigger” đăng ký với pub/sub hub, lọc message theo attribute Step Functions Express Workflow để chuyển ER7 → JSON Hai Lambda: Sửa format ER7 (newline, carriage return) Parsing logic Kết quả hoặc lỗi được đẩy lại vào pub/sub hub Tính năng mới trong giải pháp 1. AWS CloudFormation cross-stack references Ví dụ outputs trong core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/5-workshop/5.4-agent-core-run/5.4.1-run-agentcore/","title":"Cấu hình &amp; Deploy AgentCore","tags":[],"description":"","content":"Bắt đầu với AgentCore Configure Đầu tiên, bạn cần đẩy code từ máy local lên AWS AgentCore bằng lệnh:\nagentcore configure -e ./{ten_file_python.py} 1. Agent Name Nhập tên cho Agent của bạn.\n2. Configuration File Nhấn Enter để dùng file cấu hình mặc định pyproject.toml.\n3. Deployment Configuration Chọn 2 – Deploy bằng Docker để AgentCore tự tạo Docker image và quản lý dễ dàng hơn.\n4. Execution Role Để mặc định và cho AWS tự tạo IAM Role.\n5. ECR Repository Nhấn Enter để AWS tự tạo nơi lưu Docker image.\n6. Authorization Configuration Chọn No cho OAuth. Agent chỉ cho phép truy cập qua AWS IAM Access Key \u0026amp; Secret Key.\n7. Request Header Allowlist Nhấn Enter để dùng cấu hình mặc định.\nKết quả Hoàn tất bước này là bạn đã đẩy code lên AgentCore thành công.\nKhởi chạy Agent\nDùng lệnh sau để chạy Agent với API Key (dùng GROQ):\nagentcore launch \u0026ndash;env GROQ_API_KEY=your_api_key_here\nKhi terminal báo trạng thái Running, Agent của bạn đã hoạt động thành công.\n"},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Báo cáo tổng hợp: “Cloud Day AWS 2025 tại TP. Hồ Chí Minh” Mục tiêu của sự kiện Tham dự phiên khai mạc được truyền hình trực tiếp từ Hà Nội Cập nhật các bài phát biểu quan trọng và thông báo mang tính đột phá, dự báo xu hướng điện toán đám mây tại Việt Nam Generative AI: Tìm hiểu các thành tựu mới và các ứng dụng thực tế Phân tích dữ liệu: Học cách chuyển hóa dữ liệu thành giá trị kinh doanh Di trú \u0026amp; Hiện đại hóa: Nắm rõ lộ trình chuyển đổi công nghệ trên nền tảng đám mây Diễn giả nổi bật Eric Yeo – Tổng Giám đốc Quốc gia, AWS khu vực Việt Nam – Campuchia – Lào – Myanmar Jaime Valles – Phó Chủ tịch, Khối Kinh Doanh \u0026amp; Phát Triển Thương Mại APJ, AWS Jeff Johnson – Giám đốc Điều hành ASEAN, AWS Tiến sĩ Jens Lottner – CEO, Techcombank Trang Phùng – CEO, U2U Network Vũ Văn – Đồng sáng lập \u0026amp; CEO, ELSA Corp Nguyễn Hòa Bình – Chủ tịch Tập đoàn Nexttech Dieter Botha – CEO, Tymex Nguyễn Văn Hải – Giám đốc Kỹ thuật Phần mềm, Techcombank Nguyễn Thế Vinh – Đồng sáng lập \u0026amp; CTO, Ninety Eight Nguyễn Minh Ngân – Chuyên gia AI, OCB Nguyễn Mạnh Tuyên – Trưởng bộ phận Ứng dụng Dữ liệu, LPBank Securities Những điểm nhấn của sự kiện Vietnam Cloud Day 2025 – Mô hình Hybrid ấn tượng Mặc dù sự kiện chính diễn ra tại Hà Nội, phiên bản hybrid tại TP. Hồ Chí Minh đem đến trải nghiệm liền mạch và sống động. Người tham dự có thể giao lưu trực tiếp với cộng đồng cloud builders và các chuyên gia trong ngành ngay tại địa phương. Buổi livestream phiên toàn thể mang đến nhiều thông tin giá trị và những định hướng quan trọng cho tương lai công nghệ đám mây tại Việt Nam. Chủ đề trọng tâm \u0026amp; các phiên thảo luận Sự kiện tập trung vào việc xây dựng hệ thống theo hướng mô-đun, nơi mỗi chức năng hoạt động như một dịch vụ độc lập giao tiếp qua sự kiện (event). Ba lĩnh vực chính được nhấn mạnh:\nGenerative AI: Công nghệ mới nhất \u0026amp; ứng dụng cụ thể Data Analytics: Khai thác sức mạnh dữ liệu để ra quyết định Migration \u0026amp; Modernization: Các bước chuyển đổi lên đám mây hiệu quả Những lợi ích đáng giá Mở rộng kết nối: Tương tác với cộng đồng công nghệ sôi động tại TP. Hồ Chí Minh Học từ chuyên gia: Tiếp thu kiến thức thực tiễn từ các lãnh đạo đầu ngành Cập nhật xu thế: Nắm bắt sớm các thay đổi trong lĩnh vực cloud tại Việt Nam Những bài học quan trọng Tầm nhìn chiến lược từ Vietnam Cloud Day 2025 Đà tăng tốc của điện toán đám mây: Nhiều doanh nghiệp Việt đang mở rộng chuyển đổi số mạnh mẽ Định hướng từ Chính phủ: Bài phát biểu mở màn nhấn mạnh chiến lược “cloud-first” ở cấp quốc gia Câu chuyện khách hàng: Techcombank và U2U Network chia sẻ hành trình áp dụng AWS Vai trò lãnh đạo: Tọa đàm khẳng định tầm quan trọng của việc gắn kết GenAI với mục tiêu kinh doanh Phân tích kỹ thuật: Generative AI \u0026amp; Dữ liệu Nền tảng dữ liệu thống nhất: Cách mô hình hóa quy trình nghiệp vụ theo hướng hệ thống Lộ trình ứng dụng GenAI: Hướng triển khai các dịch vụ AI của AWS AI-DLC (AI-Driven Development Lifecycle): Khi nào nên sử dụng sync, async, pub/sub hoặc streaming Bảo mật: Tiêu chí lựa chọn VM, container hoặc serverless AI Agents: Mô hình hệ thống thông minh giúp gia tăng năng suất vượt trội Kiến trúc \u0026amp; Vận hành Thiết kế event-driven \u0026amp; modular: Giúp hệ thống linh hoạt, dễ mở rộng Tùy chọn compute: EC2 – containers – serverless tùy theo workload Khả năng mở rộng \u0026amp; giám sát: Xây dựng hệ thống logging – monitoring – cost control chuẩn doanh nghiệp Ứng dụng thực tế vào công việc Đánh giá ứng dụng hiện tại: Xác định workload có thể đưa lên AWS để tối ưu chi phí và hiệu suất Xây dựng nền tảng dữ liệu: Thiết kế pipeline ingestion – lưu trữ – xử lý Thử nghiệm GenAI: Bắt đầu với Bedrock hoặc SageMaker cho các POC nhanh Tăng cường bảo mật: Áp dụng IAM theo nguyên tắc least privilege Nâng cấp kỹ năng đội nhóm: Chú trọng học và áp dụng AI/ML trên AWS Trải nghiệm cá nhân tại sự kiện Đây là lần đầu tiên tôi được biết đến Eric Yeo, Giám đốc khu vực của AWS – một nhà lãnh đạo truyền cảm hứng mạnh mẽ.\nTham dự “Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders” thực sự mang lại góc nhìn sâu sắc về chiến lược hiện đại hóa hạ tầng, ứng dụng và dữ liệu.\nHọc hỏi từ các chuyên gia Những chia sẻ từ chuyên gia AWS và lãnh đạo công nghệ giúp tôi mở rộng tư duy và hiểu rõ hơn cách tiếp cận quy mô lớn. Các case study thực tế cho tôi cái nhìn rõ ràng về cách áp dụng Domain-Driven Design (DDD) và kiến trúc hướng sự kiện (Event-Driven). Trải nghiệm kỹ thuật thực hành Tham gia session Event Storming, chuyển đổi quy trình nghiệp vụ thành các sự kiện miền (domain events). Thực hành chia hệ thống thành microservices có bounded context rõ ràng. So sánh ưu/nhược điểm của giao tiếp đồng bộ – bất đồng bộ, pub/sub, point-to-point và streaming. Khai thác công cụ hiện đại Khám phá Amazon Q Developer, trợ lý AI hỗ trợ liền mạch vòng đời phát triển phần mềm. Học cách tự động hóa refactor và xây dựng kiến trúc serverless bằng AWS Lambda. Kết nối \u0026amp; trao đổi chuyên sâu Gặp gỡ các chuyên gia AWS và cộng đồng công nghệ, giúp củng cố “ngôn ngữ chung” giữa nghiệp vụ và kỹ thuật. Thảo luận mở cho thấy tầm quan trọng của việc đặt business-first trong mọi quyết định công nghệ. Những điều rút ra DDD + event-driven giúp hệ thống ổn định, dễ bảo trì và mở rộng bền vững. Hiện đại hóa hiệu quả cần lộ trình theo từng giai đoạn, kèm đánh giá ROI rõ ràng. Công cụ AI như Amazon Q Developer thực sự giúp tăng tốc độ phát triển và hiệu quả làm việc nhóm. Một số hình ảnh tại sự kiện Nhìn chung, sự kiện không chỉ mang lại kiến thức chuyên sâu mà còn giúp tôi định hình lại tư duy về thiết kế hệ thống, hiện đại hóa hạ tầng và nâng cao sự phối hợp hiệu quả giữa các đội ngũ kỹ thuật – nghiệp vụ.\n"},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Báo cáo sự kiện: AI-Driven Development Session với Amazon Q Developer \u0026amp; Kiro Mục tiêu sự kiện Hiểu rõ cách Generative AI đang thay đổi toàn bộ vòng đời phát triển phần mềm Tìm hiểu vai trò của Amazon Q Developer và Kiro trong việc tăng tốc năng suất và quy trình xây dựng ứng dụng Khám phá cách AI hỗ trợ trong các khía cạnh kiến trúc, lập trình, kiểm thử, triển khai và vận hành hệ thống Trực tiếp quan sát demo về phát triển phần mềm dựa trên AI trong các tình huống kỹ thuật thực tế Tối ưu hiệu suất công việc bằng cách tự động hóa các tác vụ lặp lại, giúp kỹ sư tập trung vào tư duy sáng tạo và giải quyết vấn đề Diễn giả chính Toàn Huỳnh – Giảng viên, AWS GenAI Builder Club Mỹ Nguyễn – Giảng viên, AWS GenAI Builder Club Ban điều phối Diễm My – Điều phối chương trình Đại Trương – Điều phối sự kiện Đinh Nguyễn – Điều phối vận hành Điểm nổi bật trong buổi học Toàn Huỳnh truyền đạt kiến thức với năng lượng tích cực và phong thái truyền cảm hứng, mang lại cảm giác rất gần gũi nhưng vẫn cực kỳ chuyên sâu. Mỹ Nguyễn hỗ trợ tận tình, giải đáp câu hỏi của người tham dự một cách chi tiết và có chiều sâu chuyên môn. Cuộc cách mạng trong phát triển phần mềm với Generative AI Generative AI đang mở ra một cách tiếp cận mới cho ngành phần mềm: từ cách học, cách phân tích yêu cầu, lên ý tưởng cho đến phát triển, kiểm thử và quản lý ứng dụng.\nBuổi session nhấn mạnh rằng AI cho phép:\nTự động hóa các tác vụ kỹ thuật lặp lại Tạo bản mẫu (prototype) nhanh hơn và rút ngắn chu kỳ phát triển Cải thiện chất lượng mã và bảo mật thông qua gợi ý \u0026amp; review của AI Chuyển trọng tâm của developer sang công việc có giá trị cao hơn như thiết kế và tư duy hệ thống AI trong toàn bộ vòng đời phát triển phần mềm (AI-DLC) Người tham dự được giới thiệu cách AI tích hợp xuyên suốt SDLC:\nThiết kế và xây dựng kiến trúc Sinh mã nguồn và tự động refactor Tạo test cases \u0026amp; kiểm thử tự động Triển khai thông qua pipeline DevOps Giám sát \u0026amp; bảo trì liên tục Qua Amazon Q Developer và Kiro, mọi người có cái nhìn rõ ràng hơn về việc AI đang nâng tầm khả năng của lập trình viên như thế nào.\nAgenda 2:00 PM – 2:15 PM | Khởi động \u0026amp; Giới thiệu\n2:15 PM – 3:30 PM | Tổng quan AI-Driven Development Lifecycle \u0026amp; Demo Amazon Q Developer\nGiảng viên: Toàn Huỳnh\n3:30 PM – 3:45 PM | Nghỉ giải lao\n3:45 PM – 4:30 PM | Demo Kiro\nGiảng viên: Mỹ Nguyễn\nNhững điều rút ra từ sự kiện Góc nhìn chiến lược về AI-Driven Development AI giúp tăng tốc phát triển phần mềm bằng cách tự động hóa phần lớn công việc lặp lại Nhóm kỹ thuật có thể tập trung nhiều hơn vào thiết kế kiến trúc và giải quyết bài toán cốt lõi Thời gian cho debugging, refactor và viết tài liệu được giảm đáng kể Doanh nghiệp áp dụng AI sớm sẽ có lợi thế cạnh tranh rõ rệt Kiến thức thực tiễn – Amazon Q Developer Sinh code chất lượng cao từ mô tả tự nhiên bằng tiếng Việt hoặc tiếng Anh Tự động sửa lỗi, tối ưu hàm, cải thiện hiệu năng Viết unit test, tài liệu và giải thích logic code Hỗ trợ DevOps: tạo pipeline CI/CD và tự động hóa quá trình triển khai Kiến thức thực nghiệm – Kiro Hỗ trợ thiết kế hệ thống \u0026amp; kiến trúc ngay từ bước đầu Gợi ý thời gian thực cho cấu trúc code, patterns và best-practices Giúp đội ngũ kỹ thuật duy trì chuẩn coding đồng nhất và dễ đọc hơn Ứng dụng AI vào công việc Tích hợp công cụ AI để giảm tải các công việc tốn thời gian Dùng Amazon Q Developer để thử nghiệm ý tưởng nhanh và cải thiện chất lượng code Sử dụng Kiro trong bước phân tích kiến trúc và lập kế hoạch hệ thống Khuyến khích đội ngũ kỹ thuật làm quen với AI để tăng tốc hiệu suất làm việc Bắt đầu tiếp cận mô hình AI-augmented SDLC để hiện đại hóa quy trình kỹ thuật Trải nghiệm sự kiện Buổi workshop mang lại cảm giác mới mẻ và đầy động lực, cho thấy tiềm năng to lớn của Generative AI trong việc định hình lại cách chúng ta phát triển phần mềm.\nHọc hỏi từ chuyên gia Nhận được nhiều chia sẻ giá trị từ anh Toàn Huỳnh và chị Mỹ Nguyễn về cách ứng dụng AI vào công việc hằng ngày Hiểu rõ AI không chỉ hỗ trợ viết code, mà còn tham gia vào kiến trúc và DevOps Demo trực quan Quan sát Amazon Q Developer tự động refactor, tạo tài liệu và sinh test chỉ trong vài giây Trải nghiệm Kiro hỗ trợ phân tích và đưa ra quyết định về thiết kế hệ thống Kết nối cộng đồng Gặp gỡ các thành viên của AWS GenAI Builder Club và trao đổi về ứng dụng AI trong công việc thật Thảo luận về những kịch bản sử dụng AI khả thi trong môi trường doanh nghiệp Bài học quan trọng AI giúp tăng tốc độ phát triển, nâng cao độ chính xác và cải thiện năng suất Lập trình viên trong thời đại mới cần chuyển sang vai trò người thiết kế hệ thống, không chỉ viết code Việc sớm làm chủ AI sẽ là lợi thế to lớn cho các nhóm kỹ thuật hiện đại Một số hình ảnh tại sự kiện Tổng thể, sự kiện không chỉ cung cấp kiến thức kỹ thuật mà còn giúp tôi thay đổi cách tư duy về thiết kế ứng dụng, hiện đại hóa hệ thống và phối hợp hiệu quả hơn giữa các team.\n"},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/5-workshop/5.1-workshop-overview/","title":"Giới thiệu","tags":[],"description":"","content":"Giới thiệu về WorkShop Workshop này hướng dẫn từng bước thiết lập IAM, AWS CLI, UV, Groq API, triển khai mã nguồn RAG tích hợp Groq LLM vào AWS AgentCore và cuối cùng là publish API qua AWS Gateway. Mục tiêu WorkShop \u0026ldquo;Cách gọi api\u0026rdquo; hiểu cách gọi api bên ngoài Aws Agent Core \u0026ldquo;Chunking\u0026rdquo; cách chunking chia dữ liệu cho Rag có thể lấy ra được một cách tối ưu \u0026ldquo;Thêm bộ nhớ cho Rag\u0026rdquo; tìm hiểu cách mà Agent Rag có thể nhớ được từng dữ liệu khi tương tác với người đùng \u0026ldquo;Deploy Aws Agent Core\u0026rdquo; tim hiểu cách mà triển khai được Aws Agent Core \u0026ldquo;Triển khai API\u0026rdquo; cách gọi Agent Core thông qua API "},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/5-workshop/5.4-agent-core-run/5.4.2-call-agentcore/","title":"Gọi AgentCore","tags":[],"description":"","content":"Demo đơn giản với AgentCore 1. Gửi câu hỏi đầu tiên Dùng lệnh:\nagentcore invoke \u0026#34;{\u0026#39;prompt\u0026#39;: \u0026#39;Tell me about roaming activations\u0026#39;}\u0026#34; Agent sẽ trả lời dựa trên dữ liệu bạn đã triển khai (database + logic trong code).\n2. Kiểm tra khả năng ghi nhớ giữa các lần gọi (session) Sau khi hỏi lần đầu, bạn tiếp tục hỏi một câu có liên quan \u0026mdash; ví dụ\nagentcore invoke \u0026#34;{\u0026#39;prompt\u0026#39;: \u0026#39;Activate it for Vietnam\u0026#39;}\u0026#34; agentcore invoke \u0026#34;{\u0026#39;prompt\u0026#39;: \u0026#39;which country was i referring to\u0026#39;}\u0026#34; Nếu Agent phản hồi đúng ý và nhớ thông tin trước đó → chứng tỏ Memory hoạt động và AgentCore đang lưu ngữ cảnh giữa các lần gọi.\nAgentCore hoạt động đúng như mong đợi trong demo này.\n"},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/1-worklog/","title":"Nhật ký công việc","tags":[],"description":"","content":"Tuần 1: Làm quen với AWS và các dịch vụ cơ bản trong AWS\nTuần 2: Tìm hiểu và cấu hình Amazon VPC\nTuần 3: Triển khai và Quản lý Amazon EC2 trên Windows và Linux\nTuần 4: Triển khai IAM Role và Làm việc với AWS Cloud9\nTuần 5: Triển khai và Quản lý Website Tĩnh với Amazon S3\nTuần 6: Làm việc với Amazon RDS – Tạo, Cấu hình, Kết nối \u0026amp; Vận hành Cơ sở dữ liệu trên AWS\nTuần 7: AWS CloudWatch \u0026amp; Hybrid DNS với Route 53 Resolver\nTuần 8: AWS DynamoDB \u0026amp; ElastiCache\nTuần 9: Tối ưu chi phí EC2 với AWS Lambda\nTuần 10: DMS - Giới thiệu và viết Lambda function\nTuần 11: Serverless - Sử dụng Amplify Authentication và Storage \u0026amp; Serverless - Hướng dẫn viết Front-end gọi API Gateway\nTuần 12: AI Agent with Amazon Bedrock on AWS\n"},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/1-worklog/1.1-week1/","title":"Worklog Tuần 1","tags":[],"description":"","content":"Mục tiêu tuần 1: Kết nối, làm quen với các thành viên trong First Cloud Journey. Hiểu dịch vụ AWS cơ bản, cách dùng console \u0026amp; CLI. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Làm quen với các thành viên FCJ - Đọc và lưu ý các nội quy, quy định tại đơn vị thực tập 08/09/2025 08/09/2025 3 - Tìm hiểu AWS và các loại dịch vụ + Compute + Storage + Networking + Database + Management Tools 09/09/2025 09/09/2025 - https://cloudjourney.awsstudygroup.com/vi/1-explore/ - https://aws.amazon.com/vi/ 4 - Tạo AWS Free Tier account - Tìm hiểu AWS Console \u0026amp; AWS CLI - Thực hành: + Tạo AWS account + Thiết lập MFA cho tài khoản Root + Tạo Admin Group và Admin User + Hỗ trợ xác thực tài khoản + Cài AWS CLI \u0026amp; cấu hình + Cách sử dụng AWS CLI 10/09/2025 10/09/2025 https://000001.awsstudygroup.com/vi/ 5 - Quản lý chi phí với AWS Budget: + Cost Budget + Usage Budget + RI Budget + Savings Plans Budget - Thực hành: + Tạo Budget theo template + Tạo Cost Budget + Tạo Usage Budget + Tạo RI Budget + Tạo Saving Plans Budget + Dọn Dẹp Tài nguyên 11/09/2025 11/09/2025 https://000007.awsstudygroup.com/vi/0-createtemplate/ 6 -Yêu cầu Hỗ trợ từ AWS Support: + Các gói hỗ trợ của AWS + Truy cập AWS Support + Quản lý Yêu cầu Hỗ trợ 12/09/2025 12/09/2025 https://000009.awsstudygroup.com/vi/ Kết quả đạt được tuần 1: Hiểu AWS là gì và nắm được các nhóm dịch vụ cơ bản:\nCompute Storage Networking Database Management Tools Đã tạo và cấu hình AWS Free Tier account thành công.\nHoàn thành 5 nhiệm vụ nhận thêm 100$ credit\nLàm quen với AWS Management Console và biết cách tìm, truy cập, sử dụng dịch vụ từ giao diện web.\nCài đặt và cấu hình AWS CLI trên máy tính bao gồm:\nAccess Key Secret Key Region mặc định Sử dụng AWS CLI để thực hiện các thao tác cơ bản như:\nKiểm tra thông tin tài khoản \u0026amp; cấu hình Lấy danh sách region Xem dịch vụ EC2 Tạo và quản lý key pair Kiểm tra thông tin dịch vụ đang chạy Có khả năng kết nối giữa giao diện web và CLI để quản lý tài nguyên AWS song song.\nBiết quản lý chi phí với AWS Butgets\nHiểu cách truy cập và tạo yêu cầu hỗ trợ trên AWS Support\n"},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/1-worklog/1.2-week2/","title":"Worklog Tuần 2","tags":[],"description":"","content":"Mục tiêu tuần 2: Hiểu về Amazon Virtual Private Cloud (VPC) Hiểu cách triển khai Amazon EC2 instances Biết cách cấu hình AWS Site-to-Site VPN Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Tìm hiểu về Amazon Virtual Private Cloud (Amazon VPC): + Subnets + Route Table + Internet Gateway + NAT Gateway - Thực hành: + Tạo VPC + Tạo Subnet + Tạo Internet Gateway + Tạo Route Table 15/09/2025 15/09/2025 - https://000003.awsstudygroup.com/vi/1-introduce/ - https://000003.awsstudygroup.com/vi/3-prerequisite/ 3 - Tìm hiểu Tường lửa trong VPC: + Security Group + Network ACLs + NetworkingVPC Resource Map - Thực hành: + Tạo Security Group + Kích hoạt VPC Flow Logs 16/09/2025 16/09/2025 - https://000003.awsstudygroup.com/vi/2-firewallinvpc/ - https://000003.awsstudygroup.com/vi/3-prerequisite/ 4 - Triển khai Amazon EC2 Instances: + Triển khai Hạ tầng EC2 Production-Ready + Tính năng Production-Ready - Thực hành: + Tạo máy chủ EC2 + Kiểm tra kết nối + Tạo NAT Gateway + Sử dụng Reachability Analyzer + Tạo EC2 Instance Connect Endpoint (Optional) + AWS Systems Manager Session Manager + CloudWatch Monitoring \u0026amp; Alerting 17/09/2025 17/09/2025 https://000003.awsstudygroup.com/vi/4-createec2server/ 5 - Cấu hình Site to Site VPN: + Tạo môi trường VPN + Cấu hình kết nối VPN + Cấu hình VPN bằng strongSwan với Transit Gateway (Tùy chọn) - Dọn dẹp tài nguyên - Thực hành: + Tạo VPC cho VPN + Tạo EC2 Instance + Tạo Virtual Private Gateway + Tạo Customer Gateway + Tạo kết nối VPN + Cấu hình Customer Gateway + Tùy chỉnh AWS VPN Tunnel + Cấu hình VPN Nâng cao + Troubleshooting VPN + Tạo Transit Gateway Attachment + Cấu hình Route Tables 18/09/2025 18/09/2025 - https://000003.awsstudygroup.com/vi/5-vpnsitetosite/ - https://000003.awsstudygroup.com/vi/6-cleanup/ 6 - Infrastructure as Code Templates: + Tự động hóa Triển khai VPC với Infrastructure as Code + Lợi ích của IaC cho Triển khai VPC + CloudFormation Template 19/09/2025 19/09/2025 https://000003.awsstudygroup.com/vi/7-infrastructureascode/ Kết quả đạt được tuần 2: Hiểu và triển khai được Amazon VPC:\nSubnets Route Table Internet Gateway NAT Gateway. Cấu hình thành công Security Group, Network ACLs, VPC Flow Logs.\nTạo và kiểm thử EC2 Instance, áp dụng NAT Gateway, Reachability Analyzer, Instance Connect Endpoint, Session Manager, CloudWatch.\nThiết lập Site-to-Site VPN:\nTạo và cấu hình Virtual Private Gateway, Customer Gateway, VPN Connection, Route Tables Thử nghiệm cấu hình strongSwan và troubleshooting. Làm quen với hạ tầng tự động hóa bằng Infrastructure as Code (CloudFormation Templates).\n"},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/1-worklog/1.3-week3/","title":"Worklog Tuần 3","tags":[],"description":"","content":"Mục tiêu tuần 3: Nắm bắt khái niệm và mục đích sử dụng của Amazon EC2 (Elastic Compute Cloud). Làm quen với quy trình khởi tạo, truy cập, cấu hình và quản lý các máy ảo EC2 trên môi trường Windows và Linux. Tiến hành triển khai thử nghiệm một ứng dụng web cơ bản (AWS User Management) lên các EC2 instance. Rèn luyện kỹ năng giám sát, bảo mật và tối ưu tài nguyên EC2 trong suốt quá trình sử dụng. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Tìm hiểu tổng quan về Amazon EC2 cùng các thuật ngữ quan trọng: + Instance, AMI, Key Pair, Elastic IP, Security Group, Volume + Các mô hình chi phí: On-Demand, Spot, Reserved Instance, Savings Plan 22/09/2025 22/09/2025 https://cloudjourney.awsstudygroup.com/ 3 - Thực hành: Khởi tạo và thiết lập Windows EC2 instance + Lựa chọn AMI và loại instance phù hợp + Tạo key pair, điều chỉnh security group + Kết nối qua Remote Desktop (RDP) + Khám phá giao diện Windows Server 2022 23/09/2025 23/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Thực hành: Khởi tạo và thiết lập Linux EC2 instance + Tạo máy ảo Amazon Linux 2 + Đăng nhập bằng SSH với key pair + Làm quen terminal và lệnh căn bản + Cập nhật gói và hệ thống 24/09/2025 24/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Triển khai ứng dụng “AWS User Management” + Cài Node.js, npm và phụ thuộc cần thiết trên Windows \u0026amp; Linux + Triển khai ứng dụng CRUD quản lý người dùng + Kiểm thử các chức năng thêm, chỉnh sửa, xoá, tìm kiếm + Cho phép người khác truy cập ứng dụng thông qua Security Group 25/09/2025 25/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Tìm hiểu các công cụ theo dõi và quản lý EC2: + Amazon CloudWatch (theo dõi metric \u0026amp; log) + AWS Systems Manager + EC2 Instance Connect - Thu gom và giải phóng tài nguyên: xoá instance, Elastic IP, Security Group không dùng 26/09/2025 26/09/2025 https://cloudjourney.awsstudygroup.com/ Kết quả đạt được trong tuần 3: Nắm rõ những thành phần quan trọng của Amazon EC2, bao gồm:\nAMI: Mẫu hệ điều hành và phần mềm để tạo instance. Instance Type: Quy định hiệu năng (CPU, RAM, lưu trữ). Key Pair: Công cụ xác thực đăng nhập an toàn. Elastic IP: Địa chỉ IP tĩnh để truy cập công khai. Security Group: Lớp bảo vệ kiểm soát luồng truy cập. Tạo và cấu hình thành công cả Windows và Linux EC2 instance:\nKết nối bằng RDP (Windows) và SSH (Linux). Điều khiển instance qua AWS Console và CLI. Điều chỉnh các luật mạng (ports 80, 443, 22, 3389) để phục vụ ứng dụng web. Hoàn tất triển khai ứng dụng AWS User Management trên hai hệ điều hành:\nCài đặt các môi trường chạy như Node.js và npm. Vận hành đầy đủ chức năng CRUD. Cho phép người khác truy cập qua Public IP hoặc Elastic IP. Áp dụng thành công các phương pháp giám sát instance:\nSử dụng CloudWatch để theo dõi tài nguyên (CPU, RAM, network). Dùng Systems Manager để tự động hóa và quản lý phiên làm việc. Tận dụng EC2 Instance Connect để truy cập nhanh qua trình duyệt. Nâng cao khả năng quản lý chi phí và tài nguyên EC2:\nTắt hoặc xoá các instance không còn nhu cầu sử dụng. Thu hồi Elastic IP còn thừa. Dọn dẹp Security Group và Key Pair không cần thiết. Tóm tắt kiến thức đạt được: Amazon EC2: Hiểu rõ cơ chế vận hành và cách triển khai máy chủ ảo trong môi trường cloud. Quản trị Windows \u0026amp; Linux: Nắm được cách truy cập, cấu hình và bảo mật trên hai hệ điều hành. Triển khai ứng dụng: Hoàn thiện triển khai ứng dụng Node.js CRUD trên EC2. Giám sát hệ thống: Biết cách sử dụng CloudWatch \u0026amp; Systems Manager để theo dõi hiệu suất hoạt động. Tối ưu chi phí: Có khả năng phân tích và dọn dẹp tài nguyên nhằm giảm chi phí vận hành. "},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/1-worklog/1.4-week4/","title":"Worklog Tuần 4","tags":[],"description":"","content":"Mục tiêu tuần 4 Làm chủ cách cấp quyền truy cập vào tài nguyên AWS cho ứng dụng một cách an toàn. Phân biệt rõ ràng cơ chế hoạt động giữa Access Key/Secret Access Key và IAM Role. Biết quy trình tạo và gán IAM Role cho EC2 Instance, giúp ứng dụng truy cập dịch vụ AWS mà không cần gửi kèm thông tin xác thực trong mã nguồn. Làm quen với môi trường phát triển dựa trên trình duyệt AWS Cloud9, nơi đã tích hợp sẵn AWS CLI. Thực hành viết mã, chạy thử, debug và quản lý mã nguồn trực tiếp trên Cloud9. Các công việc triển khai trong tuần Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Tổng quan về IAM và cơ chế kiểm soát truy cập AWS - Ôn lại khái niệm User, Group, Policy, Role - Tìm hiểu cách AWS xác thực \u0026amp; ủy quyền truy cập 22/09/2025 22/09/2025 https://cloudjourney.awsstudygroup.com/ 3 - Thử nghiệm cấp quyền bằng Access Key/Secret Access Key - Kiểm thử ứng dụng truy cập AWS bằng Access Key - Phân tích các rủi ro và điểm yếu khi nhúng Access Key trong ứng dụng 23/09/2025 23/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Tạo IAM Role dành cho EC2 - Gán role cho instance và kiểm tra quyền truy cập S3/DynamoDB qua ứng dụng mẫu Node.js - Kiểm tra lại ứng dụng sau khi thu hồi quyền 24/09/2025 24/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Khởi tạo và làm quen với Cloud9 IDE - Thiết lập workspace, cấu hình môi trường làm việc - Trải nghiệm terminal, file explorer, debugger, syntax highlight 25/09/2025 25/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Bài tập tổng hợp: + Viết script thao tác AWS CLI trong Cloud9 + Xây dựng ứng dụng Node.js CRUD (User Management) chạy trực tiếp trên Cloud9 + Xóa tài nguyên để tránh tính phí 26/09/2025 26/09/2025 https://cloudjourney.awsstudygroup.com/ Kết quả đạt được trong tuần 4 Nắm rõ điểm khác biệt giữa IAM Role và Access Key, cũng như lý do tại sao việc dùng role là lựa chọn an toàn hơn trong thực tế. Biết cách tạo IAM Role với quyền hạn phù hợp, chẳng hạn truy cập đọc/ghi vào S3. Gán IAM Role cho EC2 Instance và cho phép ứng dụng chạy trên đó truy cập dịch vụ AWS mà không cần thông tin đăng nhập cố định. Sử dụng thành thạo AWS Cloud9 để: Tạo workspace phát triển. Kết nối tới EC2. Chạy các lệnh AWS CLI, viết và kiểm thử mã trực tiếp. Triển khai ứng dụng Node.js nhỏ kết nối với S3 hoặc DynamoDB ngay trong Cloud9. Hiểu quy trình thu hồi, xoá tài nguyên sau bài thực hành nhằm tối ưu chi phí. Tóm tắt kiến thức tích luỹ IAM Role: Cơ chế cấp quyền động, giảm rủi ro bảo mật do không cần dùng Access Key. Access Key: Hiểu điểm yếu khi dùng trực tiếp trong code hoặc trên máy developer. AWS Cloud9: Một môi trường lập trình linh hoạt trên browser, hỗ trợ nhiều ngôn ngữ và tích hợp AWS CLI. AWS CLI/SDK: Công cụ để truy cập và thao tác dịch vụ AWS dựa trên IAM Role. Quy trình triển khai thực tế: Thiết lập môi trường, viết và chạy ứng dụng, kiểm tra quyền truy cập, và dọn dẹp tài nguyên khi hoàn thành. "},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/1-worklog/1.5-week5/","title":"Worklog Tuần 5","tags":[],"description":"","content":"Mục tiêu tuần 5: Nắm vững cơ chế hoạt động của Amazon S3, hiểu cấu trúc lưu trữ dạng object, các tùy chọn lưu trữ và phạm vi áp dụng trong thực tế. Triển khai static website trên S3: bật tính năng hosting, tinh chỉnh quyền truy cập và thử nghiệm tốc độ hoạt động. Thành thạo các thiết lập bảo mật cho bucket, bao gồm chặn truy cập công khai, viết chính sách IAM và cấu hình Bucket Policy một cách an toàn. Làm quen với các thao tác nâng cao: Versioning, Transfer Acceleration, Replication sang vùng khác, cùng các lệnh di chuyển object. Thực hiện quy trình giải phóng tài nguyên đúng chuẩn để tránh phát sinh chi phí không mong muốn và tuân thủ best practice khi sử dụng S3. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 Ôn tập nền tảng \u0026amp; chuẩn bị môi trường + Giải thích khái niệm bucket, object, region, key và các phân lớp lưu trữ (STANDARD, IA, GLACIER) + Tìm hiểu mức độ bền (11-nines) và khả năng sẵn sàng + Tổng hợp bài toán phù hợp cho S3: website tĩnh, sao lưu, phân tích dữ liệu 06/10/2025 06/10/2025 https://cloudjourney.awsstudygroup.com/ 3 Khởi tạo bucket \u0026amp; bật hosting website + Tạo bucket theo đúng quy tắc đặt tên và chọn region + Upload index.html, error.html + Kích hoạt chế độ Static Website Hosting và truy cập endpoint để test 07/10/2025 07/10/2025 https://cloudjourney.awsstudygroup.com/ 4 Thiết lập Public Access \u0026amp; cấu hình Bucket Policy + Nắm rõ các chế độ Block Public Access ở cấp tài khoản và bucket + Viết Bucket Policy để chỉ cho phép public một object cụ thể (index.html) + Kiểm tra truy cập website qua trình duyệt + Thử thu hồi public access và kiểm tra lại hành vi bảo mật 08/10/2025 08/10/2025 https://cloudjourney.awsstudygroup.com/ 5 Nâng tốc độ \u0026amp; mở rộng khả năng hosting + So sánh ưu điểm — hạn chế giữa: S3 + CloudFront, S3 Transfer Acceleration, AWS Amplify Hosting + Bật Transfer Acceleration và benchmark tốc độ bằng curl ở nhiều vị trí + (Tùy chọn) Tạo CloudFront để bật HTTPS/CDN + Thêm giám sát CloudWatch và đánh giá chi phí sử dụng 09/10/2025 09/10/2025 https://cloudjourney.awsstudygroup.com/ 6 Quản lý vòng đời, replication \u0026amp; dọn dẹp tài nguyên + Bật Versioning để kiểm soát thay đổi object và thử phục hồi file cũ + Cài đặt Lifecycle chuyển sang IA/Glacier sau một số ngày nhất định + Thiết lập Cross-Region Replication với IAM Role tương ứng + Dùng cp/mv/sync để chuyển dữ liệu giữa bucket/region + Xóa tài nguyên thử nghiệm: disable acceleration, remove CloudFront, empty bucket, xóa bucket 10/10/2025 10/10/2025 https://cloudjourney.awsstudygroup.com/ Kết quả đạt được trong tuần 5: 1. Thiết lập \u0026amp; vận hành cơ bản:\nHoàn chỉnh cấu hình AWS CLI (Access Key, Secret Key, Region). Kiểm tra trạng thái CLI bằng: aws configure list aws s3 ls (liệt kê bucket) aws s3api list-buckets (xem chi tiết) Tạo/xóa bucket bằng lệnh: aws s3 mb s3://ten-bucket aws s3 rb s3://ten-bucket --force 2. Static Website Hosting:\nTạo bucket mới, đưa file HTML vào và bật hosting. Xem website chạy qua endpoint được cung cấp. Kiểm tra trang lỗi khi truy cập file không tồn tại để đảm bảo error.html hoạt động đúng. 3. Quản lý quyền \u0026amp; bảo mật:\nThử bật/tắt Block Public Access để quan sát sự khác biệt. Viết chính sách bucket chỉ công khai một file duy nhất. Test quyền truy cập bằng cách public/private từng object. 4. Quản lý dữ liệu \u0026amp; tăng tốc hiệu năng:\nBật versioning và thử upload nhiều phiên bản của cùng một file. Khôi phục file từ version cũ để hiểu cơ chế. Áp dụng lifecycle rules để tự động chuyển object sang IA hoặc Glacier. Thử tốc độ Transfer Acceleration bằng curl từ nhiều region. 5. Replication \u0026amp; di chuyển dữ liệu:\nThiết lập CRR và quan sát object được sao chép sang region khác. Sử dụng các lệnh: aws s3 cp aws s3 mv aws s3 sync Kiểm tra bucket đích để xác minh replication thành công. 6. Dọn dẹp \u0026amp; tối ưu chi phí sử dụng:\nXóa object test, vô hiệu hóa Transfer Acceleration và CloudFront. Xóa bucket thử nghiệm, tắt hosting. Tổng hợp best practices giúp kiểm soát chi phí và đảm bảo an toàn dữ liệu. Ghi chú: Luôn tránh public toàn bộ bucket — chỉ public object thật sự cần thiết. Block Public Access là lớp an toàn quan trọng, chỉ nên override nếu hiểu rõ quyền và rủi ro. Versioning kết hợp Lifecycle giúp quản lý dữ liệu lâu dài mà vẫn tiết kiệm chi phí. CloudFront phù hợp khi cần HTTPS, domain riêng và hiệu năng toàn cầu; Transfer Acceleration thích hợp khi client thường upload từ xa. Amplify Hosting là lựa chọn đơn giản khi chỉ cần triển khai website tĩnh từ repo Git. IAM phải được cấu hình chặt chẽ, không lưu Access Key trong mã nguồn. CloudWatch + S3 Storage Lens hỗ trợ theo dõi dung lượng, traffic và chi phí. Khi xóa bucket có Versioning, cần xóa tất cả version + delete marker, tránh bị tính phí lưu trữ không mong muốn. Tóm tắt kiến thức đạt được: Vận hành S3 ở mức đầy đủ: hosting, bảo mật, replication, versioning và tối ưu cost. Biết chọn giải pháp triển khai static site phù hợp: S3 đơn thuần, S3 + CloudFront, hoặc Amplify. Sử dụng hiệu quả các công cụ quản lý dữ liệu lớn: lifecycle, policy, replication, sync/copy. Nắm kỹ quy trình dọn dẹp tài nguyên để tránh phát sinh chi phí. "},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/1-worklog/1.6-week6/","title":"Worklog Tuần 6","tags":[],"description":"","content":" Mục tiêu tuần 6: Làm quen với phương thức giám sát, phân tích hiệu năng qua Performance Insights, Monitoring và Enhanced Logging. Hiểu rõ kiến trúc tổng thể của Amazon RDS, bao gồm các loại engine như MySQL, PostgreSQL, MariaDB, SQL Server… Thực hành backup – restore – snapshot – automated backup – failover để đảm bảo khả năng phục hồi dữ liệu. Tạo, cấu hình và vận hành một RDS Instance từ bước nền tảng đến cấu hình nâng cao. Xây dựng parameter group, subnet group, security group dành riêng cho RDS. Thiết lập kết nối giữa RDS và các môi trường như EC2, Cloud9, hoặc máy local. Thành thạo phương pháp scale tài nguyên và tối ưu chi phí, đồng thời biết cách dọn dẹp tài nguyên khi không còn nhu cầu sử dụng. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 Giới thiệu RDS + Các lựa chọn engine + Mô hình Multi-AZ và Read Replica + Chu kỳ backup \u0026amp; snapshot 13/10/2025 13/10/2025 https://cloudjourney.awsstudygroup.com/ 3 Khởi tạo RDS Instance + Xây dựng Subnet Group + Cấu hình Security Group dành cho Cloud9/EC2 + Chọn engine, dung lượng và loại instance 14/10/2025 14/10/2025 https://cloudjourney.awsstudygroup.com/ 4 Kết nối \u0026amp; thao tác trên RDS + Truy cập từ EC2 hoặc Cloud9 + Tạo schema \u0026amp; bảng dữ liệu + Hành động CRUD bằng MySQL hoặc PostgreSQL client 15/10/2025 15/10/2025 https://cloudjourney.awsstudygroup.com/ 5 Monitoring – Backup – Restore + Tạo snapshot thủ công + Khôi phục instance từ snapshot + Kiểm tra CPU, kết nối, throughput + Dùng Performance Insights phân tích truy vấn 16/10/2025 16/10/2025 https://cloudjourney.awsstudygroup.com/ 6 Scaling – Tối ưu hóa – Thu hồi tài nguyên + Tăng/giảm instance class + Mở rộng dung lượng storage + Điều chỉnh backup retention hợp lý + Xoá snapshot \u0026amp; RDS không còn dùng 17/10/2025 17/10/2025 https://cloudjourney.awsstudygroup.com/ Kết quả đạt được trong tuần 6: 1. Làm chủ quy trình Backup – Restore – Snapshot:\nThực hiện tạo snapshot thủ công. Khôi phục (restore) sang một RDS instance mới. Quan sát thời điểm backup (backup window). Xác nhận automated backup được tạo tự động mỗi ngày. 2. Hiểu rõ cơ chế hoạt động của Amazon RDS:\nNhận biết sự khác nhau giữa Single-AZ, Multi-AZ và Read Replica. Nắm cách automated backup hoạt động và thời gian lưu trữ. Hiểu snapshot là bản lưu trữ thủ công và không tự động xoá. 3. Kết nối RDS từ local/EC2/Cloud9:\nCài MySQL/PostgreSQL client để truy cập. Kết nối bằng endpoint dạng: mysql -h endpoint.amazonaws.com -u admin -p Tạo database, table và thực hiện các câu lệnh CRUD: CREATE DATABASE demo; CREATE TABLE users (...); INSERT, SELECT, UPDATE, DELETE 4. Cấu hình một RDS Instance hoàn chỉnh:\nTạo DB Subnet Group gồm 2 subnet ở hai AZ khác nhau. Tạo Security Group cho phép kết nối (port 3306/5432). Cài đặt: Engine MySQL/PostgreSQL Instance class db.t3.micro Storage gp3 20GB Backup retention Endpoint private hoặc public 5. Giám sát hiệu năng \u0026amp; hoạt động hệ thống:\nTheo dõi CPU, RAM, số lượng kết nối qua mục Monitoring. Dùng Performance Insights để tìm truy vấn gây tải nặng. Kiểm tra Slow Query Log khi được bật. 6. Tối ưu chi phí và scale tài nguyên:\nThay đổi instance class t3.micro → t3.small theo nhu cầu tải. Kéo dung lượng storage lên 30GB khi cần. Điều chỉnh thời gian giữ backup xuống 3 ngày để tiết kiệm chi phí. Xóa snapshot không còn phục vụ mục đích lưu trữ. 7. Dọn dẹp tài nguyên:\nXóa RDS instance. Xóa subnet group và security group tương ứng. Dọn dẹp toàn bộ snapshot cũ. Tóm tắt kiến thức đạt được Hiểu cơ chế hoạt động của Amazon RDS và các engine phổ biến. Tạo, vận hành và kết nối tới RDS từ Cloud9, EC2 hoặc máy local. Nắm rõ nền tảng backup, snapshot và quá trình khôi phục. Biết sử dụng Performance Insights để đánh giá hiệu năng truy vấn. Làm quen với việc scale tài nguyên và tối ưu chi phí. Thực hành dọn dẹp tài nguyên để tránh chi phí phát sinh. "},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/1-worklog/1.7-week7/","title":"Worklog Tuần 7","tags":[],"description":"","content":" Mục tiêu tuần 7: Phần 1 – AWS CloudWatch\nHiểu được kiến trúc tổng thể của CloudWatch và chức năng từng thành phần: Metrics, Logs, Events, Dashboards, Alarms. Quan sát hoạt động của các metric từ EC2, RDS và Lambda trong môi trường vận hành thực tế. Thiết lập thu thập log và nắm được cách CloudWatch Logs lưu trữ – lập chỉ mục – tìm kiếm – phân tích dữ liệu. Cấu hình ngưỡng cảnh báo nhằm hỗ trợ giám sát liên tục. Xây dựng Dashboard để trực quan hóa tình trạng hệ thống theo thời gian thực. Kích hoạt Container Insights để theo dõi chi tiết hiệu năng container. Loại bỏ tài nguyên không còn sử dụng để tối ưu chi phí. Phần 2 – Hybrid DNS với Route 53 Resolver\nNắm được chiến lược kiến trúc khi tích hợp DNS on-premise với DNS trong AWS. Thiết lập các endpoint inbound và outbound của Route 53 Resolver. Xây dựng các bộ quy tắc Resolver Rules để điều hướng truy vấn DNS theo yêu cầu. Tích hợp DNS của Active Directory nội bộ với Private Hosted Zone của Route 53. Kiểm thử phân giải tên miền hai chiều giữa hệ thống nội bộ và AWS. Thu hồi toàn bộ tài nguyên (AD server, endpoint, cấu hình VPC) sau khi hoàn tất. Các công việc triển khai trong tuần Thứ Công việc chi tiết Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 CloudWatch – Tổng quan \u0026amp; Kiến trúc • Ôn lại mục tiêu thiết kế của CloudWatch và cách nó tích hợp với dịch vụ AWS. • Tìm hiểu cơ chế của Metrics, Logs, Alarms, Events, Dashboards và Container Insights. • So sánh giữa giám sát cơ bản và giám sát chi tiết. • Phân tích luồng hoạt động từ ứng dụng → agent → log group → metric → alarm → SNS → xử lý. • Tạo biểu đồ đầu tiên dựa trên dữ liệu từ EC2. 20/10/2025 20/10/2025 https://cloudjourney.awsstudygroup.com/ 3 CloudWatch Metrics \u0026amp; Logs – Thực hành • Tạo Log Group và tìm hiểu luồng vận hành của Log Stream. • Cài đặt CloudWatch Agent để thu dữ liệu CPU, RAM, network từ hệ điều hành. • Chỉnh sửa file cloudwatch-agent.json để gửi log hệ thống và log ứng dụng. • Tạo metric filter để phát hiện lỗi phổ biến (“ERROR”, “Failed”, “CRITICAL”). • Gửi dữ liệu metric thủ công bằng AWS CLI. • Thử đổi chế độ retention và theo dõi thay đổi. 21/10/2025 21/10/2025 https://cloudjourney.awsstudygroup.com/ 4 CloudWatch Alarm \u0026amp; Dashboard – Tự động hóa giám sát • Tạo cảnh báo CPU vượt 70% trong 5 phút. • Tạo alarm theo dõi EC2 Status Check. • Tích hợp SNS để gửi email thông báo. • Dùng alarm để thực hiện hành động tự động như reboot EC2. • Thiết kế Dashboard với nhiều widget hiển thị logs, metrics và trạng thái. • Tạo Dashboard tổng hợp dữ liệu của EC2, RDS, network\u0026hellip; • Kích hoạt Container Insights để theo dõi CPU/memory và số lần restart. 22/10/2025 22/10/2025 https://cloudjourney.awsstudygroup.com/ 5 Route 53 Resolver – Kiến trúc DNS Hybrid • Tìm hiểu Private Hosted Zone và cơ chế phân giải DNS trong VPC. • Thiết lập Outbound Endpoint để chuyển truy vấn DNS từ AWS về hệ thống nội bộ. • Tạo Inbound Endpoint để nhận truy vấn từ on-premise. • Tạo rule forward domain “corp.local” về DNS AD. • Gán rule cho VPC và kiểm tra hoạt động của endpoint. 23/10/2025 23/10/2025 https://cloudjourney.awsstudygroup.com/ 6 Microsoft AD + DNS Integration – Mô phỏng on-premise • Khởi tạo Windows Server và cài Active Directory Domain Services. • Thiết lập domain “corp.local” và DNS zone. • Thêm Conditional Forwarder trỏ đến Inbound Endpoint. • Tạo các bản ghi A và CNAME. • Kiểm thử phân giải hai chiều: – Từ AWS: nslookup dc1.corp.local – Từ AD: nslookup api.internal.aws • Giả lập lỗi endpoint và quan sát hành vi failover. 24/10/2025 24/10/2025 https://cloudjourney.awsstudygroup.com/ 7 Dọn dẹp tài nguyên \u0026amp; Tối ưu chi phí • Xóa Log Group, Log Stream, Dashboard, Alarm không dùng. • Tắt Container Insights để giảm chi phí. • Gỡ Resolver Endpoint (tính phí theo giờ). • Xóa toàn bộ Resolver Rules. • Tắt domain controller. • Xóa các EC2 thử nghiệm. • Kiểm tra chi phí CloudWatch Logs, metrics và endpoint. 25/10/2025 25/10/2025 https://cloudjourney.awsstudygroup.com/ Kết quả đạt được trong tuần 7: 1. AWS CloudWatch CloudWatch Metrics – Thu thập \u0026amp; phân tích\nNắm danh sách metric mặc định của EC2, EBS, VPC và Load Balancer. Tạo metric tùy chỉnh bằng AWS CLI để kiểm thử khả năng ghi dữ liệu. Quan sát retention và độ phân giải dữ liệu từng giây. Phân tích các dấu hiệu bất thường như CPU tăng đột biến, network spike, lượng DiskOps cao. CloudWatch Logs – Thu thập log ứng dụng\nCài agent để gửi log ứng dụng và log hệ điều hành. Tạo log group – stream phục vụ debugging. Thiết lập retention và thử export sang S3 để lưu lâu dài. Nhận diện các lỗi phổ biến: unauthorized request, throttling, CPU credit cạn. CloudWatch Alarms – Cảnh báo \u0026amp; phản hồi\nTạo cảnh báo CPU vượt mức trong thời gian xác định. Theo dõi EC2 system check để đảm bảo instance ổn định. Tích hợp SNS gửi email khi có sự cố. Kích hoạt auto-recovery để khắc phục lỗi phần cứng. CloudWatch Dashboards\nXây dashboard theo dõi realtime. Tổng hợp dữ liệu từ EC2, NAT Gateway, RDS, ELB. Dùng nhiều kiểu widget để trực quan dữ liệu. Container Insights\nKích hoạt theo dõi container trên ECS và Fargate. Phân tích CPU/memory từng task. Theo dõi số lần restart container để phát hiện lỗi. Đánh giá cách Container Insights rút ngắn thời gian tìm lỗi. Dọn dẹp tài nguyên\nXóa dashboard, alarm, custom metrics. Gỡ agent và xóa log group không cần thiết. Tắt Container Insights để giảm chi phí. Kết luận CloudWatch\nHiểu trọn vẹn quy trình: thu thập → phân tích → cảnh báo → hiển thị. Vận hành đầy đủ pipeline logs/metrics. Cải thiện kỹ năng phân tích sự cố. Tối ưu vận hành và chi phí hơn. Tự xây dựng cơ chế cảnh báo realtime. 2. Hybrid DNS với Route 53 Resolver Nắm kiến trúc DNS lai\nHiểu các vấn đề khi doanh nghiệp dùng DNS nội bộ. Biết chức năng của Inbound/Outbound Endpoint và Resolver Rules. Chuẩn bị môi trường\nTạo VPC, subnet và security group cho endpoint. Tạo Windows Server mô phỏng AD. Kết nối RDGW\nDùng RDGW để truy cập server trong private subnet. Kiểm tra phân giải DNS nội bộ trước khi tích hợp. Triển khai Active Directory\nTạo domain controller và DNS zone nội bộ. Kiểm tra bản ghi SRV, A, NS. Thiết lập DNS Hybrid\nOutbound Endpoint\nCho phép AWS gửi truy vấn DNS về AD. Forward domain nội bộ về DNS on-premise. Inbound Endpoint\nCho phép on-premise truy vấn bản ghi AWS. Kiểm tra phân giải Private Hosted Zone. Resolver Rules\nTạo rule chuyển tiếp domain nội bộ. Dùng system rule để kế thừa DNS mặc định VPC. Kiểm thử hệ thống\nThực hiện phân giải hai chiều AD ↔ AWS. Đánh giá độ ổn định và độ trễ. Đảm bảo bản ghi phân giải chính xác. Dọn dẹp tài nguyên\nXóa endpoint. Xóa Private Hosted Zone và record test. Tắt AD server và EC2 thử nghiệm. Kết luận DNS\nHiểu rõ mô hình DNS hybrid. Thiết lập hoàn thiện endpoint và rule. Tích hợp DNS nội bộ với AWS thành công. Phân giải hai chiều hoạt động ổn định. Tóm tắt kiến thức đạt được CloudWatch:\nGiám sát logs và metrics toàn diện. Tự động hóa cảnh báo. Xây dashboard trực quan. Giám sát container hiệu quả. Route 53 Hybrid DNS:\nHiểu cơ chế DNS trong môi trường lai. Tích hợp DNS nội bộ và DNS AWS. Thực hiện phân giải hai chiều. Sẵn sàng áp dụng vào môi trường doanh nghiệp. "},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/1-worklog/1.8-week8/","title":"Worklog Tuần 8","tags":[],"description":"","content":"Mục tiêu tuần 8: Tuần này tập trung vào hai nhóm nội dung: lưu trữ NoSQL với DynamoDB và bộ nhớ đệm tốc độ cao bằng Redis trong ElastiCache. Mục tiêu chính là hiểu cách hai dịch vụ này vận hành, cấu trúc dữ liệu, khả năng mở rộng, cùng việc kết hợp chúng để tăng tốc các tác vụ đọc trong hệ thống.\nPhần 1 – Amazon DynamoDB Khảo sát mô hình dữ liệu key–value và document, đặc trưng của DynamoDB. Tìm hiểu cơ chế phân vùng dữ liệu (partition), cách lựa chọn khóa để tránh điểm nóng. Đi sâu vào các khái niệm quan trọng: RCU/WCU, chế độ On-Demand, TTL, Streams, và Global Tables. Thiết kế bảng có Partition Key và Sort Key; thử cấu hình thêm GSI và LSI cho nhiều kiểu truy vấn khác nhau. Làm quen nhóm thao tác CRUD và truy vấn nâng cao qua Query/Scan, kết hợp FilterExpression và ConditionExpression. Sử dụng AWS CLI và file JSON để tương tác bảng. Nhập dữ liệu thử nghiệm nhằm đánh giá hiệu năng truy vấn. Phần 2 – Amazon ElastiCache for Redis Ôn lại nguyên lý của bộ nhớ đệm in-memory và cách Redis tổ chức cụm (cluster mode enabled/disabled). Tạo môi trường Redis: cấu hình node, subnet group, security group và parameter group. Kết nối từ EC2 thông qua redis-cli để kiểm tra hoạt động. Thử các lệnh cơ bản: SET/GET, TTL, EXPIRE, và snapshot RDB. Áp dụng mô hình Cache-Aside để kết hợp Redis với DynamoDB. So sánh độ trễ giữa truy vấn cache và truy vấn DynamoDB. Thu hồi tài nguyên sau khi hoàn thành. Các công việc thực hiện Thứ Nội dung triển khai Bắt đầu Hoàn thành Tài liệu 2 Tổng quan DynamoDB Khám phá cấu trúc NoSQL, cơ chế phân vùng, throughput, và các thành phần như GSI/LSI, TTL, Streams. Hiểu cách DynamoDB phân phối dữ liệu theo Partition Key và tác động của lựa chọn khóa đến hiệu năng. 27/10/2025 27/10/2025 https://cloudjourney.awsstudygroup.com/ 3 Thiết kế bảng \u0026amp; truy vấn Tạo bảng với khóa phân vùng và khóa sắp xếp; bổ sung GSI để mở rộng đường truy vấn. Kiểm tra cơ chế TTL, thực hiện đủ CRUD và so sánh Query/Scan trong các tình huống thực tế. 28/10/2025 28/11/2025 https://cloudjourney.awsstudygroup.com/ 4 Làm việc với CLI Khởi tạo bảng bằng CLI, nhập dữ liệu JSON mẫu, chạy truy vấn bằng KeyConditionExpression, cập nhật có điều kiện và theo dõi mức tiêu thụ throughput. 29/10/2025 29/10/2025 https://cloudjourney.awsstudygroup.com/ 5 Redis – Cấu trúc và triển khai Chuẩn bị subnet group, security group, tạo cụm Redis có primary và replica, bật failover, sau đó kiểm tra kết nối từ EC2. 30/10/2025 30/10/2025 https://cloudjourney.awsstudygroup.com/ 6 Tích hợp DynamoDB – Redis Kiểm tra redis-cli với GET/SET/TTL, thử snapshot, thiết lập Cache-Aside (cache hit/miss) và đo độ trễ giữa Redis và DynamoDB. 31/10/2025 31/10/2025 https://cloudjourney.awsstudygroup.com/ 7 Dọn tài nguyên và đánh giá Xóa Redis Cluster và các group liên quan; gỡ bảng DynamoDB thử nghiệm. Kiểm tra billing để xác định chi phí phát sinh và ghi nhận sự khác biệt tốc độ giữa cache và database. 1/11/2025 1/11/2025 https://cloudjourney.awsstudygroup.com/ Kết quả trong tuần 8 1. DynamoDB Thiết kế \u0026amp; mô hình hóa dữ liệu\nHiểu rõ cách DynamoDB phân tán dữ liệu và mở rộng theo chiều ngang. Thực hành xây dựng khóa phân vùng/sắp xếp phù hợp để giảm va chạm partition. Trải nghiệm vai trò của chỉ mục phụ GSI/LSI trong việc định hình access pattern. Truy vấn \u0026amp; cập nhật\nHoàn chỉnh toàn bộ CRUD qua giao diện console và CLI. Sử dụng thành thạo KeyConditionExpression cho truy vấn có điều kiện. Biết kết hợp FilterExpression, UpdateExpression trong tình huống lọc phức tạp. Khảo sát Scan và giới hạn của phương pháp này khi dữ liệu lớn. Tính năng nâng cao\nKích hoạt TTL và quan sát hành vi tự loại bỏ bản ghi. Thử DynamoDB Streams để xem cách log sự thay đổi. Đánh giá chi phí dựa trên nhu cầu đọc/ghi và mức tiêu thụ thực tế. 2. ElastiCache for Redis Triển khai cụm Redis\nXây dựng cụm Redis trong VPC với cấu hình mạng đầy đủ. Kiểm tra hoạt động replication và failover. Thử kết nối từ EC2 để xác nhận môi trường hoạt động ổn định. Thao tác và quan sát\nThực hiện SET, GET, DEL, TTL, EXPIRE trong nhiều tình huống. Đo ảnh hưởng của snapshot RDB lên trạng thái dữ liệu. Đánh giá hành vi cache khi dung lượng bị giới hạn. 3. Kết hợp DynamoDB – Redis (Cache-Aside) Xây dựng luồng xử lý: kiểm tra cache → truy vấn DynamoDB nếu thiếu → đưa dữ liệu trở lại Redis. So sánh kết quả cache hit và miss, đo độ trễ thật để rút ra đặc tính hiệu năng. Nhận ra lý do các hệ thống đọc nhiều thường dùng Redis như lớp đệm. Nhận diện các trường hợp áp dụng điển hình: session, user profile, dữ liệu tra cứu tần suất cao. Tổng hợp kiến thức DynamoDB\nMô hình NoSQL phân tán. Cách chọn Partition Key và Sort Key hợp lý. Các dạng truy vấn phổ biến và các biểu thức liên quan. Cơ chế throughput theo RCU/WCU hoặc On-Demand. TTL và Streams cho quản lý vòng đời dữ liệu. ElastiCache Redis\nMô hình cache tốc độ cao dựa trên RAM. Cấu hình cluster, replica và failover. TTL, snapshot và xử lý dữ liệu tạm thời. Tích hợp DynamoDB – Redis\nÁp dụng Cache-Aside để giảm tải đọc. Phân tích cache hit/miss. Tối ưu hóa hiệu năng trong dịch vụ lớn. "},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/1-worklog/1.9-week9/","title":"Worklog Tuần 9","tags":[],"description":"","content":" Mục tiêu tuần 9: Phần 1 – Tối ưu chi phí EC2 bằng Lambda Function\nNắm được cơ chế tự động bật/tắt EC2 nhằm giảm chi phí vận hành. Tạo hệ thống tag để xác định các máy chủ cần được xử lý theo lịch. Xây dựng Lambda Function quản lý Start/Stop EC2 dựa trên tag. Tạo lịch chạy định kỳ bằng EventBridge Scheduler. Theo dõi hoạt động thực tế thông qua CloudWatch Logs. Thu hồi toàn bộ tài nguyên sau khi thử nghiệm xong. Phần 2 – Tối ưu chi phí EC2 với Savings Plan\nTìm hiểu Savings Plan và nguyên tắc giảm giá dựa trên mức cam kết. Phân biệt Compute Savings Plans và EC2 Instance Savings Plans. Hiểu cơ chế commit USD/giờ và cách công cụ estimator tính toán. Biết quy trình mua và áp dụng Savings Plan theo chuẩn AWS. Thực hành ước tính chi phí qua AWS Pricing Calculator. Các công việc triển khai trong tuần Thứ Công việc chi tiết Bắt đầu Hoàn thành Nguồn 2 Phân tích mô hình tối ưu hóa EC2\n• Xác định vấn đề lãng phí khi EC2 chạy 24/7 nhưng khối lượng sử dụng ít.\n• Mô tả luồng hoạt động: User → EventBridge → Lambda → EC2 API.\n• Nhận dạng workload phù hợp với cơ chế tự động Start/Stop.\n• Xem xét rủi ro và giới hạn của việc bật/tắt EC2. 03/11/2025 03/11/2025 https://cloudjourney.awsstudygroup.com/ 3 Tạo Tag phân loại EC2\n• Thiết lập Tag Key: Schedule.\n• Tag Value mẫu: office-hours, training-only, weekend-shutdown.\n• Gán tag vào các EC2 cần tự động hóa.\n• Kiểm tra bằng CLI: describe-instances --filters tag:Schedule=office-hours. 04/11/2025 04/11/2025 https://cloudjourney.awsstudygroup.com/ 4 Tạo IAM Role cho Lambda\n• Tạo role: LambdaEC2ScheduleRole.\n• Gán policy: AmazonEC2FullAccess (dùng cho mục đích lab).\n• Thêm quyền ghi log: AWSLambdaBasicExecutionRole.\n• Kiểm tra trust relationship với lambda.amazonaws.com. 05/11/2025 05/11/2025 https://cloudjourney.awsstudygroup.com/ 5 Phát triển Lambda Start/Stop EC2\n• Viết code Python (boto3) để StartInstances/StopInstances.\n• Thêm logic lọc instance theo tag \u0026lt;Schedule\u0026gt;.\n• Ghi log instance ID được bật/tắt.\n• Kiểm thử bằng “Test event”. 06/11/2025 06/11/2025 https://cloudjourney.awsstudygroup.com/ 6 Tích hợp EventBridge Scheduler\n• Tạo rule bật EC2 lúc 08:00 AM.\n• Tạo rule tắt EC2 lúc 18:00 PM.\n• Gán Lambda làm target.\n• Xem log trong CloudWatch để xác nhận hoạt động. 07/11/2025 07/11/2025 https://cloudjourney.awsstudygroup.com/ 7 Kiểm thử \u0026amp; Dọn dẹp\n• Xác minh EC2 bật/tắt đúng lịch.\n• Theo dõi CloudWatch Logs để xử lý lỗi quyền hạn.\n• Xóa rule, Lambda Function và IAM Role sau khi hoàn tất.\n• So sánh chi phí EC2 trước và sau tối ưu hóa. 08/11/2025 08/11/2025 https://cloudjourney.awsstudygroup.com/ Kết quả đạt được trong tuần 9 1. Tối ưu chi phí EC2 qua Lambda\nThiết lập thành công quy trình bật/tắt EC2 dựa trên giờ làm việc. Hiểu rõ cách dùng Tag để nhóm EC2 cần tối ưu hóa. Viết Lambda Function điều khiển Start/Stop bằng boto3. Tự động hóa hoàn toàn thông qua EventBridge Scheduler. Ghi nhận log đầy đủ: thời gian, instance ID, lỗi permission. Tiết kiệm được 50–70% chi phí đối với workload không chạy liên tục. 2. Tối ưu chi phí với Savings Plan\nHiểu cơ chế giảm giá của Savings Plan dựa trên cam kết USD/giờ. Sử dụng Pricing Calculator để ước tính mức commit phù hợp. Nắm rõ sự khác nhau giữa hai loại Savings Plan: Compute Savings Plan – linh hoạt, áp dụng đa dịch vụ. EC2 Instance Savings Plan – tiết kiệm cao nhưng ràng buộc hơn. Nắm quy trình đăng ký và kích hoạt Savings Plan. Thấy rõ lợi ích tiết kiệm có thể đạt đến ~66%. Tóm tắt kiến thức đạt được AWS Lambda + EC2 Automation\nVận hành tự động bật/tắt EC2 bằng Lambda. Sử dụng Boto3 để tương tác với API EC2. Dùng EventBridge Scheduler để đặt lịch chạy. CloudWatch Logs để kiểm tra và xử lý lỗi. AWS Savings Plans\nCơ chế commit chi phí để giảm giá sử dụng hạ tầng. Áp dụng phù hợp cho workload chạy liên tục 24/7. Phân loại Compute và EC2 Plans. Kết hợp Savings Plan + Auto Scheduling để tối ưu chi phí toàn diện. "},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/5-workshop/5.2-prerequiste/","title":"Các bước chuẩn bị","tags":[],"description":"","content":"IAM permissions Các quyền cần thêm AdministratorAccess AmazonBedrockFullAccess AWSCodeBuildAdminAccess AWSCodeBuildDeveloperAccess BedrockAgentCoreFullAccess Tạo user và gán quyền Vào IAM → Users → chọn Create user. Thêm các quyền đã liệt kê ở trên. Hoàn tất tạo user và lưu Access Key nếu cần dùng SDK. Tải AWS CLI Tải AWS CLI: AWS CLI Link\nSau đó cài đặt theo hướng dẫn.\nCấu hình UV management 1. Vì sao dùng UV? UV nhanh, nhẹ, quản lý môi trường tốt hơn pip.\n2. Cài đặt UV trên Windows Chạy lệnh:\npowershell -ExecutionPolicy ByPass -c \u0026#34;irm https://astral.sh/uv/install.ps1 | iex\u0026#34; Thêm UV vào PATH:\n$env:Path = \u0026#34;C:\\Users\\leamo\\.local\\bin;$env:Path\u0026#34; Khởi động lại máy để nhận PATH mới.\n3. Khởi tạo môi trường UV Trong thư mục dự án:\nuv init Sau đó chọn environment trong VS Code.\nKết nối máy với AWS CLI Vào lại IAM để tạo Access Key.\nTạo Access Key và cấu hình AWS CLI Trong user: Security credentials → Create access key Chọn loại Command Line Interface (CLI) Cấu hình AWS CLI Chạy lệnh:\naws configure Nhập lần lượt:\nAWS Access Key ID\nAWS Secret Access Key\nDefault region name (demo: ap-southeast-1)\nDefault output format\njson Khởi chạy AWS CLI AgentCore Chạy:\nuv run which agentcore sau khi chạy sẽ tải các thư viện cần thiết cho AWS Agentcore về máy\nTạo Groq API bạn vào trang Groq và tạo api như hình.Đây là các tool bên ngoài bổ trợ cho Rag của liên kết thông qua AWS Agentcore "},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/5-workshop/5.3-architecture/5.3.2-groq-api/","title":"Gọi Groq API","tags":[],"description":"","content":"Mục tiêu Sử dụng thư viện Groq (ở đây là ChatGroq / init_chat_model với model_provider=\u0026quot;groq\u0026quot;) để gọi model OpenAI (Groq-hosted).\nCấu hình trong Code Trong code demo:\nLấy API Key từ Environment GROQ_API_KEY = os.getenv(\u0026#34;GROQ_API_KEY\u0026#34;) Biến GROQ_API_KEY lấy API key từ environment variable.\nKhởi tạo Model llm = init_chat_model( model=\u0026#34;openai/gpt-oss-20b\u0026#34;, model_provider=\u0026#34;groq\u0026#34;, api_key=GROQ_API_KEY ) Tích hợp vào Agent Agent gọi LLM thông qua create_agent(...) với tham số model=llm:\nagent = create_agent( model=llm, tools=tools, checkpointer=checkpointer, store=store, middleware=[MemoryMiddleware()], system_prompt=system_prompt, ) Luồng xử lý Agent → Groq API → Model Inference → Response "},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/2-proposal/","title":"Proposal","tags":[],"description":"","content":"APT Magic Nền tảng AI Serverless cho tạo ảnh cá nhân hóa và tương tác xã hội 1. Tóm tắt điều hành APT Magic là một ứng dụng web serverless sử dụng AI, cho phép người dùng tạo, tùy chỉnh và chia sẻ nội dung nghệ thuật như hình ảnh được tạo bởi mô hình AI. Nền tảng tích hợp trực tiếp với các foundation models thông qua Amazon Bedrock, đồng thời cung cấp trải nghiệm web mượt mà nhờ Next.js (SSR) được triển khai bằng AWS Amplify.\nPhiên bản MVP tập trung vào việc tạo ảnh theo thời gian thực và chia sẻ nội dung, trong khi Thiết kế Tương Lai hướng đến khả năng mở rộng với SageMaker Inference, Step Functions, và quy trình AWS MLOps để tự động hóa quản lý mô hình và điều phối toàn hệ thống.\nAPT Magic hiện được phát triển với kiến trúc AWS-native hiện đại, tiết kiệm chi phí và an toàn, phù hợp cho lượng người dùng nhỏ đến trung bình; sau đó sẽ mở rộng thành nền tảng AI quy mô doanh nghiệp.\n2. Vấn đề đặt ra Thách thức là gì? Hầu hết các nền tảng tạo ảnh AI đều tốn kém, phụ thuộc vào API của bên thứ ba và thiếu khả năng tùy chỉnh sâu.\nNhà phát triển và người sáng tạo thường gặp vấn đề về độ trễ, thiếu minh bạch trong quản lý mô hình, và ít quyền kiểm soát dữ liệu người dùng.\nGiải pháp APT Magic sử dụng kiến trúc serverless của AWS để cung cấp:\nTạo ảnh AI thời gian thực với mô hình Amazon Bedrock – Stability AI. Xác thực người dùng và quản lý nội dung bảo mật với Amazon Cognito và DynamoDB. Điều phối API linh hoạt bằng AWS Lambda và API Gateway. Phân phối nội dung tốc độ cao với CloudFront CDN kết hợp WAF để bảo vệ. Các nâng cấp tương lai sẽ bao gồm Step Functions, SQS/SNS, SageMaker Inference, và CI/CD tiết kiệm chi phí với CodeBuild, CodePipeline, CloudFormation — biến APT Magic thành một nền tảng MLOps tự động hóa toàn diện.\n3. Kiến trúc giải pháp Kiến trúc MVP MVP được xây dựng theo hướng hoàn toàn serverless, tập trung vào khả năng mở rộng, dễ bảo trì và tối ưu chi phí.\nCác dịch vụ AWS chủ chốt:\nRoute53 + CloudFront + WAF — đảm bảo truy cập toàn cầu an toàn và cache hiệu quả. Amplify (Next.js SSR) — triển khai giao diện và tầng server-side rendering. API Gateway + Lambda Functions — xử lý backend (image processing, subscription, post APIs). Amazon Cognito — Xác thực và phân quyền người dùng. Amazon S3 + DynamoDB — Lưu trữ dữ liệu và hình ảnh. Amazon Bedrock — Tích hợp mô hình tạo ảnh (Stability AI). Secrets Manager, CloudWatch, CloudTrail — Bảo mật, giám sát và ghi log. Bảo mật\nPrivateLink cho kết nối an toàn giữa Lambda và backend services. WAF + IAM để lọc truy cập và kiểm soát quyền chi tiết. Thiết kế tương lai (Kiến trúc nâng cao) Ở giai đoạn tiếp theo, APT Magic sẽ phát triển thành nền tảng điều phối AI, bổ sung các lớp tự động hóa, độ bền hệ thống và quản lý vòng đời mô hình.\nCác dịch vụ sẽ được bổ sung:\nAWS Step Functions — điều phối workflow bất đồng bộ như:\nTạo ảnh nhiều bước (kiểm tra prompt → inference → upload kết quả). Xác nhận thanh toán → xử lý mô hình → gửi thông báo. Amazon SQS — truyền tải thông điệp giữa các Lambda async.\nAmazon SNS — gửi thông báo thời gian thực cho người dùng hoặc admin.\nAmazon ElastiCache (Redis) — caching và hạn chế tốc độ request.\nAmazon SageMaker Inference — triển khai các mô hình tùy chỉnh, fine-tuned.\nAWS CodePipeline + SageMaker Pipelines — tự động hóa toàn bộ MLOps.\nAWS PrivateLink + VPC Endpoints — đảm bảo dữ liệu đi trong mạng riêng.\nAWS WAF \u0026amp; Shield Advanced — bảo vệ khỏi DDoS và nâng cao an ninh.\nCI/CD + MLOps\nCodePipeline + CodeBuild + CloudFormation để triển khai hạ tầng tự động. 4. Triển khai kỹ thuật Các giai đoạn triển khai Giai đoạn 1 – MVP (Hiện tại / Đã hoàn thành)\nTriển khai Amplify (Next.js SSR) + API Gateway + Lambda. Tích hợp API Stability AI qua Bedrock. Thiết lập CI/CD bằng CodePipeline + CloudFormation. Kích hoạt Cognito cho user auth và S3 + DynamoDB để lưu trữ. Giai đoạn 2 – Mở rộng theo thiết kế tương lai\nThêm Step Functions + SQS/SNS để quản lý workflow AI bất đồng bộ. Tích hợp ElastiCache để caching và rate limiting. Kết nối SageMaker Inference cho các mô hình tùy chỉnh. Dùng SageMaker Pipelines để tự động training và deploy. Tăng cường bảo mật với Shield Advanced + GuardDuty + PrivateLink. Kết nối GitLab Runner với CodeBuild để hợp nhất CI/CD. 5. Timeline \u0026amp; Milestones Giai đoạn Mô tả Thời gian ước tính Mốc triển khai (Milestone) Tháng 1: Thiết lập \u0026amp; Core API Triển khai hạ tầng IaC, Cognito, API Gateway, DynamoDB, và các hàm Lambda cơ bản. 4 Tuần Core Backend hoạt động, Auth/User Management hoàn thành. Tháng 2: AI \u0026amp; Payment Integration Tích hợp LLM Claude Haiku 3 Amazon Bedrock (Stability AI), Replicate API hoàn thiện hàm Image Processing và tích hợp cổng thanh toán bên thứ ba. 4 Tuần Demo xử lý ảnh AI đầu cuối (end-to-end) thành công. Tháng 3: Front-end \u0026amp; CI/CD Phát triển UI/UX (Amplify/Next.js), hoàn thiện pipeline CI/CD, và cấu hình Giám sát/Bảo mật (CloudWatch/WAF). 4 Tuần Nền tảng hoàn chỉnh, sẵn sàng cho kiểm thử người dùng. Tháng 4: Tối ưu \u0026amp; Go-Live Kiểm thử hiệu năng (Stress Test), tối ưu chi phí, và triển khai Production. 4 Tuần Go-Live (Sản phẩm chính thức ra mắt). 6. Ước tính chi phí (AWS Pricing Estimate) Tổng Chi Phí Hàng tháng: $9.80 Trả trước: $0.00 12 tháng: $117.60 Tổng quan dịch vụ Dịch vụ Khu vực Chi phí tháng Trả trước Chi phí 12 tháng Ghi chú Amazon Route 53 Asia Pacific (Singapore) $0.50 $0.00 $6.00 1 Hosted Zone, 1 domain, 1 VPC liên kết Amazon CloudFront Asia Pacific (Singapore) $0.00 $0.00 $0.00 Không có cấu hình cụ thể AWS WAF Asia Pacific (Singapore) $6.00 $0.00 $72.00 1 Web ACL; 1 rule mỗi ACL AWS Amplify Asia Pacific (Singapore) $0.00 $0.00 $0.00 Build instance: Standard (8GB/4vCPU); thời lượng request 500ms AWS CloudFormation Asia Pacific (Singapore) $0.00 $0.00 $0.00 Không có extension; không có thao tác Amazon API Gateway Asia Pacific (Singapore) $0.13 $0.00 $1.59 10k requests/tháng; message WebSocket 1KB; request size 30KB AWS Lambda Asia Pacific (Singapore) $1.67 $0.00 $20.04 1 triệu invokes; x86; 512MB ephemeral storage Amazon CloudWatch Asia Pacific (Singapore) $0.85 $0.00 $10.22 1 metric; 0.5GB logs in; 0.5GB logs tới S3 S3 Standard Asia Pacific (Singapore) $0.23 $0.00 $2.76 10GB storage; 20k PUT; 40k GET DynamoDB On-Demand Asia Pacific (Singapore) $0.42 $0.00 $5.04 1GB storage; item 1KB; chế độ on-demand Tổng (Ước tính) — $9.80 $0.00 $117.60 Theo AWS Pricing Calculator Metadata Tiền tệ: USD Locale: en_US Ngày tạo: 12/9/2025 Share URL: AWS Calculator Link Lưu ý pháp lý: AWS Pricing Calculator chỉ cung cấp ước tính; chi phí thực tế có thể thay đổi theo mức sử dụng. Bảng Giá Mô Hình AI Mô hình Độ phân giải / Sử dụng Token Chất lượng Giá cho mỗi yêu cầu (USD) Ghi chú Titan Image Generator v2 \u0026lt; 512×512 Tiêu chuẩn 0.008 Giá cố định cho 1 ảnh Titan Image Generator v2 \u0026lt; 512×512 Cao cấp 0.01 Giá cố định cho 1 ảnh Titan Image Generator v2 \u0026gt; 1024×1024 Tiêu chuẩn 0.01 Giá cố định cho 1 ảnh Titan Image Generator v2 \u0026gt; 1024×1024 Cao cấp 0.012 Giá cố định cho 1 ảnh Stable Diffusion 3.5 Large Bất kỳ N/A 0.08 Giá cố định cho 1 ảnh Claude (text + image) 40 token đầu vào + 1 ảnh N/A 0.00195 Giá cho 1 yêu cầu gồm văn bản và 1 ảnh 1024×1024 Tùy chọn bổ sung Chế độ Tăng cường Giá (USD) text→img không tăng cường 0.08 text→img có tăng cường 0.08195 img→img không tăng cường 0.012 img→img có tăng cường 0.094 7. Đánh giá rủi ro Rủi ro Mức độ ảnh hưởng Khả năng xảy ra Giảm thiểu Độ trễ khi gọi mô hình AI Trung bình Cao Dùng ElastiCache + Step Functions Chi phí tăng vì inference Cao Trung bình Kiểm soát Bedrock usage, autoscaling SageMaker Lỗi cấu hình CI/CD Trung bình Thấp Áp dụng rollback của CloudFormation Lỗ hổng bảo mật Cao Trung bình WAF, GuardDuty, PrivateLink, IAM least privilege Phụ thuộc API bên thứ ba Trung bình Trung bình Dùng fallback inference từ S3 8. Giá trị đạt được Lợi ích kỹ thuật: Hoàn thiện pipeline serverless cho tạo ảnh AI. Nền tảng điều phối dễ mở rộng, phù hợp tích hợp MLOps. Cải thiện độ trễ và tính ổn định nhờ caching và workflows async. Giá trị dài hạn: Nền tảng mở rộng lên mô hình AI as a Service. Khung MLOps sẵn sàng cho tự động hóa training/retraining. Hạ tầng tái sử dụng cho nhiều sản phẩm AI trong tương lai. "},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/1-worklog/1.10-week10/","title":"Worklog Tuần 10","tags":[],"description":"","content":" Mục tiêu tuần 10: Phần 1 – Xây dựng backend Document Management System (DMS) với Lambda và DynamoDB Nắm được kiến trúc hoạt động của một hệ thống quản lý tài liệu. Thiết kế bảng DynamoDB dựa trên nhu cầu truy cập thực tế của ứng dụng. Viết các Lambda Function đảm nhiệm việc tạo mới, lấy danh sách, truy vấn chi tiết và xoá tài liệu. Cấu hình IAM Role cho Lambda theo nguyên tắc tối thiểu quyền. Thực hiện kiểm thử thông qua test event và giám sát bằng CloudWatch Logs. Chuẩn bị phần backend để tuần sau tích hợp thêm API Gateway và Amplify. Phần 2 – Hiểu cách DMS sẽ được mở rộng trong những tuần kế tiếp Nắm rõ vai trò của Cognito, Amplify Storage trong hệ thống. Biết cách DynamoDB Streams được dùng để đồng bộ dữ liệu sang OpenSearch phục vụ tìm kiếm. Hình dung cách toàn bộ backend sẽ được triển khai bằng AWS SAM. Hiểu luồng CI/CD dự kiến vận hành trên AWS CodePipeline. Các công việc đã thực hiện trong tuần Thứ Nội dung triển khai Bắt đầu Hoàn thành Nguồn 2 Phân tích toàn bộ kiến trúc DMS\n• Làm rõ mục tiêu: upload, xem, tải về, xoá và tìm kiếm tài liệu.\n• Xác định các dịch vụ chính: Lambda, DynamoDB, S3, Cognito, API Gateway.\n• Phân tích luồng xử lý từ lúc upload đến khi lưu metadata và phục vụ tìm kiếm.\n• Xây dựng danh sách API cần có cho backend. 10/11/2025 10/11/2025 https://cloudjourney.awsstudygroup.com/ 3 Thiết kế bảng DynamoDB dùng cho metadata tài liệu\n• Tạo bảng Documents (PK: userId, SK: documentId).\n• Xác định các attributes như filename, fileType, size, tags, createdAt, updatedAt.\n• Phân tích access patterns: truy vấn theo user, lấy chi tiết theo documentId, lọc theo tag.\n• Chuẩn bị data mẫu và thử nghiệm bằng Console/CLI. 11/11/2025 11/11/2025 https://cloudjourney.awsstudygroup.com/ 4 Tạo IAM Role dành cho Lambda\n• Khởi tạo role DMSLambdaRole.\n• Cấp quyền DynamoDB CRUD (GetItem, Query, PutItem, DeleteItem).\n• Cho phép ghi log vào CloudWatch Logs.\n• Rà lại trust policy cho Lambda.\n• Xác nhận role hoạt động đúng bằng cách test trực tiếp trong Lambda. 12/11/2025 12/11/2025 https://cloudjourney.awsstudygroup.com/ 5 Viết Lambda CreateDocument\n• Kiểm tra input do người dùng gửi lên.\n• Sinh documentId dạng UUID.\n• Lưu metadata vào bảng thông qua put_item.\n• Ghi log để phục vụ theo dõi.\n• Kiểm thử qua test event và CloudWatch Logs.\n• Xử lý lỗi nhập thiếu, lỗi DynamoDB hoặc lỗi bất thường. 13/11/2025 13/11/2025 https://cloudjourney.awsstudygroup.com/ 6 Viết Lambda GetDocuments và GetDocumentById\n• Viết logic Query toàn bộ tài liệu theo userId.\n• Thêm khả năng phân trang bằng LastEvaluatedKey.\n• Viết hàm lấy chi tiết một document theo documentId.\n• Chuẩn hoá JSON trả về để khớp với front-end.\n• Kiểm thử bằng dữ liệu thật trong DynamoDB. 14/11/2025 14/11/2025 https://cloudjourney.awsstudygroup.com/ 7 Viết Lambda DeleteDocument và cleanup dữ liệu\n• Kiểm tra sự tồn tại của tài liệu trước khi xoá.\n• Xoá metadata bằng delete_item.\n• Chuẩn bị logic xoá file S3 (sẽ làm trong tuần tới).\n• Ghi log chi tiết quá trình xoá.\n• Dọn dẹp các bản ghi test không còn dùng. 15/11/2025 15/11/2025 https://cloudjourney.awsstudygroup.com/ Kết quả đạt được trong tuần 10 1. Hoàn chỉnh bảng DynamoDB với mô hình tối ưu Xây dựng bảng Documents đáp ứng trọn nhu cầu lưu metadata của DMS. Thiết kế khóa phân vùng/sắp xếp phù hợp với việc truy vấn theo user và lấy chi tiết tài liệu. Cấu trúc dữ liệu dễ mở rộng sang DynamoDB Streams hoặc tích hợp OpenSearch. 2. Hoàn thành bộ Lambda CRUD cho hệ thống Tạo tài liệu mới bằng CreateDocument. Lấy danh sách tài liệu theo userId qua GetDocuments. Truy vấn chi tiết bằng GetDocumentById. Xoá tài liệu bằng DeleteDocument. Toàn bộ logic theo hướng serverless, log rõ ràng và có xử lý lại khi gặp lỗi. 3. Triển khai đúng các best practices về bảo mật và giám sát IAM Role được cấu hình theo nguyên tắc “ít quyền nhất”. CloudWatch Logs phục vụ theo dõi toàn bộ quá trình xử lý. Bộ xử lý lỗi đủ rõ ràng: lỗi input, lỗi DynamoDB hoặc bản ghi không tồn tại. 4. Hiểu đầy đủ kiến trúc tổng thể của hệ thống DMS Nắm được mối liên kết giữa Authentication – API – Storage – Metadata. Biết cách Lambda sẽ được tích hợp vào API Gateway ở tuần kế. Có nền tảng kỹ thuật để tuần sau tiếp tục với Amplify Authentication và Storage. Tóm tắt kiến thức trong tuần AWS DynamoDB Thiết kế bảng theo access patterns. Dùng đúng Partition/Sort Key. Thành thạo các thao tác PutItem, GetItem, Query, DeleteItem (Boto3). AWS Lambda Viết hàm serverless cho CRUD. Tạo IAM Role cùng policy phù hợp. Làm quen với logging và giám sát bằng CloudWatch. Xử lý lỗi theo chuẩn dùng trong sản phẩm. Kiến trúc DMS Metadata nằm trong DynamoDB. File thực được lưu trên S3. Xác thực người dùng bằng Cognito. API định tuyến qua API Gateway và Lambda. "},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/1-worklog/1.11-week11/","title":"Worklog Tuần 11","tags":[],"description":"","content":" Mục tiêu tuần 11: Phần 1 – Serverless với Amplify Authentication và Storage Tìm hiểu cách hoạt động của Amplify Libraries, đặc biệt là Amplify Auth và Storage. Kết nối ứng dụng web với Cognito để quản lý người dùng. Xây dựng chức năng upload từ frontend và lưu file trực tiếp vào S3 thông qua Amplify Storage. Hiểu rõ ba chế độ truy cập file: public, protected và private. Nắm được cơ chế giao tiếp giữa Cognito – S3 – Amplify trong một ứng dụng serverless. Phần 2 – Serverless: Xây dựng Frontend giao tiếp với API Gateway Hiểu cách tạo API Gateway và ánh xạ request đến Lambda. Viết frontend (JavaScript/React) để gọi API Gateway → Lambda → DynamoDB. Kiểm thử API bằng Postman và từ chính frontend. Nắm chắc quy trình toàn bộ đường đi của dữ liệu:\nFrontend → API Gateway → Lambda → DynamoDB. Các công việc đã thực hiện trong tuần Thứ Nội dung Bắt đầu Hoàn thành Nguồn 2 Khởi động Amplify và chuẩn bị môi trường\n• Tìm hiểu Amplify Libraries, CLI và UI Components.\n• Khởi tạo project tích hợp Amplify.\n• Cài đặt aws-amplify và @aws-amplify/ui.\n• Cấu hình AWS trong local. 17/11/2025 17/11/2025 https://cloudjourney.awsstudygroup.com/ 3 Xác thực người dùng với Amplify Auth\n• Tạo User Pool và Identity Pool.\n• Thiết lập cấu hình Amplify Auth trong frontend.\n• Sử dụng Amplify UI Components để tạo giao diện login.\n• Thử luồng tạo tài khoản, đăng nhập, xác nhận mã và đổi mật khẩu. 18/11/2025 18/11/2025 https://cloudjourney.awsstudygroup.com/ 4 Lưu trữ file qua Amplify Storage\n• Tạo S3 bucket bằng Amplify.\n• Cài đặt quyền truy cập public/protected/private.\n• Viết chức năng upload từ frontend lên S3.\n• Kiểm tra lại trên S3 console.\n• Viết logic list file và lấy URL. 19/11/2025 19/11/2025 https://cloudjourney.awsstudygroup.com/ 5 Tạo API Gateway kết nối Lambda\n• Xây dựng REST API.\n• Gắn các route CRUD vào Lambda.\n• Bật CORS cho frontend.\n• Cập nhật ARN mới cho Lambda functions. 20/11/2025 20/11/2025 https://cloudjourney.awsstudygroup.com/ 6 Kiểm thử API bằng Postman\n• Gửi request với các method GET/POST/DELETE.\n• Đính kèm token Cognito trong header.\n• Theo dõi log bằng CloudWatch.\n• Fix lỗi CORS và quyền IAM. 21/11/2025 21/11/2025 https://cloudjourney.awsstudygroup.com/ 7 Tích hợp frontend với API Gateway\n• Viết service JS gọi API Gateway.\n• Tự động gắn token Cognito.\n• Hiển thị dữ liệu trả về từ Lambda/DynamoDB.\n• Hoàn thiện quy trình end-to-end: Frontend → API → Lambda → DynamoDB.\n• Dọn dữ liệu test. 22/11/2025 22/11/2025 https://cloudjourney.awsstudygroup.com/ Kết quả đạt được trong tuần 11 1. Thiết lập thành công Authentication bằng Amplify + Cognito Hoàn thiện đầy đủ các luồng: đăng nhập, đăng ký, xác thực bằng mã và đặt lại mật khẩu. Frontend lấy được JWT token để sử dụng khi gọi API Gateway. Hiểu rõ cách Cognito quản lý danh tính qua User Pool và Identity Pool. 2. Lưu trữ và quản lý file bằng Amplify Storage + S3 Upload file trực tiếp từ frontend lên S3 thành công. Bucket S3 được tạo tự động bởi Amplify. Thử nghiệm đầy đủ 3 loại quyền truy cập. Hoàn thiện các hàm xử lý file: upload, list, get URL, và tùy chọn delete. 3. Xây dựng API Gateway phục vụ hệ thống DMS Tạo REST API hoàn chỉnh với các route CRUD. Kết nối trực tiếp đến Lambda của tuần trước. Cấu hình CORS chuẩn cho frontend. Gửi request kèm JWT token và kiểm thử toàn bộ bằng Postman. 4. Tích hợp frontend với API Gateway Viết logic frontend gửi request kèm token Cognito. Hiển thị danh sách metadata từ DynamoDB. Hoàn thiện luồng tổng thể: Tóm tắt kiến thức trong tuần Amplify Amplify Auth sử dụng Cognito làm hệ thống xác thực. Amplify Storage hỗ trợ thao tác S3 đơn giản từ frontend. Bộ UI component giúp tạo trang login nhanh. Cognito (Authentication) User Pool: quản lý tài khoản. Identity Pool: cấp quyền truy cập tài nguyên AWS. JWT token dùng khi gửi request đến API Gateway. S3 (Storage) Hỗ trợ phân quyền file theo public / protected / private. Upload trực tiếp từ client thông qua Amplify. API Gateway Điều phối request đến Lambda. CORS rất quan trọng để frontend hoạt động. Có thể yêu cầu xác thực bằng token Cognito. Lambda + DynamoDB Lambda thực thi các hàm CRUD. DynamoDB lưu metadata tài liệu. Frontend chỉ giao tiếp qua API Gateway. "},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/1-worklog/1.12-week12/","title":"Worklog Tuần 12","tags":[],"description":"","content":"Mục tiêu trong tuần Phần 1 – Giới thiệu Amazon Bedrock \u0026amp; Foundation Models\nNắm được cách Amazon Bedrock được thiết kế và các thành phần vận hành chính. Tìm hiểu phương thức truy cập và sử dụng các foundation models thông qua Bedrock. Khảo sát các mô hình phổ biến như Claude, Llama, Mistral, Amazon Titan… Hiểu rõ sự khác nhau giữa mô hình sinh văn bản, mô hình embeddings và mô hình đa phương thức. Phần 2 – Xây dựng AI Agent với Bedrock Agent\nHiểu khái niệm AI Agent và cơ chế hoạt động của nó.\nNắm được các thành phần:\nAction groups Knowledge bases Orchestration Tích hợp Lambda functions Thiết lập một AI Agent có khả năng gọi và sử dụng công cụ thông qua Lambda.\nPhần 3 – Sử dụng Knowledge Base cho Enterprise Search\nTạo một Knowledge Base bằng vector embeddings. Liên kết Bedrock KB với dữ liệu lưu trong S3. Kiểm thử khả năng tìm kiếm ngữ nghĩa và cơ chế RAG (Retrieval Augmented Generation). Hoàn tất tích hợp KB vào Bedrock Agent. Phần 4 – Tích hợp Front-end với Bedrock Agent\nXây dựng giao diện chat để tương tác trực tiếp với Agent. Thực hiện truyền dữ liệu dạng streaming để hiển thị phản hồi theo thời gian thực. Mô tả luồng xử lý: Người dùng nhập → Agent → (KB, Lambda) → Kết quả trả về. Đưa front-end lên môi trường Amplify Hosting. Các công việc triển khai trong tuần Thứ Công việc chi tiết Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 Tìm hiểu nền tảng Bedrock\n• Khám phá danh sách mô hình.\n• Thử Claude / Llama 3 trong Playground.\n• Đối chiếu chi phí và năng lực xử lý.\n• Nắm API Bedrock cơ bản. 24/11/2025 24/11/2025 https://cloudjourney.awsstudygroup.com/ 3 Phát triển ứng dụng Text Generation đơn giản\n• Viết Lambda gọi InvokeModel API.\n• Thử nghiệm temperature và max tokens.\n• Tạo CLI nhỏ bằng SDK. 25/11/2025 25/11/2025 https://cloudjourney.awsstudygroup.com/ 4 Thiết lập Bedrock Agent\n• Cấu hình agent.\n• Tạo hướng dẫn và guardrails.\n• Tạo Action Group dùng Lambda.\n• Kết nối agent với DynamoDB mẫu. 26/11/2025 26/11/2025 https://cloudjourney.awsstudygroup.com/ 5 Xây dựng hệ thống Knowledge Base (RAG)\n• Tạo bucket S3 chứa tài liệu.\n• Thiết lập embeddings (Titan Embeddings G1).\n• Kiểm thử Q\u0026amp;A với PDF/CSV.\n• Tích hợp KB vào Agent. 27/11/2025 27/11/2025 https://cloudjourney.awsstudygroup.com/ 6 Kiểm thử toàn bộ Agent\n• Thử chuỗi hoạt động: Agent → KB → Lambda.\n• Khắc phục lỗi IAM, KMS.\n• Test nhiều lượt hội thoại.\n• Bổ sung xử lý lỗi và fallback. 28/11/2025 28/11/2025 https://cloudjourney.awsstudygroup.com/ 7 Phát triển giao diện Chat bằng Amplify\n• Tạo UI React Chat.\n• Kết nối với Agent qua API Gateway.\n• Thử streaming output.\n• Triển khai bằng Amplify Hosting. 29/11/2025 29/11/2025 https://cloudjourney.awsstudygroup.com/ Kết quả đạt được trong tuần 12 1. Kiến thức về Amazon Bedrock \u0026amp; Foundation Models\nBiết cách gọi mô hình thông qua SDK Bedrock. Thực hành với nhiều nhóm mô hình khác nhau. Hiểu các tình huống ứng dụng: tạo văn bản, tóm tắt, sinh mã, embeddings. 2. Hoàn thiện một AI Agent có khả năng chạy thực tế\nAgent nhận diện được mục đích người dùng. Sử dụng Action Groups để kích hoạt Lambda. Hỗ trợ suy luận theo nhiều bước. Truy vấn DynamoDB thông qua Lambda. 3. Xây dựng Knowledge Base phục vụ RAG\nTạo kho vector từ dữ liệu trong S3. Thực hiện tìm kiếm ngữ nghĩa thành công. Agent cải thiện chất lượng trả lời khi tích hợp KB. 4. Tạo giao diện chat hoạt động với Bedrock\nXây dựng UI chat bằng React. Kết nối qua API Gateway để đảm bảo an toàn. Hoạt động tốt với phản hồi dạng streaming. Quy trình đầy đủ: UI → Agent → KB/Lambda → Phản hồi. Tóm tắt kiến thức đã học Amazon Bedrock\nCung cấp truy cập tới nhiều mô hình mạnh mẽ. Hệ thống được quản lý hoàn toàn và đạt chuẩn bảo mật doanh nghiệp. Hỗ trợ mô hình sinh văn bản, embeddings và đa phương thức. Bedrock Agents\nCó khả năng duy trì hội thoại, xử lý logic và gọi công cụ. Action Groups cho phép kết nối backend thực tế. Guardrails giúp kiểm soát an toàn nội dung. Knowledge Bases\nEmbeddings nâng cao khả năng tìm kiếm ngữ nghĩa. Kết hợp KB và Agent tạo thành một hệ thống RAG hoàn chỉnh. Hỗ trợ tự động cập nhật tài liệu từ S3. Tích hợp \u0026amp; Triển khai\nLambda thực thi logic phía backend. API Gateway đảm bảo kết nối an toàn cho front-end. Amplify hỗ trợ triển khai giao diện nhanh chóng. "},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/3-blogstranslated/","title":"Các bài blogs đã dịch","tags":[],"description":"","content":"Blog 1 - AWS named as a Leader in 2025 Gartner Magic Quadrant for Cloud-Native Application Platforms and Container Management Blog này thông báo rằng AWS được Gartner vinh danh là “Leader” trong Magic Quadrant 2025 cho hai hạng mục: Cloud-Native Application Platforms và Container Management. Bài viết nhấn mạnh khả năng thực thi mạnh mẽ và tầm nhìn toàn diện của AWS trong việc hỗ trợ doanh nghiệp xây dựng, triển khai và quản lý ứng dụng cloud-native. AWS cung cấp danh mục dịch vụ phong phú như AWS Lambda, App Runner, Amazon ECS, Amazon EKS và Fargate, giúp khách hàng phát triển ứng dụng linh hoạt, mở rộng dễ dàng và vận hành hiệu quả trên cả môi trường đám mây lẫn hybrid. Blog cũng khẳng định cam kết của AWS trong việc tiếp tục đổi mới, cung cấp công cụ AI/ML và giải pháp container hiện đại để đáp ứng nhu cầu đa dạng của khách hàng toàn cầu.\nBlog 2 - Automate and orchestrate Amazon EMR jobs using AWS Step Functions and Amazon EventBridge Blog này hướng dẫn cách xây dựng pipeline xử lý dữ liệu Apache Spark trên Amazon EMR hoàn toàn tự động và tiết kiệm chi phí bằng cách kết hợp AWS Step Functions và Amazon EventBridge. Giải pháp này cho phép bạn tự động provision cụm EMR tạm thời trên EC2, chạy job Spark, lưu kết quả vào S3 và tự động xóa cụm sau khi hoàn thành, giúp giảm chi phí vận hành và loại bỏ thao tác thủ công. Bài viết minh họa quy trình qua ví dụ xử lý dữ liệu COVID-19 công khai, bao gồm việc tính toán các chỉ số sử dụng giường bệnh và ICU theo tháng cho từng bang. Ngoài ra, hướng dẫn còn trình bày chi tiết cách triển khai hạ tầng bằng AWS CloudFormation, cấu hình EventBridge schedule, giám sát workflow qua Step Functions, theo dõi CloudWatch logs, và dọn dẹp tài nguyên sau khi chạy. Kiến trúc này đặc biệt phù hợp cho các workload định kỳ như ETL, phân tích batch, hoặc báo cáo tuân thủ — nơi yêu cầu kiểm soát chi tiết hạ tầng, bảo mật cao và tối ưu chi phí.\nBlog 3 - Streamline Spark application development on Amazon EMR with the Data Solutions Framework on AWS Blog này giới thiệu cách tinh giản toàn bộ quy trình phát triển ứng dụng Apache Spark trên Amazon EMR bằng cách kết hợp AWS Cloud Development Kit (CDK), Data Solutions Framework (DSF) và Amazon EMR Toolkit for VS Code. Bạn sẽ tìm hiểu cách thiết lập môi trường phát triển cục bộ (local) nhất quán với môi trường sản xuất, triển khai hạ tầng Spark serverless dưới dạng mã (IaC), và xây dựng pipeline CI/CD tự động cho việc kiểm thử và triển khai đa môi trường. Bài viết hướng dẫn chi tiết cách đóng gói ứng dụng PySpark thành artifact có thể triển khai trên EMR Serverless, tích hợp kiểm thử tự động bằng Pytest, và sử dụng CDK Pipelines self-mutating để tối ưu quá trình cập nhật. Giải pháp giúp developer kiểm soát toàn bộ code và hạ tầng, giảm phụ thuộc giữa các nhóm, tăng tốc chu kỳ phát triển, và tối ưu hiệu năng Spark workload trên AWS.\n"},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/5-workshop/5.3-architecture/5.3.3-chunking/","title":"Chunking &amp; Embedding","tags":[],"description":"","content":"Lý do Chunking Tài liệu/FAQ thường dài; để tính embedding hiệu quả và tối ưu truy vấn tương đồng, cần chia văn bản lớn thành các đoạn nhỏ (chunk).\nLợi ích Giảm loss of context khi embed Retrieval chính xác hơn bằng vector similarity Tối ưu hiệu suất khi tìm kiếm Phù hợp với giới hạn token của model embedding Chiến lược Chunking Trong code sử dụng RecursiveCharacterTextSplitter:\nsplitter = RecursiveCharacterTextSplitter( chunk_size=500, chunk_overlap=0 ) chunks = splitter.split_documents(docs) Tham số Chunking Tham số Giá trị Ý nghĩa chunk_size 500 Kích thước mỗi chunk (characters) chunk_overlap 0 Không có phần chồng lấn giữa các chunk Gợi ý Tối ưu Kích thước chunk hợp lý: 500–1000 tokens (tùy model embedding) Chunk overlap: Nếu cần context liên tục, set overlap 50-100 characters Trade-off: Chunk nhỏ → chính xác cao nhưng nhiều vectors; Chunk lớn → ít vectors nhưng có thể mất ngữ cảnh Tạo Embedding và Vector Store Khởi tạo Embedding Model emb = HuggingFaceEmbeddings( model_name=\u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; ) Model được chọn: all-MiniLM-L6-v2\nLightweight và nhanh Phù hợp cho tiếng Anh Kích thước embedding: 384 dimensions Tạo FAISS Vector Store faq_store = FAISS.from_documents(chunks, emb) FAISS (Facebook AI Similarity Search) cung cấp:\nTìm kiếm vector nhanh chóng Hiệu quả với datasets lớn Hỗ trợ nhiều thuật toán index Truy vấn Vector Store results = faq_store.similarity_search(query, k=3) Tham số:\nquery: Câu hỏi người dùng k=3: Trả về top 3 chunks có độ tương đồng cao nhất Ghi chú Quan trọng Cập nhật Dữ liệu Nếu dữ liệu thay đổi (add/update documents), cần: Re-embed toàn bộ hoặc Incremental update vector store Chọn Embedding Model Trade-off cần cân nhắc:\nTiêu chí Lightweight Model Heavy Model Tốc độ Nhanh Chậm Độ chính xác Tốt Rất tốt Chi phí Thấp Cao Use case FAQ, chatbot Research, legal Model tiếng Việt Nếu cần hỗ trợ tiếng Việt tốt hơn:\nkeepitreal/vietnamese-sbert sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 Code Hoàn chỉnh from langchain_text_splitters import RecursiveCharacterTextSplitter from langchain_huggingface import HuggingFaceEmbeddings from langchain_community.vectorstores import FAISS # Load documents docs = load_faq_csv() # Chunking splitter = RecursiveCharacterTextSplitter( chunk_size=500, chunk_overlap=0 ) chunks = splitter.split_documents(docs) # Embedding emb = HuggingFaceEmbeddings( model_name=\u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; ) # Vector Store faq_store = FAISS.from_documents(chunks, emb) # Query query = \u0026#34;Làm thế nào để đổi mật khẩu?\u0026#34; results = faq_store.similarity_search(query, k=3) Checklist Triển khai Chuẩn bị documents (CSV, JSON, text files) Chọn chunk_size phù hợp (test với 500, 750, 1000) Chọn embedding model (tiếng Anh hoặc đa ngôn ngữ) Tạo và lưu FAISS index Test retrieval với các query mẫu Monitor và điều chỉnh k value "},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/5-workshop/5.3-architecture/","title":"Kiến trúc mô hình Rag triển khai trên AWS Agent core","tags":[],"description":"","content":"Sử dụng Gateway endpoint Trong phần này, chúng ta sẽ tìm hiểu cách tích hợp Groq để gọi model OpenAI-compatible và cách chunking dữ liệu cho RAG.\n"},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/4-eventparticipated/","title":"Các events đã tham gia","tags":[],"description":"","content":" ⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nTrong phần này, các bạn cần liệt kê và mô tả chi tiết các sự kiện (event) mà mình đã tham gia trong suốt quá trình thực tập hoặc làm việc.\nMỗi sự kiện nên được trình bày theo định dạng Event 1, Event 2, Event 3…, kèm theo các thông tin:\nTên sự kiện Thời gian tổ chức Địa điểm (nếu có) Vai trò của bạn trong sự kiện (người tham dự, hỗ trợ tổ chức, diễn giả, v.v.) Mô tả ngắn gọn nội dung và hoạt động chính trong sự kiện Kết quả hoặc giá trị đạt được (bài học, kỹ năng mới, đóng góp cho nhóm/dự án) Việc liệt kê này giúp thể hiện rõ sự tham gia thực tế của bạn, cũng như các kỹ năng mềm và kinh nghiệm bạn đã tích lũy qua từng sự kiện. Trong quá trình thực tập, em đã tham gia 2 events, với mỗi event là một trải nghiệm đáng nhớ với những kiến thức mới, hay và bổ ích, cùng với đó là nhứng món quà và những khoảnh khắc rất tuyệt vời.\nEvent 1 Tên sự kiện: GenAI-powered App-DB Modernization workshop\nThời gian: 09:00 ngày 13/08/2025\nĐịa điểm: Tầng 26, tòa nhà Bitexco, số 02 đường Hải Triều, phường Sài Gòn, thành phố Hồ Chí Minh\nVai trò trong sự kiện: Người tham dự\nEvent 2 Tên sự kiện: GenAI-powered App-DB Modernization workshop\nThời gian: 09:00 ngày 13/08/2025\nĐịa điểm: Tầng 26, tòa nhà Bitexco, số 02 đường Hải Triều, phường Sài Gòn, thành phố Hồ Chí Minh\nVai trò trong sự kiện: Người tham dự\n"},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/5-workshop/5.4-agent-core-run/","title":"Run Agent Core","tags":[],"description":"","content":"Tổng quan Trong phần này, bạn sẽ học cách triển khai và gọi AWS Agent Core từ máy local Tại sao nên sử dụng AWS CLI: AWS CLI có thể giúp bạn truy cập và cấu hình set up được Agent Core từ máy của mình, linh hoạt và tiện lợi "},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Xây dựng RAG Agent với Groq API và AgentCore Memory Tổng quan Trong workshop này, chúng ta sẽ xây dựng một RAG (Retrieval-Augmented Generation) Agent hoàn chỉnh với khả năng:\nGọi Groq API để sử dụng LLM models với hiệu năng cao Chunking \u0026amp; Embedding documents để tối ưu vector search AgentCore Memory để duy trì context lâu dài qua các phiên chat Tool Integration để agent có thể tự động search FAQ và reformulate queries AgentCore cung cấp framework để xây dựng AI agents với memory persistence, middleware hooks, và tool orchestration - cho phép agent \u0026ldquo;nhớ\u0026rdquo; lịch sử hội thoại và personalize responses.\nNội dung Tổng quan về Workshop Chuẩn bị Kiến trúc 5.3.1. Gọi Groq API 5.3.2. Chunking \u0026amp; Embedding 5.3.3. Code Handler AgentCore Chạy Agent Core Tech Stack Component Technology LLM Provider Groq API (OpenAI models) Embedding Model HuggingFace (all-MiniLM-L6-v2) Vector Store FAISS Agent Framework LangChain + AgentCore Memory Backend AgentCore Memory Store Text Splitting RecursiveCharacterTextSplitter Điều kiện tiên quyết Python 3.8+ Groq API Key Kiến thức cơ bản về RAG và LLM Hiểu biết về vector embeddings Kết quả mong đợi Sau workshop, bạn sẽ có:\nAgent có khả năng trả lời FAQ dựa trên vector search\nMemory system để nhớ preferences và context người dùng\nTool orchestration để agent tự quyết định khi nào dùng tool\nProduction-ready code với error handling và logging\nBắt đầu: 5.1. Tổng quan về Workshop\n"},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/6-self-evaluation/","title":"Tự đánh giá","tags":[],"description":"","content":"Trong suốt thời gian thực tập tại [AWS Vietnam] từ [08/09/2025] đến [09/12/2025], tôi đã có cơ hội học hỏi, rèn luyện và áp dụng kiến thức đã được trang bị tại trường vào môi trường làm việc thực tế.\nTôi đã tham gia [dự án APT Magic], qua đó cải thiện kỹ năng [AWS, lập trình, thiết kế hệ thống, phân tích, viết báo cáo, giao tiếp, làm việc nhóm \u0026hellip;].\nVề tác phong, tôi luôn cố gắng hoàn thành tốt nhiệm vụ, tuân thủ nội quy, và tích cực trao đổi với đồng nghiệp để nâng cao hiệu quả công việc.\nĐể phản ánh một cách khách quan quá trình thực tập, tôi xin tự đánh giá bản thân dựa trên các tiêu chí dưới đây:\nSTT Tiêu chí Mô tả Tốt Khá Trung bình 1 Kiến thức và kỹ năng chuyên môn Hiểu biết về ngành, áp dụng kiến thức vào thực tế, kỹ năng sử dụng công cụ, chất lượng công việc ☐ ✅ ☐ 2 Khả năng học hỏi Tiếp thu kiến thức mới, học hỏi nhanh ☐ ✅ ☐ 3 Chủ động Tự tìm hiểu, nhận nhiệm vụ mà không chờ chỉ dẫn ✅ ☐ ☐ 4 Tinh thần trách nhiệm Hoàn thành công việc đúng hạn, đảm bảo chất lượng ✅ ☐ ☐ 5 Kỷ luật Tuân thủ giờ giấc, nội quy, quy trình làm việc ✅ ☐ ☐ 6 Tính cầu tiến Sẵn sàng nhận feedback và cải thiện bản thân ✅ ☐ ☐ 7 Giao tiếp Trình bày ý tưởng, báo cáo công việc rõ ràng ✅ ☐ ☐ 8 Hợp tác nhóm Làm việc hiệu quả với đồng nghiệp, tham gia nhóm ✅ ☐ ☐ 9 Ứng xử chuyên nghiệp Tôn trọng đồng nghiệp, đối tác, môi trường làm việc ✅ ☐ ☐ 10 Tư duy giải quyết vấn đề Nhận diện vấn đề, đề xuất giải pháp, sáng tạo ☐ ✅ ☐ 11 Đóng góp vào dự án/tổ chức Hiệu quả công việc, sáng kiến cải tiến, ghi nhận từ team ✅ ☐ ☐ 12 Tổng thể Đánh giá chung về toàn bộ quá trình thực tập ✅ ☐ ☐ Cần cải thiện Nâng cao kiến thức chuyên ngành và tay nghề nhiều hơn Chủ động học hỏi các thành viên công ty và sáng tạo hơn trong công việc Cải thiện sự tập trung trong công việc, chăm chỉ và kỷ luật với bản thân hơn nữa "},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/7-feedback/","title":"Chia sẻ, đóng góp ý kiến","tags":[],"description":"","content":"Đánh giá chung 1. Môi trường làm việc\nKhông khí làm việc tại FCJ thân thiện và dễ hòa nhập. Mọi người hỗ trợ nhau rất nhiệt tình, kể cả khi mình cần giúp đỡ ngoài giờ. Không gian làm việc sạch sẽ và thoải mái, tạo điều kiện để tập trung tốt. Mình nghĩ công ty có thể tổ chức thêm các buổi giao lưu nội bộ hoặc hoạt động nhóm để tăng kết nối giữa các thành viên.\n2. Sự hỗ trợ từ mentor / team admin\nMentor giải thích chi tiết, kiên nhẫn và luôn khuyến khích mình tự đặt câu hỏi khi chưa rõ vấn đề. Team admin hỗ trợ nhanh chóng các thủ tục và tài liệu cần thiết, giúp mình bắt nhịp công việc thuận lợi. Điều mình thích nhất là mentor cho mình tự thử nghiệm và tự giải quyết bài toán trước khi hướng dẫn.\n3. Mức độ phù hợp giữa công việc và chuyên ngành\nCác nhiệm vụ được giao khá sát với kiến thức mình học trong trường, đồng thời giúp mình tiếp cận thêm những nội dung mới. Nhờ vậy, mình vừa củng cố được kiến thức nền tảng, vừa học thêm các kỹ năng thực tiễn mà chương trình học trên lớp chưa đề cập.\n4. Cơ hội học hỏi và phát triển kỹ năng\nTrong quá trình thực tập, mình được rèn luyện nhiều kỹ năng như phối hợp nhóm, quản lý nhiệm vụ, sử dụng các công cụ làm việc chuyên nghiệp và cách trao đổi trong môi trường công sở. Mentor cũng chia sẻ những kinh nghiệm thực tế rất hữu ích để định hướng nghề nghiệp của mình sau này.\n5. Văn hóa \u0026amp; tinh thần làm việc nhóm\nMình cảm nhận văn hóa công ty tích cực: tôn trọng lẫn nhau, hỗ trợ hết mình và làm việc trong bầu không khí thoải mái nhưng vẫn đảm bảo chuyên nghiệp. Khi có dự án gấp, mọi người cùng hợp tác để hoàn thành mục tiêu chung, không phân biệt vị trí hay vai trò. Điều này khiến mình cảm thấy được chào đón dù chỉ là thực tập sinh.\n6. Chính sách / phúc lợi cho thực tập sinh\nCông ty có phụ cấp thực tập và khá linh hoạt về thời gian khi thực tập sinh có nhu cầu điều chỉnh. Ngoài ra, việc được tham gia các buổi đào tạo nội bộ cũng là một điểm cộng lớn.\nMột số câu hỏi khác Trong thời gian thực tập, điều gì khiến bạn hài lòng nhất? Mình hài lòng nhất với môi trường hỗ trợ tốt và mentor hướng dẫn rất tận tình. Bạn nghĩ công ty có thể cải thiện điểm nào để hỗ trợ thực tập sinh tốt hơn? Giao việc đúng khả năng chuyên môn để sinh viên có cơ hội tự xử lý, nhờ đó học được nhiều kỹ năng thực tế. Nhiều tài liệu còn thiếu cần được bổ sung thêm. Nếu có bạn bè quan tâm, bạn có đề xuất họ thực tập tại đây không? Lý do? Có. Vì môi trường thân thiện, mentor nhiệt tình, công việc phù hợp chuyên ngành và giúp học hỏi thực tế nhanh chóng. Đề xuất \u0026amp; mong muốn Bạn có ý kiến nào giúp nâng cao trải nghiệm thực tập? Nên có buổi onboarding riêng cho thực tập sinh mới để làm quen quy trình và công cụ nhanh hơn. Bạn có muốn tiếp tục tham gia chương trình này nếu có cơ hội? Có. Vì chương trình mang lại nhiều trải nghiệm thực tế và cơ hội phát triển. Các góp ý bổ sung (tự do chia sẻ): Mình đánh giá cao môi trường làm việc hiện tại. Nếu có thêm workshop chuyên sâu định kỳ thì trải nghiệm sẽ còn tốt hơn. "},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://taindAI19.github.io/aws-internship-report/vi/tags/","title":"Tags","tags":[],"description":"","content":""}]