[{"uri":"https://taindAI19.github.io/aws-internship-report/5-workshop/5.3-architecture/5.3.1-agentcore-memory/","title":"AgentCore Memory","tags":[],"description":"","content":"Configuring Memory in AgentCore To enable memory for AgentCore, follow the steps below.\n1. Create Memory in Bedrock Go to Bedrock → select AgentCore. Switch to the Memory tab. Click Create memory. In the memory creation interface, you will see the following sections:\nMemory name Set the name for the memory that AgentCore will use.\nShort-term memory (raw event) expiration The number of days detailed conversation history is stored. For this demo, you can keep the default 90 days.\n2. Types of Memory in AgentCore 1. Summarization – Conversation Summaries Function: Summarizes the conversation after it ends or periodically. Purpose: Keeps long-term context while using minimal storage.\nExample: You have 100 messages about AWS CLI errors → later the Agent remembers:\n“The user was experiencing AWS CLI connection issues.”\n2. Semantic Memory Function: Stores key facts or knowledge independent of context. Purpose: Used to answer questions based on previously mentioned information.\nExample:\n“Project A uses Python 3.9.” Ask again later → Agent responds with Python 3.9 immediately.\n3. User Preferences Function: Learns user habits and communication style. Purpose: Personalizes responses.\nExample: If you often say:\n“Keep the answer short.” The Agent will consistently respond concisely.\n4. Episodes Function: Stores sequences of events and analyzes success/failure through Reflections. Purpose: Helps the Agent learn from past experiences.\nExample: A previous flight booking failed due to missing dates → the Agent remembers. Next time, it asks for the date first.\n3. Memory Type Used in the Demo For the demo, you only need:\nSummarization Choose Summarization and click Create to complete the setup.\n4. Update Memory ID in Python After creating a Memory, you will receive a Memory ID.\nAdd it to your Python file:\n# AgentCore Memory Configuration REGION = \u0026#34;ap-southeast-1\u0026#34; MEMORY_ID = \u0026#34;memory_j98zj-4LFDxqB2o1\u0026#34; GROQ_API_KEY = os.getenv(\u0026#34;GROQ_API_KEY\u0026#34;) Be sure to update the Memory ID and Region to match your configuration.\n"},{"uri":"https://taindAI19.github.io/aws-internship-report/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"AWS named as a Leader in 2025 Gartner Magic Quadrant for Strategic Cloud Platform Services for 15 years in a row In the context of strong digital transformation, cloud-native platforms and container ecosystems have become the core infrastructure for businesses that want to build and operate flexible, modern applications. Recently, according to the Gartner Magic Quadrant 2025 report, Amazon Web Services (AWS) continues to hold the Leader position in two important areas:\nCloud-Native Application Platforms Container Management This not only reflects the maturity of the AWS service ecosystem but also shows the strategic direction in supporting customers to build large-scale, secure and efficient application architectures.\nThis article analyzes how AWS achieved the Leader position, the notable changes in the service platform, and how they affect modern application architecture.\nArchitecture overview and why AWS is ranked Leader From the perspective of cloud-native architecture, AWS\u0026rsquo;s strengths lie in its deep service coverage, high level of integration, and comprehensive ecosystem support for the application lifecycle: build → deploy → operate → optimize.\nGartner rates AWS highly in 2 criteria:\nAbility to Execute Completeness of Vision AWS responds well because:\nDiverse compute ecosystem: Lambda, App Runner, Amplify, Elastic Beanstalk Automated and stable scalability Native AI/ML integration: Amazon Bedrock, SageMaker Flexible serverless container deployment: ECS, EKS with Fargate Increasingly mature hybrid \u0026amp; edge model: EKS Hybrid Nodes, ECS Anywhere AWS\u0026rsquo;s approach to cloud-native platforms AWS\u0026rsquo;s cloud-native platform is built on the principles of:\nProviding managed runtime environments to optimize operations Full lifecycle support for applications Deep integration with AI/ML and Serverless Diverse deployments: from monoliths to microservices and event-driven Cloud-native application platforms help businesses shorten time to market and reduce operating costs through automation.\nCore service portfolio includes: AWS Lambda – serverless compute AWS App Runner – automated web/app backend deployment AWS Amplify – fullstack cloud-native for web/mobile AWS Elastic Beanstalk – traditional yet powerful PaaS These services can be flexibly combined based on business requirements.\nChoosing compute services: Communication layering Application deployment scenarios Corresponding services / models to consider Fully serverless AWS Lambda, Amazon API Gateway, DynamoDB Simple web app – need automatic scaling AWS App Runner, AWS Amplify Traditional multi-tier applications AWS Elastic Beanstalk, Amazon EC2 Microservices Amazon ECS, Amazon EKS, AWS Fargate, Amazon EventBridge AI/ML inference \u0026amp; fine-tuning Amazon Bedrock, Amazon SageMaker Thanks to modularity, businesses can start small and scale gradually according to needs.\nContainer Management: Why AWS excels? AWS continues to hold the Leader position for 3 consecutive years in the Container Management group thanks to:\n1. Diverse orchestration Amazon ECS – native orchestrator, simple, stable Amazon EKS – Kubernetes fully managed, multi-AZ, multi-region support 2. Serverless containers with Fargate No need to manage nodes, automatic scaling, cost optimization.\n3. Hybrid \u0026amp; Edge EKS Hybrid Nodes ECS Anywhere EKS Anywhere Allows workloads to run on-prem, edge, or hybrid cloud in a unified manner.\n4. Observability and Governance Deep integration with:\nCloudWatch X-Ray AWS OpenSearch Managed Prometheus \u0026amp; Grafana Pub/Sub and event-driven in AWS environment Similar to the event-driven architecture in the sample form, AWS provides services that support the pub/sub model:\nAmazon SNS – simple pub/sub Amazon EventBridge – powerful event bus, intelligent routing SQS – decoupling queue Step Functions – orchestration workflow Advantages:\nReduces synchronous calls Limits coupling between microservices Ensures scalability and stability Disadvantages:\nNeed to monitor event flow \u0026amp; handle failed messages\nMust ensure idempotency when there are duplicate messages\nImpact on businesses and technical teams AWS holds the Leader position not only because of technology but also because of its ability to support customers:\nFor businesses: Shorten product launch time Reduce infrastructure operating costs Flexible expansion according to actual needs Easy AI/ML integration For technical teams: Reduce cognitive load thanks to managed services Increase experimentation speed Easy deployment of microservices \u0026amp; event-driven Comprehensive CI/CD support Conclusion AWS\u0026rsquo;s continued leadership in the Gartner Magic Quadrant 2025 is a clear demonstration of its strategy focusing on continuous innovation, service diversification, and ensuring developer experience optimal.\nWhether you are a startup or a large enterprise, adopting AWS cloud-native platforms and container services provides a long-term competitive advantage, especially as AI, hybrid cloud, and serverless become the new norm.\nAWS provides not only technology, but also a complete ecosystem that helps businesses accelerate their application modernization journey.\n"},{"uri":"https://taindAI19.github.io/aws-internship-report/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Automating Data Processing with Amazon EMR: Orchestration with Step Functions \u0026amp; EventBridge As data pipelines continue to grow, the need to automate processes, reduce manual effort, and optimize compute costs is more important than ever. Amazon EMR provides a powerful environment for running workloads such as Apache Spark, ETL, batch analytics, and data enrichment. However, in many cases – especially in industries that require tight infrastructure control such as finance, healthcare, and government – ​​running transient EMR clusters without automation is costly and operationally risky.\nIn this blog, I share how to build a fully automated Spark pipeline using EMR on EC2, orchestrated by AWS Step Functions and triggered by Amazon EventBridge. This architecture illustrates how to run data processing jobs on a scheduled, cost-effective, and easy-to-administer schedule.\nArchitecture Guide The solution uses the public COVID-19 dataset to simulate a periodic data processing pipeline. The entire workflow is separated into independent, replaceable, and scalable components.\nThe data processing flow consists of 7 main steps:\nCSV data is stored in S3 (raw input).\nEventBridge triggers on a schedule, triggering the state machine.\nStep Functions creates a temporary EMR cluster on EC2.\nPySpark job is submitted to calculate COVID-19 indicators by state.\nResults are written to S3 output.\nEMR cluster automatically deletes after job completion.\nAll logs are saved to S3 for monitoring \u0026amp; troubleshooting.\nThis architecture is especially suitable for batch Spark workloads, which require detailed control of the infrastructure but still want to optimize costs.\nAutomation architecture features When working with transient EMR clusters, three important factors are considered:\nInternal: Spark cluster performance, resource control, EC2 cost optimization External: scheduled triggers, orchestration of multiple job pipelines Human: reduce operational burden, limit manual errors, clear audit The orchestration model using Step Functions helps to clearly monitor the lifecycle – from provisioning → spark submit → cleanup.\nTechnology selection and communication scope Components Technology used Provision \u0026amp; teardown cluster AWS Step Functions, Amazon EC2, Amazon EMR Scheduling Amazon EventBridge Scheduler Data storage \u0026amp; logs Amazon S3 Observability Amazon CloudWatch, S3 logs, EMR Application UIs IaC (infrastructure deployment) AWS CloudFormation The orchestration pipeline A model connecting micro-components is built in an event-driven direction:\nEventBridge: triggers workflow.\nStep Functions: manages sequential steps.\nEMR: executes Spark workload.\nS3: stores input, output, and logs.\nAdvantages of this model:\nSeparate compute (EMR) and coordination logic (Step Functions) Pay for compute only when the cluster is running Transparent processing per state Disadvantages: need to closely monitor state machine \u0026amp; error handling.\nCore workflow components 1. Provisioning EMR cluster Step Functions initializes EMR cluster on EC2 with optimal configuration for workload. IAM roles are set according to least privilege principle.\n2. Spark job execution The PySpark job is uploaded to S3, and Step Functions sends step submission to EMR cluster.\nWhen run, the script performs:\nNormalize data format Remove erroneous records Calculate monthly average of: inpatient beds ICU beds COVID-19 patient rate Export CSV file by timestamp to S3 output 3. Automated teardown After the job completes, the cluster is automatically deleted to save costs. No more EC2 costs incurred when forgetting to shut down the cluster.\nSetup \u0026amp; operate pipeline CloudFormation A single template deploys the entire infrastructure:\nEMR roles S3 buckets Log bucket Step Functions state machine EventBridge schedule Deployment typically completes in 5 minutes.\nLoad and prepare data Dataset from healthdata.gov is renamed and uploaded to raw/.\nPySpark script The script is uploaded to the scripts/ directory on S3 and used when submitting the job.\nScheduling with EventBridge Workflows can run:\nOnce Periodically (cron or rate-based) Business triggers This turns the Spark pipeline into a recurring ETL system, for example:\nend-of-day report processing weekly data aggregation periodic health index calculation Monitoring \u0026amp; observability Step Functions monitoring Track each state Quickly catch errors, view input/output Intuitive operation EMR monitoring Spark step monitoring View job details via Spark UI, YARN Timeline Observe cluster-level metrics CloudWatch logs If logging is enabled:\ndriver logs executor logs system metrics All streamed in real time.\nMulti-layer observation helps shorten troubleshooting time, especially with complex Spark workloads.\nProcessing \u0026amp; Testing Results The CSV output is saved to the s3:///processed//\nThe output can be used for:\ndashboard analytics\ncompliance reporting\ndownstream data enrichment\nResource Cleanup To avoid additional costs:\nDelete S3 buckets (input/output/log) Delete CloudFormation stack Recheck EventBridge schedule to avoid unwanted triggers Conclusion The solution using Amazon EMR + Step Functions + EventBridge provides a fully automated, cost-effective, and observable approach to Spark on-demand processing.\nThis is an ideal choice for businesses that want to:\nAutomate ETL/batch analytics pipeline Reduce compute costs with temporary clustering Maintain granular control over security and infrastructure Ensure enterprise-standard auditing, logging, and scheduling You can extend this solution by:\nadding more Spark jobs adding more complex workflows in Step Functions integrating partitioned output with Glue Data Catalog integrating with Lake Formation for data control "},{"uri":"https://taindAI19.github.io/aws-internship-report/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Accelerate Spark Application Development on Amazon EMR With Data Solutions Framework (DSF) on AWS As enterprises increasingly rely on big data processing, developing and operating Apache Spark applications requires a unified, repeatable, and maintainable process. However, many engineering teams still face difficulties: heterogeneous Spark configurations, distributed source code, unstandardized pipelines, or unautomated deployment workflows.\nTo address this issue, AWS introduced Data Solutions Framework (DSF) – a standardized development framework that simplifies the process of building, testing, packaging, and deploying Spark applications on Amazon EMR. In this blog, I show how DSF accelerates the development lifecycle, standardizes the Spark pipeline, and brings tangible benefits to data engineering teams.\nWhat is DSF? Data Solutions Framework on AWS is a standardized toolkit that allows you to:\nDevelop Spark applications in a modular model Standardize project structure Easily package them for deployment on EMR Integrate with services like AWS Glue, EMR, S3 Support CI/CD pipelines (GitHub Actions, CodePipeline) Reduce heterogeneous Spark configuration errors DSF helps engineering teams avoid the need to build a standard framework for Spark from scratch, while avoiding fragmentation between projects.\nSolution Architecture with DSF DSF defines three main layers:\nFramework Layer – provides abstraction for reading, writing, processing, logging, config.\nApplication Layer – where Spark business logic is written (ETL, transform…).\nPipeline \u0026amp; Deployment Layer – CI/CD, packaging, orchestration and job submission.\nGeneral workflow:\nDeveloper writes Spark jobs according to DSF standard DSF CLI is used to run locally, test logic, and package Artifact is uploaded to S3 EMR cluster executes DSF standard jobs via bootstrap or step submit Monitoring \u0026amp; logging integrated via CloudWatch and S3 Strengths: completely separates task code (business logic) from infrastructure (EMR config).\nBenefits of using DSF 1. Standardize project structure Spark projects will have a unified layout: src/ main/ python/ resources/ conf/ tests/ scripts/\nThis makes onboarding developers easier, reducing confusion between projects.\n2. Centralized configuration management DSF supports YAML-based config, including:\ninput/output paths desired Spark settings environment-specific overrides → No more hardcoded config in Spark code.\n3. Simple local debugging \u0026amp; running Through DSF CLI, you can run the pipeline locally – simulating the same as when running on EMR.\n4. Automatic packaging DSF automatically packages the application with dependencies → deploy directly to EMR.\n5. Orchestration support DSF works well with:\nAWS Step Functions Amazon MWAA / Airflow Amazon EMR on EC2 or EMR Serverless Supported tools: DSF CLI DSF CLI provides commands:\ndsf init – create a standardized Spark project dsf run – run the application locally/test dsf package – package \u0026amp; build dsf deploy – upload artifact to S3 dsf generate – create a config file or module template CLI simplifies the workflow: Build → Test → Deploy.\nDevelopment process with DSF Standard process when developing Spark applications with DSF:\n1. Project initialization Developers use DSF CLI to create projects with full modules and standard structures.\n2. Write Spark processing logic ETL or transform logic is separated by module, easy to maintain.\n3. Set up YAML configuration Includes:\ndataset inputs schema Spark settings running environment (dev, staging, prod) 4. Run local to unit test DSF allows running locally without EMR → accelerate development.\n5. Build \u0026amp; package Artifact contains code + configs that are built automatically.\n6. Deploy \u0026amp; orchestrate Artifact is pushed to S3, then run on EMR via Step Functions/Airflow.\nIntegration with Amazon EMR DSF supports many Spark running models:\n1. EMR on EC2 Highly customizable Suitable for heavy workloads Runs on job flow or step submit model 2. EMR Serverless No cluster management required Automatic scaling Suitable for flexible or ad-hoc workloads How DSF makes EMR more efficient: Unified configuration → easy to run multiple environments Consistent packaging → reduced deployment errors Standardized logging → easier debugging Easy automation with Step Functions or EventBridge CI/CD for Spark applications using DSF Built-in DSF for pipeline:\nGitHub Actions AWS CodePipeline + CodeBuild Sample pipeline:\nDeveloper pushes code CodeBuild runs unit test DSF package application Upload artifact to S3 Trigger run on EMR (optional) → Easy to ensure quality \u0026amp; consistency when deploying.\nLogging \u0026amp; Observability When running a Spark job with DSF:\nLogs are sent about Amazon CloudWatch\nDetailed driver/executor logs are saved to S3\nFramework supports standardized logging\nThis makes forensics, auditing, and debugging convenient for data teams.\nExtending the architecture You can extend DSF for:\nMulti-step pipeline: extract → transform → enrich → publish Lakehouse combines Glue Catalog + Iceberg/Hudi Real-time ingestion with Kinesis/Managed Kafka Combined with Lake Formation data governance DSF creates a unified platform to scale according to business needs.\nConclusion Data Solutions Framework on AWS helps standardize the entire Spark application development lifecycle, from local development to production deployment. In an environment where data workloads are increasingly complex, DSF helps data engineering teams:\nReduce development costs Avoid errors due to lack of standardization Increase productivity and deployment speed Ensure consistency across Spark pipelines Easily integrate with EMR, Step Functions, MWAA, and CI/CD DSF is not simply a tool, but a best-practice framework that helps businesses build a powerful, flexible, and scalable data processing platform.\n"},{"uri":"https://taindAI19.github.io/aws-internship-report/3-blogstranslated/3.4-blog4/","title":"Blog 4","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://taindAI19.github.io/aws-internship-report/3-blogstranslated/3.5-blog5/","title":"Blog 5","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://taindAI19.github.io/aws-internship-report/3-blogstranslated/3.6-blog6/","title":"Blog 6","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://taindAI19.github.io/aws-internship-report/5-workshop/5.4-agent-core-run/5.4.2-call-agentcore/","title":"Calling AgentCore","tags":[],"description":"","content":"Simple Demo with AgentCore 1. Send the first question Use the command:\nagentcore invoke \u0026#34;{\u0026#39;prompt\u0026#39;: \u0026#39;Tell me about roaming activations\u0026#39;}\u0026#34; The Agent will respond based on the data you deployed (database + logic in your code).\n2. Test memory between invocations (session) After the first question, send another related one — for example:\nagentcore invoke \u0026#34;{\u0026#39;prompt\u0026#39;: \u0026#39;Activate it for Vietnam\u0026#39;}\u0026#34; Then ask:\nagentcore invoke \u0026#34;{\u0026#39;prompt\u0026#39;: \u0026#39;which country was i referring to\u0026#39;}\u0026#34; If the Agent responds correctly and remembers the previous information → this confirms the Memory is working and AgentCore is maintaining context across invocations.\nAgentCore behaves as expected in this demo.\n"},{"uri":"https://taindAI19.github.io/aws-internship-report/5-workshop/5.4-agent-core-run/5.4.1-run-agentcore/","title":"Configure &amp; Deploy AgentCore","tags":[],"description":"","content":"Getting Started with AgentCore Configure First, push your local code to AWS AgentCore using the command:\nagentcore configure -e ./{your_python_file.py} 1. Agent Name Enter a name for your Agent.\n2. Configuration File Press Enter to use the default configuration file pyproject.toml.\n3. Deployment Configuration Select 2 – Deploy using Docker, allowing AgentCore to automatically build and manage your Docker image.\n4. Execution Role Keep the default setting and let AWS create the IAM Role automatically.\n5. ECR Repository Press Enter to let AWS create the ECR repository for storing the Docker image.\n6. Authorization Configuration Choose No for OAuth. The Agent will only allow access via AWS IAM Access Key \u0026amp; Secret Key.\n7. Request Header Allowlist Press Enter to use the default allowlist configuration.\nResult Once this step is completed, your code has been successfully uploaded to AgentCore.\nLaunch the Agent Use the command below to start the Agent with your API Key (using GROQ):\nagentcore launch --env GROQ_API_KEY=your_api_key_here When the terminal shows Running, your Agent is successfully running.\n"},{"uri":"https://taindAI19.github.io/aws-internship-report/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: “Cloud Day AWS 2025 in HCMC” Event Objectives Join the plenary session broadcast live from Hanoi Listen to keynote speakers and major cloud announcements shaping Vietnam’s digital landscape Generative AI: Discover the newest advancements and real-world use cases Data Analytics: Learn how data-driven decision-making can transform businesses Migration \u0026amp; Modernization: Gain clarity on how to move forward in your cloud journey Speakers Eric Yeo – Country General Manager, Vietnam, Cambodia, Laos \u0026amp; Myanmar, AWS Jaime Valles – Vice President, Commercial Sales \u0026amp; Business Development APJ, AWS Jeff Johnson – Managing Director ASEAN, AWS Dr Jens Lottner – CEO, Techcombank Trang Phung – CEO, U2U Network Vu Van – Co-founder \u0026amp; CEO, ELSA Corp Nguyen Hoa Binh – Chairman, Nexttech Group Dieter Botha – CEO, Tymex Nguyen Van Hai – Director of Software Engineering, Techcombank Nguyen The Vinh – Co-Founder \u0026amp; CTO, Ninety Eight Nguyen Minh Ngan – AI Specialist, OCB Nguyen Manh Tuyen – Head of Data Application, LPBank Securities Key Highlights Vietnam Cloud Day 2025 – A Seamless Hybrid Format Even though the main conference was hosted in Hanoi, the hybrid setup allowed me to enjoy the full experience right in Ho Chi Minh City. It was a great opportunity to engage with leading cloud builders and industry experts without traveling far. The live-streamed plenary session delivered inspiring talks and key announcements expected to influence Vietnam’s cloud technology direction in the coming years. Core Themes \u0026amp; Interactive Learning This year’s event revolved around the shift toward a modular architecture—where each component is an independent service communicating through events. The discussions focused on three central domains:\nGenerative AI: Latest innovations and hands-on applications Data Analytics: Leveraging insights to drive better business outcomes Migration \u0026amp; Modernization: Strategies to modernize existing systems effectively Major Benefits Expand your local network: Meet professionals from the vibrant HCMC tech ecosystem Learn from top experts: Gain practical insights directly from Vietnam’s cloud leaders Stay ahead of the curve: Receive the newest updates on cloud technology trends Key Takeaways Strategic Insights from Vietnam Cloud Day 2025 Cloud adoption is accelerating as Vietnamese enterprises intensify digital transformation efforts Government initiatives underline the nation’s commitment toward a cloud-first approach Success stories from Techcombank and U2U Network highlighted real AWS migration journeys Leadership alignment: Panel discussions stressed the importance of connecting GenAI initiatives with tangible business goals Technical Deep-Dive – Generative AI \u0026amp; Data Unified data foundation: Practical frameworks for modeling business processes GenAI roadmap: Guidance on adopting AWS AI services in real business scenarios AI-Driven Development Lifecycle (AI-DLC): When to choose sync, async, pub/sub, or streaming Security recommendations: Deciding between VM, containers, or serverless architectures AI Agents: Moving towards intelligent systems that enhance team productivity Architecture \u0026amp; Operational Excellence Event-driven, modular design: Build systems that are flexible and resilient Compute choices: Select EC2, containers, or serverless based on workload patterns Scalability and observability: Implement robust logging, monitoring, and cost governance Applying These Insights to Work Assess existing workloads: Identify what can be migrated to AWS for immediate value Develop a strong data layer: Set up ingestion, storage, and processing pipelines Experiment with GenAI: Start with small POCs using Amazon Bedrock or SageMaker Enhance security posture: Enforce least-privilege IAM and secure networking Invest in team development: Encourage learning AWS AI/ML tools to stay competitive Event Experience I had the chance to learn about Eric Yeo, the AWS Regional Manager, for the first time—an inspiring leader with great vision. Participating in “Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders” offered tremendous clarity on how to modernize applications and databases with modern architectures and cloud-native tools.\nLearning from outstanding speakers The sessions delivered rich insights from AWS specialists and executives from top tech organizations. Real-life case studies helped me better understand how to apply Domain-Driven Design (DDD) and Event-Driven Architecture to large-scale systems. Hands-on technical learning Took part in event storming workshops, turning business processes into domain events. Practiced designing systems as microservices with clearly scoped bounded contexts. Explored communication models—understanding when to use synchronous flows, asynchronous events, pub/sub, point-to-point, or streaming. Exploring modern tools Discovered Amazon Q Developer, an AI assistant supporting the entire SDLC. Learned how to automate refactoring and build serverless solutions with AWS Lambda to enhance delivery speed. Networking \u0026amp; insightful conversations Connected with AWS experts, business leaders, and other builders, helping bridge the gap between technical and business perspectives. Discussions reinforced the need for a business-first mindset in technology decision-making. Lessons learned Applying DDD and event-driven patterns greatly increases scalability, resilience, and maintainability. Successful modernization requires a step-by-step approach with clear ROI tracking. Using AI tools like Amazon Q Developer can significantly accelerate productivity. Event Photos Overall, the event not only delivered valuable technical insights but also reshaped my perspective on system design, modernization strategies, and effective collaboration across teams.\n"},{"uri":"https://taindAI19.github.io/aws-internship-report/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Event Report: AI-Driven Development Session with Amazon Q Developer \u0026amp; Kiro Event Objectives Understand how Generative AI is transforming the entire software development lifecycle Explore the roles of Amazon Q Developer and Kiro in accelerating productivity and application development workflows Learn how AI supports architecture design, coding, testing, deployment, and system operations Experience live demonstrations of AI-powered development in real technical use cases Improve engineering efficiency by automating repetitive tasks, allowing developers to focus on creativity and problem-solving Main Speakers Toan Huynh – Instructor, AWS GenAI Builder Club My Nguyen – Instructor, AWS GenAI Builder Club Coordinators Diem My – Program Coordinator Dai Truong – Event Coordinator Dinh Nguyen – Operations Coordinator Session Highlights Toan Huynh delivered the content with infectious energy and inspiration, presenting advanced knowledge in a clear and engaging way. My Nguyen provided enthusiastic support, addressing every participant’s question with depth and expertise. The Transformation of Software Development with Generative AI Generative AI is reshaping the software industry—from learning and requirement analysis to ideation, development, testing, and application management.\nThe session emphasized how AI enables:\nAutomation of repetitive engineering tasks Rapid prototyping and shorter development cycles Improved code quality and security through AI-generated insights and reviews A shift toward higher-value work such as system design and architectural thinking AI Across the Software Development Lifecycle (AI-DLC) Participants learned how AI integrates throughout the SDLC:\nArchitecture and system design Code generation and automated refactoring Creating test cases \u0026amp; automated testing Deployment through DevOps pipelines Continuous monitoring and maintenance Through Amazon Q Developer and Kiro, attendees gained a clear understanding of how AI elevates developer capabilities.\nAgenda 2:00 PM – 2:15 PM | Welcome \u0026amp; Introduction\n2:15 PM – 3:30 PM | Overview of the AI-Driven Development Lifecycle \u0026amp; Amazon Q Developer Demo\nInstructor: Toan Huynh\n3:30 PM – 3:45 PM | Break\n3:45 PM – 4:30 PM | Kiro Demonstration\nInstructor: My Nguyen\nKey Takeaways Strategic Insights on AI-Driven Development AI speeds up software development by automating repetitive tasks Engineering teams can focus more on architectural design and solving core problems Time spent debugging, refactoring, and writing documentation is significantly reduced Organizations adopting AI early gain a strong competitive advantage Practical Learnings – Amazon Q Developer Generate high-quality code from natural language (English or Vietnamese) Automatically fix bugs, optimize functions, and enhance performance Create unit tests, documentation, and code explanations Improve DevOps pipelines with AI-assisted CI/CD automation Practical Learnings – Kiro Support system and architecture design from the initial stages Provide real-time suggestions for code structure, patterns, and best practices Help development teams maintain consistent and readable coding standards Applying AI to Work Integrate AI tools to reduce time-consuming repetitive tasks Use Amazon Q Developer to quickly prototype ideas and enhance code quality Apply Kiro during architecture analysis and system planning Encourage engineering teams to familiarize themselves with AI to improve productivity Begin adopting an AI-augmented SDLC to modernize engineering workflows Event Experience The workshop offered a refreshing and motivating experience, revealing the powerful impact of Generative AI on modern software development.\nLearning from Experts Gained valuable insights from Toan Huynh and My Nguyen on applying AI in daily engineering tasks Understood that AI supports not only coding, but also architecture and DevOps processes Hands-On Demonstrations Observed Amazon Q Developer automatically refactor code, generate documentation, and create tests within seconds Experienced how Kiro assists with analyzing and making system design decisions Community Networking Connected with members of the AWS GenAI Builder Club and discussed real-world AI development use cases Explored AI integration opportunities in enterprise environments Key Lessons Learned AI significantly enhances development speed, accuracy, and overall productivity Modern developers should transition from “code writers” to system designers and solution thinkers Mastering AI early provides major advantages for engineering teams Event Photos Overall, the event not only delivered technical knowledge but also helped shift my mindset about application design, system modernization, and cross-team collaboration.\n"},{"uri":"https://taindAI19.github.io/aws-internship-report/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Nguyen Duc Tai\nPhone Number: 0961287204\nEmail: taindse180365@fpt.edu.vn\nUniversity: Ho Chi Minh City FPT University\nMajor: Artificial Intelligence\nClass: AWS092025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: GenAI Intern\nInternship Duration: From 08/09/2025 to 09/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://taindAI19.github.io/aws-internship-report/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Introduction to the Workshop This workshop provides a step-by-step guide to setting up IAM, AWS CLI, UV, Groq API, deploying RAG source code integrated with Groq LLM into AWS AgentCore, and finally publishing the API through AWS Gateway. Workshop Objectives \u0026ldquo;How to call APIs\u0026rdquo; — understand how to call external APIs outside AWS AgentCore. \u0026ldquo;Chunking\u0026rdquo; — learn how to split data for RAG so it can retrieve information optimally. \u0026ldquo;Adding memory to RAG\u0026rdquo; — explore how the RAG Agent can remember each piece of data during interactions with users. \u0026ldquo;Deploy AWS AgentCore\u0026rdquo; — understand how to deploy AWS AgentCore. \u0026ldquo;Publish API\u0026rdquo; — learn how to call AgentCore through an API. "},{"uri":"https://taindAI19.github.io/aws-internship-report/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be implemented this week: Day Task Start date Completion date Resources 2 - Get acquainted with FCJ members - Read and note the rules and regulations at the internship unit 09/08/2025 09/08/2025 3 - Learn about AWS and its services + Compute + Storage + Networking + Database + Management Tools 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/vi/1-explore/ https://aws.amazon.com/vi/ 4 - Create AWS Free Tier account - Learn AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Set up MFA for Root account + Create Admin Group and Admin User + Support account authentication + Install AWS CLI \u0026amp; configuration + How to use AWS CLI 10/09/2025 10/09/2025 https://000001.awsstudygroup.com/vi/ 5 - Manage costs with AWS Budget: + Cost Budget + Usage Budget + RI Budget + Savings Plans Budget - Practice: + Create Budget using template + Create Cost Budget + Create Usage Budget + Create RI Budget + Create Savings Plans Budget + Clean Up Resources 11/09/2025 11/09/2025 https://000007.awsstudygroup.com/vi/0-createtemplate/ 6 -Request Support from AWS Support: + AWS Support Plans + Access AWS Support + Manage Support Requests 12/09/2025 12/09/2025 https://000009.awsstudygroup.com/vi/ Week 1 Achievements: Understand what AWS is and grasp the basic service groups:\nCompute Storage Networking Database Management Tools Successfully created and configured an AWS Free Tier account.\nComplete 5 tasks to receive an additional $100 credit\nGet familiar with AWS Management Console and know how to find, access, and use services from the web interface.\nInstall and configure AWS CLI on your computer including:\nAccess Key Secret Key Default Region Use AWS CLI to perform basic operations such as:\nCheck account information \u0026amp; configuration Get a list of regions View EC2 services Create and manage key pairs Check running service information Be able to connect between the web interface and CLI to manage AWS resources in parallel.\nKnow how to manage costs with AWS Bugets\nUnderstand how to access and create support requests on AWS Support\n"},{"uri":"https://taindAI19.github.io/aws-internship-report/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Understand Amazon Virtual Private Cloud (VPC) Understand how to deploy Amazon EC2 instances Learn how to configure AWS Site-to-Site VPN Tasks to be implemented this week: Day Task Start Date Completion Date References 2 - Learn about Amazon Virtual Private Cloud (Amazon VPC): + Subnets + Route Table + Internet Gateway + NAT Gateway - Practice: + Create VPC + Create Subnet + Create Internet Gateway + Create Route Table 15/09/2025 15/09/2025 - https://000003.awsstudygroup.com/vi/1-introduce/ - https://000003.awsstudygroup.com/vi/3-prerequisite/ 3 - Learn about Firewall in VPC: + Security Group + Network ACLs + Networking VPC Resource Map - Practice: + Create Security Group + Enable VPC Flow Logs 16/09/2025 16/09/2025 - https://000003.awsstudygroup.com/vi/2-firewallinvpc/ - https://000003.awsstudygroup.com/vi/3-prerequisite/ 4 - Deploy Amazon EC2 Instances: + Deploy Production-Ready EC2 Infrastructure + Production-Ready Features - Practice: + Launch EC2 Instance + Test connectivity + Create NAT Gateway + Use Reachability Analyzer + Create EC2 Instance Connect Endpoint (Optional) + AWS Systems Manager Session Manager + CloudWatch Monitoring \u0026amp; Alerting 17/09/2025 17/09/2025 https://000003.awsstudygroup.com/vi/4-createec2server/ 5 - Configure Site-to-Site VPN: + Create VPN environment + Configure VPN connection + Configure VPN with strongSwan using Transit Gateway (Optional) - Resource cleanup - Practice: + Create VPC for VPN + Launch EC2 Instance + Create Virtual Private Gateway + Create Customer Gateway + Create VPN Connection + Configure Customer Gateway + Customize AWS VPN Tunnel + Advanced VPN Configuration + Troubleshoot VPN + Create Transit Gateway Attachment + Configure Route Tables 18/09/2025 18/09/2025 - https://000003.awsstudygroup.com/vi/5-vpnsitetosite/ - https://000003.awsstudygroup.com/vi/6-cleanup/ 6 - Infrastructure as Code Templates: + Automate VPC Deployment with Infrastructure as Code + Benefits of IaC for VPC Deployment + CloudFormation Template 19/09/2025 19/09/2025 https://000003.awsstudygroup.com/vi/7-infrastructureascode/ Results achieved in Week 2: Understood and deployed Amazon VPC:\nSubnets Route Table Internet Gateway NAT Gateway. Successfully configured Security Group, Network ACLs, and VPC Flow Logs.\nCreated and tested EC2 Instance, applied NAT Gateway, Reachability Analyzer, Instance Connect Endpoint, Session Manager, and CloudWatch.\nSet up Site-to-Site VPN:\nCreated and configured Virtual Private Gateway, Customer Gateway, VPN Connection, Route Tables Experimented with strongSwan configuration and troubleshooting. Gained familiarity with infrastructure automation using Infrastructure as Code (CloudFormation Templates).\n"},{"uri":"https://taindAI19.github.io/aws-internship-report/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Understand the concept and purpose of using Amazon EC2 (Elastic Compute Cloud). Become familiar with the process of initializing, accessing, configuring and managing EC2 virtual machines on Windows and Linux environments. Conduct a trial deployment of a basic web application (AWS User Management) onto EC2 instances. Practice skills in monitoring, securing and optimizing EC2 resources throughout the usage process. Tasks to be deployed this week: Day Task Start date Completion date Document source 2 - Get an overview of Amazon EC2 and important terms: + Instance, AMI, Key Pair, Elastic IP, Security Group, Volume + Cost models: On-Demand, Spot, Reserved Instance, Savings Plan 09/22/2025 09/22/2025 https://cloudjourney.awsstudygroup.com/ 3 - Practice: Create and set up Windows EC2 instance + Choose the appropriate AMI and instance type + Create a key pair, adjust the security group + Connect via Remote Desktop (RDP) + Explore the Windows Server 2022 interface 09/23/2025 09/23/2025 https://cloudjourney.awsstudygroup.com/ 4 - Practice: Initialize and set up Linux EC2 instance + Create Amazon Linux 2 virtual machine + Log in using SSH with key pair + Get familiar with terminal and basic commands + Update packages and system 09/24/2025 09/24/2025 https://cloudjourney.awsstudygroup.com/ 5 - Deploy “AWS User Management” application + Install Node.js, npm and necessary dependencies on Windows \u0026amp; Linux + Deploy CRUD application to manage users + Test add, edit, delete, search functions + Allow others to access the application via Security Group 09/25/2025 09/25/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn about EC2 monitoring and management tools: + Amazon CloudWatch (monitor metrics \u0026amp; logs) + AWS Systems Manager + EC2 Instance Connect - Collect and release resources: delete unused instances, Elastic IPs, Security Groups 09/26/2025 09/26/2025 https://cloudjourney.awsstudygroup.com/ Results achieved in week 3: Understand important components of Amazon EC2, including:\nAMI: Operating system template and software to create instances.\nInstance Type: Performance regulation (CPU, RAM, storage).\nKey Pair: Secure login authentication tool.\nElastic IP: Static IP address for public access.\nSecurity Group: Protection layer to control access flow.\nSuccessfully created and configured both Windows and Linux EC2 instance:\nConnected using RDP (Windows) and SSH (Linux).\nControlled the instance via AWS Console and CLI.\nAdjusted network rules (ports 80, 443, 22, 3389) to serve the web application.\nCompleted deployment of AWS User Management application on two operating systems:\nInstalled runtime environments such as Node.js and npm.\nFully operational CRUD functionality.\nAllowed others to access via Public IP or Elastic IP.\nSuccessfully apply instance monitoring methods:\nUse CloudWatch to monitor resources (CPU, RAM, network).\nUse Systems Manager to automate and manage sessions.\nTake advantage of EC2 Instance Connect for quick access via browser.\nImprove the ability to manage EC2 costs and resources:\nTurn off or delete instances that are no longer needed.\nReclaim redundant Elastic IPs.\nClean up unnecessary Security Groups and Key Pairs.\nSummary of knowledge gained: Amazon EC2: Understand the operating mechanism and how to deploy virtual servers in a cloud environment.\nWindows \u0026amp; Linux administration: Understand how to access, configure and secure on the two operating systems.\nApplication deployment: Complete deployment of Node.js CRUD application on EC2.\nSystem Monitoring: Know how to use CloudWatch \u0026amp; Systems Manager to monitor performance.\nCost Optimization: Ability to analyze and clean up resources to reduce operating costs.\n"},{"uri":"https://taindAI19.github.io/aws-internship-report/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Master how to securely grant access to AWS resources to applications.\nClearly distinguish between the operating mechanism between Access Key/Secret Access Key and IAM Role.\nKnow the process of creating and assigning IAM Roles to EC2 Instances, allowing applications to access AWS services without having to include authentication information in the source code.\nGet familiar with the browser-based development environment AWS Cloud9, which has built-in AWS CLI.\nPractice writing code, testing, debugging and managing source code directly on Cloud9.\nDeployment tasks of the week Day Task Start date Completion date Document source 2 - Overview of IAM and AWS access control mechanism - Review the concepts of User, Group, Policy, Role - Learn how AWS authenticates \u0026amp; authorizes access 09/22/2025 09/22/2025 https://cloudjourney.awsstudygroup.com/ 3 - Testing authorization using Access Key/Secret Access Key - Testing AWS access applications using Access Key - Analyzing risks and vulnerabilities when embedding Access Key in applications 09/23/2025 09/23/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create IAM Role for EC2 - Assign role to instance and check S3/DynamoDB access via Node.js sample application - Retest application after revoking permission 09/24/2025 09/24/2025 https://cloudjourney.awsstudygroup.com/ 5 - Initialize and get familiar with Cloud9 IDE - Set up workspace, configure working environment - Experience terminal, file explorer, debugger, syntax highlight 09/25/2025 09/25/2025 https://cloudjourney.awsstudygroup.com/ 6 - Summary exercises: + Write scripts to operate AWS CLI in Cloud9 + Build a Node.js CRUD (User Management) application running directly on Cloud9 + Delete resources to avoid charges 09/26/2025 09/26/2025 https://cloudjourney.awsstudygroup.com/ Results achieved in week 4 Understand the difference between IAM Role and Access Key, as well as why using roles is a safer choice in practice.\nKnow how to create IAM Role with appropriate permissions, such as read/write access to S3.\nAssign IAM Role to EC2 Instance and allow applications running on it to access AWS services without fixed credentials.\nProficiently use AWS Cloud9 to:\nCreate a development workspace.\nConnect to EC2.\nRun AWS CLI commands, write and test code directly.\nDeploy a small Node.js application connecting to S3 or DynamoDB directly in Cloud9.\nUnderstand the process of reclaiming, deleting resources after the exercise to optimize costs.\nSummary of accumulated knowledge IAM Role: Dynamic authorization mechanism, reducing security risks due to no need to use Access Key. Access Key: Understand the weaknesses when using directly in code or on the developer machine. AWS Cloud9: A flexible programming environment on the browser, supporting many languages ​​and integrating AWS CLI. AWS CLI/SDK: Tools to access and manipulate AWS services based on IAM Role. Actual deployment process: Set up the environment, write and run the application, check access rights, and clean up resources when finished. "},{"uri":"https://taindAI19.github.io/aws-internship-report/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 objectives: Master the working mechanism of Amazon S3, understand the object storage structure, storage options and the scope of application in practice.\nDeploy static website on S3: enable hosting features, fine-tune access rights and test the speed of operation.\nMaster the security settings for bucket, including blocking public access, writing IAM policies and configuring Bucket Policy securely.\nGet familiar with advanced operations: Versioning, Transfer Acceleration, Replication to another region, and object migration commands.\nPerform the resource release process correctly to avoid unexpected costs and comply with best practices when using S3.\nTasks to be implemented this week: Day Task Start date Completion date Resources 2 Background review \u0026amp; environment preparation + Explain the concepts of bucket, object, region, key and storage classes (STANDARD, IA, GLACIER) + Understand durability levels (11-nines) and availability + Summarize suitable problems for S3: static website, backup, data analysis 06/10/2025 06/10/2025 https://cloudjourney.awsstudygroup.com/ 3 Initialize bucket \u0026amp; enable website hosting + Create bucket according to the naming rules and select region + Upload index.html, error.html + Enable Static Website Hosting mode and access the endpoint to test 07/10/2025 07/10/2025 https://cloudjourney.awsstudygroup.com/ 4 Set up Public Access \u0026amp; configure Bucket Policy + Understand Block Public Access modes at the account and bucket levels + Write Bucket Policy to only allow public a specific object (index.html) + Check website access via browser + Try to revoke public access and recheck security behavior 08/10/2025 08/10/2025 https://cloudjourney.awsstudygroup.com/ 5 Improve speed \u0026amp; expand hosting capabilities + Compare advantages — limitations between: S3 + CloudFront, S3 Transfer Acceleration, AWS Amplify Hosting + Enable Transfer Acceleration and benchmark speed with curl in multiple locations + (Optional) Create a CloudFront to enable HTTPS/CDN + Add CloudWatch monitoring and evaluate usage costs 09/10/2025 09/10/2025 https://cloudjourney.awsstudygroup.com/ 6 Lifecycle management, replication \u0026amp; resource cleanup + Enable Versioning to control object changes and try to restore old files + Set Lifecycle to switch to IA/Glacier after a certain number of days + Set up Cross-Region Replication with corresponding IAM Role + Use cp/mv/sync to transfer data between bucket/region + Delete test resources: disable acceleration, remove CloudFront, empty bucket, delete bucket 10/10/2025 10/10/2025 https://cloudjourney.awsstudygroup.com/ Results achieved in week 5: 1. Basic setup \u0026amp; operation:\nComplete AWS CLI configuration (Access Key, Secret Key, Region).\nCheck CLI status with:\naws configure list\naws s3 ls (list buckets)\naws s3api list-buckets (see details)\nCreate/Delete bucket with command:\naws s3 mb s3://bucket-ten\naws s3 rb s3://bucket-ten --force\n2. Static Website Hosting:\nCreate a new bucket, put HTML file in and enable hosting. See website running through provided endpoint. Check error page when accessing non-existent file to ensure error.html works properly. 3. Manage permissions \u0026amp; security:\nTry enabling/disabling Block Public Access to observe the difference. Write a bucket policy to publicize only one file. Test access rights by public/private each object. 4. Data Management \u0026amp; Performance Acceleration:\nEnable versioning and try uploading multiple versions of the same file.\nRestore files from older versions to understand the mechanism.\nApply lifecycle rules to automatically move objects to IA or Glacier.\nTest Transfer Acceleration speed using curl from multiple regions.\n5. Replication \u0026amp; Data Migration:\nSet up CRR and observe objects being replicated to other regions.\nUse commands:\naws s3 cp\naws s3 mv\naws s3 sync\nCheck the destination bucket to verify successful replication.\n6. Cleanup \u0026amp; Optimize Usage Costs:\nDelete test objects, disable Transfer Acceleration and CloudFront. Delete test bucket, turn off hosting. Summary of best practices to help control costs and ensure data security. Notes: Always avoid publicizing the entire bucket — only public objects that are absolutely necessary.\nBlock Public Access is an important security layer, only override if you understand the rights and risks.\nVersioning combined with Lifecycle helps manage data long-term while still saving costs.\nCloudFront suitable when HTTPS, private domain and global performance are needed; Transfer Acceleration is suitable when clients often upload remotely.\nAmplify Hosting is a simple choice when only needing to deploy static websites from Git repo.\nIAM must be tightly configured, do not save Access Key in source code.\nCloudWatch + S3 Storage Lens supports monitoring capacity, traffic and costs.\nWhen deleting buckets with Versioning, it is necessary to delete all versions + delete markers, to avoid unwanted storage charges.\nSummary of knowledge gained: Operate S3 at full level: hosting, security, replication, versioning and cost optimization. Know how to choose the right static site deployment solution: pure S3, S3 + CloudFront, or Amplify. Effectively use big data management tools: lifecycle, policy, replication, sync/copy. Understand the resource cleaning process to avoid incurring costs. "},{"uri":"https://taindAI19.github.io/aws-internship-report/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":" Week 6 Goals: Get familiar with the method of monitoring, analyzing performance through Performance Insights, Monitoring and Enhanced Logging.\nUnderstand the overall architecture of Amazon RDS, including engines such as MySQL, PostgreSQL, MariaDB, SQL Server…\nPractice backup – restore – snapshot – automated backup – failover to ensure data recovery.\nCreate, configure and operate an RDS Instance from foundation to advanced configuration.\nBuild parameter group, subnet group, security group specifically for RDS.\nSet up connections between RDS and environments such as EC2, Cloud9, or local machines.\nMaster the resource scaling and cost optimization methods, and know how to clean up resources when no longer needed.\nTasks to be deployed this week: Day Task Start date Completion date Document source 2 RDS Introduction + Engine options + Multi-AZ and Read Replica models + Backup \u0026amp; snapshot cycles 13/10/2025 13/10/2025 https://cloudjourney.awsstudygroup.com/ 3 Initialize RDS Instance + Build Subnet Group + Configure Security Group for Cloud9/EC2 + Choose engine, capacity and instance type 10/14/2025 10/14/2025 https://cloudjourney.awsstudygroup.com/ 4 Connect \u0026amp; operate on RDS + Access from EC2 or Cloud9 + Create schema \u0026amp; data table + CRUD actions using MySQL or PostgreSQL client 10/15/2025 10/15/2025 https://cloudjourney.awsstudygroup.com/ 5 Monitoring – Backup – Restore + Create manual snapshot + Restore instance from snapshot + Check CPU, connection, throughput + Use Performance Insights to analyze queries 10/16/2025 10/16/2025 https://cloudjourney.awsstudygroup.com/ 6 Scaling – Optimizing – Reclaiming resources + Increase/decrease instance class + Expand storage capacity + Adjust backup retention appropriately + Delete unused snapshots \u0026amp; RDS 10/17/2025 10/17/2025 https://cloudjourney.awsstudygroup.com/ Results achieved in week 6: 1. Master the Backup – Restore – Snapshot process:\nCreate a manual snapshot. Restore to a new RDS instance. Observe the backup window. Confirm that automated backups are automatically created every day. 2. Understand the operating mechanism of Amazon RDS:\nRecognize the difference between Single-AZ, Multi-AZ and Read Replica. Understand how automated backups work and how long they are stored. Understand that snapshots are manual archives and are not automatically deleted. 3. Connect to RDS from local/EC2/Cloud9:\nInstall MySQL/PostgreSQL client to access. Connect using endpoint: mysql -h endpoint.amazonaws.com -u admin -p Create database, table and execute CRUD commands: CREATE DATABASE demo; CREATE TABLE users (...); INSERT, SELECT, UPDATE, DELETE 4. Configure a complete RDS Instance:\nCreate DB Subnet Group including 2 subnets in two different AZs. Create Security Group to allow connection (port 3306/5432). Install: Engine MySQL/PostgreSQL Instance class db.t3.micro Storage gp3 20GB Backup retention Endpoint private or public 5. Monitor system performance \u0026amp; activity:\nMonitor CPU, RAM, number of connections via Monitoring. Use Performance Insights to find queries that cause heavy load. Check Slow Query Log when enabled. 6. Optimize costs and scale resources:\nChange instance class t3.micro → t3.small according to load demand. Increase storage capacity to 30GB when needed. Adjust backup retention time to 3 days to save costs. Delete snapshots that no longer serve storage purposes. 7. Clean up resources:\nDelete RDS instance. Delete subnet group and corresponding security group. Clean up all old snapshots. Summary of knowledge gained Understand the working mechanism of Amazon RDS and popular engines. Create, operate and connect to RDS from Cloud9, EC2 or local machine. Understand the backup platform, snapshot and recovery process. Know how to use Performance Insights to evaluate query performance. Get familiar with using cale resources and optimize costs. Practice resource cleaning to avoid additional costs. "},{"uri":"https://taindAI19.github.io/aws-internship-report/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":" Week 7 Objectives: Part 1 – AWS CloudWatch\nBuild a clear understanding of how CloudWatch is structured and how each component functions, including Metrics, Logs, Events, Dashboards, and Alarms. Observe and analyze service-level metrics from EC2, RDS, and Lambda in an actual AWS environment. Configure log collection and learn how CloudWatch Logs stores, indexes, searches, and processes application and system logs. Create and refine alert thresholds to support continuous operational monitoring. Design dashboards that visualize system health and performance in real time. Use Container Insights to evaluate the behavior and performance of containerized workloads. Remove unused monitoring components to optimize overall cost. Part 2 – Hybrid DNS with Route 53 Resolver\nUnderstand the architectural mindset behind linking on-premises DNS with AWS. Create inbound and outbound Route 53 Resolver endpoints. Build Resolver Rules to forward DNS queries to the appropriate domain. Integrate an internal Active Directory DNS server with a Route 53 Private Hosted Zone. Test two-way name resolution between AWS and on-premises environments. Clean up all created resources when the implementation is complete. Tasks Completed During the Week Day Detailed Work Start Date Completion Date Reference 2 CloudWatch – Overview \u0026amp; Architecture • Study CloudWatch’s design goals and how it integrates with AWS services. • Explore in detail how Metrics, Logs, Alarms, Events, Dashboards, and Container Insights operate. • Compare basic monitoring with detailed monitoring. • Review the end-to-end workflow: application → log agent → log groups → metrics → alarm → SNS → automated action. • Create the first chart from EC2 data. 20/10/2025 20/10/2025 https://cloudjourney.awsstudygroup.com/ 3 CloudWatch Metrics \u0026amp; Logs – Practical Setup • Create a Log Group and learn how Log Streams are managed. • Install the CloudWatch Agent to gather OS-level data such as CPU, memory, and network usage. • Modify the cloudwatch-agent.json configuration to collect system logs and application logs. • Build metric filters to identify recurring error patterns (“ERROR”, “Failed”, “CRITICAL”). • Push custom metric data manually using AWS CLI. • Adjust retention periods and examine their impact. 21/10/2025 21/10/2025 https://cloudjourney.awsstudygroup.com/ 4 CloudWatch Alarms \u0026amp; Dashboards – Automating Alerts • Configure a CPU alarm triggered when usage exceeds 70% for 5 minutes. • Create an EC2 Status Check alarm to track instance health. • Connect alarms to SNS topics for email notifications. • Use alarms to perform automated actions, such as rebooting an EC2 instance. • Build dashboards containing multiple widget types for logs, metrics, and system states. • Create a consolidated dashboard showing EC2, RDS, and network metrics. • Enable Container Insights to analyze CPU/memory usage and container restarts. 22/10/2025 22/10/2025 https://cloudjourney.awsstudygroup.com/ 5 Route 53 Resolver – Understanding Hybrid DNS Architecture • Review Private Hosted Zones and the DNS resolution flow inside a VPC. • Create an Outbound Resolver Endpoint to forward DNS queries from AWS to on-premises. • Create an Inbound Resolver Endpoint to receive DNS queries from on-premises. • Add a forwarding rule for the domain “corp.local” to the internal AD DNS server. • Validate endpoints and associate rules with the VPC. 23/10/2025 23/10/2025 https://cloudjourney.awsstudygroup.com/ 6 Microsoft AD + DNS Integration – Simulating On-Premises DNS • Deploy a Windows Server EC2 instance and install Active Directory Domain Services. • Create the domain “corp.local” along with its DNS zone. • Configure a Conditional Forwarder pointing to AWS’s Inbound Endpoint. • Add A and CNAME records inside the internal DNS. • Test bi-directional name resolution: – From AWS: nslookup dc1.corp.local – From AD: nslookup api.internal.aws • Simulate endpoint failure and observe fallback behavior. 24/10/2025 24/10/2025 https://cloudjourney.awsstudygroup.com/ 7 Resource Cleanup and Cost Optimization • Remove unused Log Groups, Log Streams, Dashboards, and Alarms. • Disable Container Insights to prevent additional charges. • Delete Route 53 Resolver Endpoints. • Remove all Resolver Rules. • Shut down the AD domain controller. • Terminate all test EC2 instances. • Review the billing breakdown for CloudWatch Logs, metrics, and endpoint costs. 25/10/2025 25/10/2025 https://cloudjourney.awsstudygroup.com/ Outcomes Achieved in Week 7 1. AWS CloudWatch CloudWatch Metrics – Collection \u0026amp; Analysis\nIdentified the default metrics provided by EC2, EBS, VPC, and Load Balancers. Created custom metrics via AWS CLI to validate ingestion pipelines. Examined long-term data storage and high-resolution metrics. Detected anomalies such as CPU spikes, sudden network surges, and abnormal Disk I/O activity. CloudWatch Logs – Application and System Log Monitoring\nInstalled agents to send OS and application logs to CloudWatch. Created log groups and log streams for monitoring and debugging. Configured log retention and tested exports to S3 for long-term archiving. Identified common issues such as unauthorized access, throttling, and CPU credit depletion. CloudWatch Alarms – Notification \u0026amp; Recovery\nBuilt threshold-based alarms for continuous EC2 monitoring. Created system-check alarms to ensure host stability. Configured SNS notifications for incident alerts. Enabled automatic recovery actions for hardware-level failures. CloudWatch Dashboards – Real-Time Visibility\nConstructed real-time dashboards for system observability. Added data from EC2, NAT Gateway, RDS, and ELB into unified views. Organized metrics using multiple widget types for clear visualization. Container Insights – Observing Microservices\nEnabled monitoring for ECS and Fargate workloads. Visualized container-level CPU/memory usage. Used restart counts to identify potential application issues. Evaluated how Container Insights accelerates troubleshooting. Resource Cleanup\nRemoved unused dashboards, alarms, and custom metrics. Uninstalled CloudWatch Agents where unnecessary. Deleted log groups and turned off Container Insights. Summary of Achievements\nGained a full understanding of the monitoring pipeline from data collection to automated remediation. Successfully implemented the workflow: metrics → logs → alarms → dashboards. Improved the ability to diagnose issues using CloudWatch data. Enhanced operational efficiency and cost control. Built a real-time monitoring and alerting flow suitable for DevOps/SRE work. 2. Hybrid DNS Deployment with Route 53 Resolver Understanding Hybrid DNS Architecture\nIdentified common challenges when integrating on-premises DNS with cloud environments. Reviewed how inbound/outbound endpoints and Resolver Rules work together. Environment Preparation\nCreated a dedicated VPC and private subnets for DNS endpoints. Configured Security Groups to allow DNS traffic on port 53. Deployed a Windows Server to simulate an on-premises Active Directory environment. Remote Access via RDGW\nUsed an RDGW instance to access private subnets. Validated DNS resolution inside the VPC before hybrid integration. Microsoft Active Directory Setup\nBuilt a domain controller to simulate local infrastructure. Configured a DNS zone such as company.local. Validated SRV, A, and NS records. Hybrid DNS Configuration\nOutbound Endpoint\nAllowed AWS to forward internal DNS queries to the on-premises AD server. Attached Resolver Rules to route internal domains. Inbound Endpoint\nAccepted DNS queries from on-premises AD to AWS’s Private Hosted Zone. Verified that AWS-based private records were resolving properly. Resolver Rules\nCreated forwarding rules for internal domains. Used system rules for default VPC DNS behavior. Full End-to-End DNS Testing\nVerified name resolution from AD → AWS and AWS → AD. Compared latency and reliability in both directions. Ensured that all DNS records resolved accurately. Cleanup Activities\nRemoved endpoints and Resolver Rules. Deleted Private Hosted Zones and test DNS records. Shut down AD servers and test EC2 instances. Knowledge Summary CloudWatch:\nEnd-to-end monitoring for logs and metrics. Automated alerting and response. Effective dashboarding for system visibility. Integrated monitoring for containerized workloads. Route 53 Hybrid DNS:\nClear understanding of hybrid DNS principles. Successful integration of on-premises DNS with AWS. Fully functioning two-way name resolution. Readiness to apply DNS design in enterprise hybrid environments. "},{"uri":"https://taindAI19.github.io/aws-internship-report/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: This week focuses on two groups of content: NoSQL storage with DynamoDB and high-speed caching with Redis in ElastiCache. The main goal is to understand how these two services operate, their data structures, scalability, and how to combine them to speed up read operations in the system.\nPart 1 – Amazon DynamoDB Survey of key-value and document data models, characteristics of DynamoDB. Understand the partitioning mechanism, how to choose keys to avoid hotspots. Delve into important concepts: RCU/WCU, On-Demand mode, TTL, Streams, and Global Tables. Design tables with Partition Key and Sort Key; Try configuring GSI and LSI for different query types. Get familiar with CRUD operations and advanced queries via Query/Scan, combining FilterExpression and ConditionExpression. Use AWS CLI and JSON files to interact with tables. Import test data to evaluate query performance. Part 2 – Amazon ElastiCache for Redis Review the principles of in-memory caching and how Redis organizes clusters (cluster mode enabled/disabled). Create a Redis environment: configure nodes, subnet groups, security groups, and parameter groups. Connect from EC2 via redis-cli to test operations. Try basic commands: SET/GET, TTL, EXPIRE, and RDB snapshots. Apply the Cache-Aside model to combine Redis with DynamoDB. Compare latency between cache queries and DynamoDB queries. Reclaim resources after completion. Tasks Day Implementation Content Start Finish Documentation 2 DynamoDB Overview Explore NoSQL structure, partitioning mechanism, throughput, and components such as GSI/LSI, TTL, Streams. Understand how DynamoDB distributes data by Partition Key and the impact of key selection on performance. 2025-10-27 2025-10-27 https://cloudjourney.awsstudygroup.com/ 3 Table \u0026amp; Query Design Create tables with partition keys and sort keys; add GSI to extend query paths. Test TTL mechanism, perform enough CRUD and compare Query/Scan in real-world scenarios. 2025/10/28 2025/11/28 https://cloudjourney.awsstudygroup.com/ 4 Working with CLI Initialize tables using CLI, import sample JSON data, run queries using KeyConditionExpression, conditionally update, and monitor throughput consumption. 2025/10/29 2025/10/29 https://cloudjourney.awsstudygroup.com/ 5 Redis – Architecture and Deployment Prepare subnet group, security group, create Redis cluster with primary and replica, enable failover, then test connection from EC2. 2025/10/30 2025/10/30 https://cloudjourney.awsstudygroup.com/ 6 DynamoDB – Redis Integration Test redis-cli with GET/SET/TTL, try snapshots, set Cache-Aside (cache hit/miss) and measure latency between Redis and DynamoDB. 2025/10/31 2025/10/31 https://cloudjourney.awsstudygroup.com/ 7 Cleanup and Evaluation Delete Redis Cluster and related groups; remove test DynamoDB table. Check billing to determine cost incurred and note speed difference between cache and database. 2025/11/11 2025/11/11 https://cloudjourney.awsstudygroup.com/ Results in Week 8 1. DynamoDB Data Design \u0026amp; Modeling\nUnderstand how DynamoDB distributes data and scales horizontally.\nPractice building appropriate partition/sort keys to reduce partition collisions.\nExperience the role of GSI/LSI secondary indexes in shaping access patterns.\nQuery \u0026amp; Update\nComplete full CRUD via console and CLI interface.\nProficiently use KeyConditionExpression for conditional queries.\nKnow how to combine FilterExpression, UpdateExpression in complex filtering scenarios.\nSurvey Scan and its limitations when data is large.\nAdvanced Features\nEnable TTL and observe record removal behavior.\nTry DynamoDB Streams to see how to log changes.\nEvaluate costs based on read/write demand and actual consumption.\n2. ElastiCache for Redis Deploy Redis cluster\nBuild a Redis cluster in VPC with full network configuration.\nTest replication and failover operations.\nTest connection from EC2 to confirm stable operating environment.\nOperations and observations\nPerform SET, GET, DEL, TTL, EXPIRE in many scenarios.\nMeasure the impact of RDB snapshots on data state.\nEvaluate cache behavior when capacity is limited.\n3. Combine DynamoDB – Redis (Cache-Aside) Build a processing flow: check cache → query DynamoDB if missing → return data to Redis.\nCompare cache hit and miss results, measure real latency to derive performance characteristics.\nRecognize why read-intensive systems often use Redis as a cache layer.\nIdentify typical use cases: sessions, user profiles, high-frequency lookup data.\nKnowledge synthesis DynamoDB\nDistributed NoSQL model.\nHow to choose the right Partition Key and Sort Key.\nCommon query types and related expressions.\nThroughput mechanism according to RCU/WCU or On-Demand.\nTTL and Streams for data lifecycle management.\nElastiCache Redis\nHigh-speed cache model based on RAM. Cluster configuration, replica and failover. TTL, snapshot and temporary data processing. DynamoDB – Redis integration\nApply Cache-Aside to reduce read load. Cache hit/miss analysis. Performance optimization in large services. "},{"uri":"https://taindAI19.github.io/aws-internship-report/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":" Week 9 Objectives: Part 1 – Optimize EC2 Costs with Lambda Function\nUnderstand the mechanism of automatically turning on/off EC2 to reduce operating costs. Create a tag system to identify servers that need to be processed on a schedule. Build a Lambda Function to manage Start/Stop EC2 based on tags. Create a periodic schedule using EventBridge Scheduler. Monitor actual operations via CloudWatch Logs. Reclaim all resources after testing is complete. Part 2 – Optimize EC2 Costs with Savings Plan\nLearn about Savings Plan and the principle of discounting based on commitment level. Distinguish between Compute Savings Plans and EC2 Instance Savings Plans. Understand the USD/hourly commit mechanism and how the estimator calculates it. Know the process of purchasing and applying Savings Plan according to AWS standards. Practice estimating costs using AWS Pricing Calculator. Deployment tasks this week Day Detailed tasks Start Complete Source 2 Analyze EC2 optimization model\n• Identify waste problems when EC2 runs 24/7 but the usage volume is low.\n• Describe the flow of activities: User → EventBridge → Lambda → EC2 API.\n• Identify workloads suitable for automatic Start/Stop mechanism.\n• Consider the risks and limitations of turning EC2 on/off. 03/11/2025 03/11/2025 https://cloudjourney.awsstudygroup.com/ 3 Create EC2 Classification Tag\n• Set Tag Key: Schedule.\n• Sample Tag Value: office-hours, training-only, weekend-shutdown.\n• Assign tags to EC2s that need to be automated.\n• Check with CLI: describe-instances --filters tag:Schedule=office-hours. 04/11/2025 04/11/2025 https://cloudjourney.awsstudygroup.com/ 4 Create IAM Role for Lambda\n• Create role: LambdaEC2ScheduleRole.\n• Assign policy: AmazonEC2FullAccess (for lab purposes).\n• Add logging permission: AWSLambdaBasicExecutionRole.\n• Test trust relationship with lambda.amazonaws.com. 05/11/2025 05/11/2025 https://cloudjourney.awsstudygroup.com/ 5 Develop Lambda Start/Stop EC2\n• Write Python code (boto3) to StartInstances/StopInstances.\n• Add logic to filter instances by tag \u0026lt;Schedule\u0026gt;.\n• Log instance ID enabled/disabled.\n• Test with “Test event”. 06/11/2025 06/11/2025 https://cloudjourney.awsstudygroup.com/ 6 Integrate EventBridge Scheduler\n• Create a rule to turn on EC2 at 08:00 AM.\n• Create a rule to turn off EC2 at 18:00 PM.\n• Assign Lambda as target.\n• View logs in CloudWatch to confirm operation. 07/11/2025 07/11/2025 https://cloudjourney.awsstudygroup.com/ 7 Test \u0026amp; Cleanup\n• Verify EC2 turns on/off on schedule.\n• Monitor CloudWatch Logs to handle permission errors.\n• Delete rules, Lambda Functions and IAM Roles after completion.\n• Compare EC2 costs before and after optimization. 08/11/2025 08/11/2025 https://cloudjourney.awsstudygroup.com/ Results achieved in week 9 1. Optimize EC2 costs via Lambda\nSuccessfully set up EC2 on/off process based on working hours. Understand how to use Tags to optimize EC2 groups. Write Lambda Function to control Start/Stop using boto3. Fully automate via EventBridge Scheduler. Record full logs: time, instance ID, permission errors. Save 50–70% of costs for workloads that do not run continuously. 2. Optimize costs with Savings Plan\nUnderstand the discount mechanism of Savings Plan based on USD/hour commitment.\nUse Pricing Calculator to estimate appropriate commitment level.\nUnderstand the difference between two types of Savings Plan:\nCompute Savings Plan – flexible, multi-service application.\nEC2 Instance Savings Plan – high savings but with more constraints.\nUnderstand the process of registering and activating Savings Plan.\nSee clearly the savings benefits that can reach ~66%.\nSummary of knowledge gained AWS Lambda + EC2 Automation\nAutomatically turn on/off EC2 using Lambda. Use Boto3 to interact with EC2 API. Use EventBridge Scheduler to schedule runs. CloudWatch Logs to check and handle errors. AWS Savings Plans\nCost commitment mechanism to reduce infrastructure usage prices. Suitable for workloads running continuously 24/7. Classify Compute and EC2 Plans. Combine Savings Plan + Auto Scheduling to optimize overall costs. "},{"uri":"https://taindAI19.github.io/aws-internship-report/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting Familiar with AWS and Basic Services in AWS\nWeek 2: Learning and Configuring Amazon VPC\nWeek 3: Deploying and Managing Amazon EC2 on Windows and Linux\nWeek 4: IAM Role Deployment and Working with AWS Cloud9\nWeek 5: Deploying and Managing Static Websites with Amazon S3\nWeek 6: Working with Amazon RDS – Creating, Configuring, Connecting \u0026amp; Operating Databases on AWS\nWeek 7: AWS CloudWatch \u0026amp; Hybrid DNS with Route 53 Resolver\nWeek 8: AWS DynamoDB \u0026amp; ElastiCache\nWeek 9: Optimizing EC2 Costs with AWS Lambda\nWeek 10: DMS - Introduction and Writing Lambda Functions\nWeek 11: Serverless - Using Amplify Authentication and Storage \u0026amp; Serverless - Instructions for Writing Front-end to Call API Gateway\nWeek 12: AI Agent with Amazon Bedrock on AWS\n"},{"uri":"https://taindAI19.github.io/aws-internship-report/5-workshop/5.3-architecture/5.3.2-groq-api/","title":"Calling Groq API","tags":[],"description":"","content":" Objective Use the Groq library (ChatGroq / init_chat_model with model_provider=\u0026quot;groq\u0026quot;) to call an OpenAI-compatible model hosted on Groq.\nConfiguration in Code In the demo code:\nRetrieve API Key from Environment GROQ_API_KEY = os.getenv(\u0026#34;GROQ_API_KEY\u0026#34;) The variable GROQ_API_KEY loads the API key from the environment variable.\nInitialize the Model llm = init_chat_model( model=\u0026#34;openai/gpt-oss-20b\u0026#34;, model_provider=\u0026#34;groq\u0026#34;, api_key=GROQ_API_KEY ) Integrate with Agent The Agent calls the LLM through create_agent(...) using the model=llm parameter:\nagent = create_agent( model=llm, tools=tools, checkpointer=checkpointer, store=store, middleware=[MemoryMiddleware()], system_prompt=system_prompt, ) Processing Flow Agent → Groq API → Model Inference → Response "},{"uri":"https://taindAI19.github.io/aws-internship-report/5-workshop/5.2-prerequiste/","title":"Preparation Steps","tags":[],"description":"","content":" IAM Permissions Required Permissions AdministratorAccess AmazonBedrockFullAccess AWSCodeBuildAdminAccess AWSCodeBuildDeveloperAccess BedrockAgentCoreFullAccess Create a User and Assign Permissions Go to IAM → Users → select Create user. Add the permissions listed above. Complete the user creation and save the Access Key if you need it for the SDK. Download AWS CLI Download AWS CLI: AWS CLI Link\nThen install it following the instructions.\nUV Management Setup 1. Why use UV? UV is fast, lightweight, and manages environments better than pip.\n2. Install UV on Windows Run:\npowershell -ExecutionPolicy ByPass -c \u0026#34;irm https://astral.sh/uv/install.ps1 | iex\u0026#34; Add UV to PATH:\n$env:Path = \u0026#34;C:\\Users\\leamo\\.local\\bin;$env:Path\u0026#34; Restart your machine to apply the new PATH.\n3. Initialize a UV Environment Inside your project directory:\nuv init Then select the environment in VS Code.\nConnect Your Machine to AWS CLI Go back to IAM to create an Access Key.\nCreate Access Key and Configure AWS CLI In the user page: Security credentials → Create access key Choose Command Line Interface (CLI) Configure AWS CLI Run:\naws configure Fill in:\nAWS Access Key ID AWS Secret Access Key Default region name (example: ap-southeast-1) Default output format json Start AWS CLI AgentCore Run:\nuv run which agentcore After running, it will download all necessary libraries for AWS AgentCore.\nCreate Groq API Go to Groq and create an API key as shown. These external tools support RAG and are integrated through AWS AgentCore.\n"},{"uri":"https://taindAI19.github.io/aws-internship-report/2-proposal/","title":"Proposal","tags":[],"description":"","content":"APT Magic A Serverless AI Platform for Personalized Image Generation and Social Interaction 1. Executive Summary APT Magic is a serverless AI-powered web application designed to enable users to generate, personalize, and share artistic content such as AI-generated images. The platform integrates with AI foundation models via Amazon Bedrock and provides a seamless web experience using Next.js (SSR) hosted on AWS Amplify.\nThe MVP version focuses on real-time image generation and sharing, while the Future Design aims to scale with SageMaker Inference, Step Functions, and AWS MLOps pipelines for advanced model orchestration and automation.\nAPT Magic is currently developed as a modern, cost-efficient, and secure AWS-native architecture for small to medium user bases, with planned expansion into enterprise-grade AI orchestration.\n2. Problem Statement What’s the Problem? Most AI image generation platforms are costly, rely on opaque third-party APIs, and offer limited personalization.\nDevelopers and creators often face high latency, lack of transparent model management, and limited control over user data security.\nThe Solution APT Magic leverages AWS serverless architecture to deliver:\nReal-time AI image generation through Amazon Bedrock Stability AI models. Secure user authentication and content management using Amazon Cognito and DynamoDB. Scalable API handling via AWS Lambda and API Gateway. Low-latency global delivery with CloudFront CDN and WAF protection. Future upgrades will include Step Functions orchestration, SQS/SNS decoupling, SageMaker Inference pipelines, and cost-efficient CI/CD via CodeBuild, CodePipeline, and CloudFormation. transforming APT Magic into a fully automated MLOps platform.\n3. Solution Architecture MVP Architecture The MVP is a fully serverless architecture, focusing on scalability, maintainability, and cost-effectiveness.\nCore AWS Services:\nRoute53 + CloudFront + WAF — Secure global access and caching. Amplify (Next.js SSR) — Hosts the frontend and server-side rendering layer. API Gateway + Lambda Functions — Manage backend logic (image processing, subscription, post APIs). Amazon Cognito — User authentication and access control. Amazon S3 + DynamoDB — Data persistence and image storage. Amazon Bedrock — Integrates foundation model (Stability AI) for image generation. Secrets Manager, CloudWatch, CloudTrail — Security, logging, and monitoring. Security\nPrivateLink for secure communication between Lambda and backend services. WAF + IAM policies for traffic filtering and role-based access control. Future Design (Enhanced Architecture) In the next phase, APT Magic will evolve into an AI orchestration platform, introducing new layers for automation, resilience, and model lifecycle management.\nNew Services to be Added:\nAWS Step Functions — To orchestrate asynchronous workflows such as:\nMulti-step AI image generation (prompt validation → inference → result upload). Payment confirmation → model processing → notification. Amazon SQS — For reliable message queuing between async Lambda tasks.\nAmazon SNS — For real-time event notifications to users or administrators.\nAmazon ElastiCache (Redis) — For rate limiting and caching of frequent inference requests.\nAmazon SageMaker Inference — For hosting custom fine-tuned models and managing model endpoints.\nAWS CodePipeline + SageMaker Pipelines — To automate MLOps: model training, evaluation, and deployment.\nAWS PrivateLink + VPC Endpoints — For secure data flow between Lambda, S3, and SageMaker.\nAWS WAF \u0026amp; Shield Advanced — For DDoS protection and advanced security filtering.\nCI/CD + MLOps\nCodePipeline + CodeBuild + CloudFormation for infrastructure deployment and automation. 4. Technical Implementation Implementation Phases Phase 1 – MVP Deployment (Completed / Current)\nImplement Amplify (Next.js SSR) + API Gateway + Lambda. Integrate Bedrock Stability AI API. Deploy CI/CD via CodePipeline + CloudFormation. Enable user authentication (Cognito) and storage (S3 + DynamoDB). Phase 2 – Future Design Expansion\nIntroduce Step Functions + SQS/SNS to manage async AI workflows. Add ElastiCache for request throttling and caching. Integrate SageMaker Inference for fine-tuned model hosting. Implement SageMaker Pipelines for automated training and deployment. Extend security with Shield Advanced + GuardDuty + PrivateLink. Connect GitLab Runner with CodeBuild for unified CI/CD. 5. Timeline \u0026amp; Milestones Phase Description Estimated Duration Deployment Milestone Month 1: Setup \u0026amp; Core API Deploy infrastructure (IaC), Cognito, API Gateway, DynamoDB, and foundational Lambda functions. 4 Weeks Core Backend operational, Auth/User Management completed. Month 2: AI \u0026amp; Payment Integration Integrate Claude Haiku 3 LLM on Amazon Bedrock (Stability AI), Replicate API, complete Image Processing functions, and integrate third-party payment gateway. 4 Weeks Successful end-to-end AI image processing demo. Month 3: Front-end \u0026amp; CI/CD Develop UI/UX (Amplify/Next.js), finalize CI/CD pipelines, and configure Monitoring/Security (CloudWatch/WAF). 4 Weeks Full platform ready for user testing. Month 4: Optimization \u0026amp; Go-Live Perform performance testing (Stress Test), cost optimization, and Production deployment. 4 Weeks Go-Live (Official product launch). 6. Cost Estimate (AWS Pricing Estimate) Total Cost Monthly: $9.80 Upfront: $0.00 12 Months: $117.60 Service Overview Service Region Monthly Cost Upfront 12-Month Cost Notes Amazon Route 53 Asia Pacific (Singapore) $0.50 $0.00 $6.00 1 Hosted Zone, 1 domain, 1 linked VPC Amazon CloudFront Asia Pacific (Singapore) $0.00 $0.00 $0.00 No specific configuration AWS WAF Asia Pacific (Singapore) $6.00 $0.00 $72.00 1 Web ACL; 1 rule per ACL AWS Amplify Asia Pacific (Singapore) $0.00 $0.00 $0.00 Build instance: Standard (8GB/4vCPU); request duration 500ms AWS CloudFormation Asia Pacific (Singapore) $0.00 $0.00 $0.00 No extensions; no operations Amazon API Gateway Asia Pacific (Singapore) $0.13 $0.00 $1.59 10k requests/month; WebSocket message 1KB; request size 30KB AWS Lambda Asia Pacific (Singapore) $1.67 $0.00 $20.04 1 million invokes; x86; 512MB ephemeral storage Amazon CloudWatch Asia Pacific (Singapore) $0.85 $0.00 $10.22 1 metric; 0.5GB logs in; 0.5GB logs to S3 S3 Standard Asia Pacific (Singapore) $0.23 $0.00 $2.76 10GB storage; 20k PUT; 40k GET DynamoDB On-Demand Asia Pacific (Singapore) $0.42 $0.00 $5.04 1GB storage; 1KB item; on-demand mode Total (Estimate) — $9.80 $0.00 $117.60 Based on AWS Pricing Calculator Metadata Currency: USD Locale: en_US Created On: 12/9/2025 Share URL: AWS Calculator Link Legal Disclaimer: AWS Pricing Calculator provides estimates only; actual costs may vary based on usage. AI Model Pricing Model Resolution / Token Usage Quality Price per Request (USD) Notes Titan Image Generator v2 \u0026lt; 512×512 Standard 0.008 Fixed price per 1 image Titan Image Generator v2 \u0026lt; 512×512 Premium 0.01 Fixed price per 1 image Titan Image Generator v2 \u0026gt; 1024×1024 Standard 0.01 Fixed price per 1 image Titan Image Generator v2 \u0026gt; 1024×1024 Premium 0.012 Fixed price per 1 image Stable Diffusion 3.5 Large Any N/A 0.08 Fixed price per 1 image Claude (text + image) 40 input tokens + 1 image N/A 0.00195 Price for 1 request including text and 1 image 1024×1024 Additional Options Mode Augmentation Price (USD) text→img no augment 0.08 text→img with augment 0.08195 img→img no augment 0.012 img→img with augment 0.094 7. Risk Assessment Risk Impact Probability Mitigation AI model inference latency Medium High Use ElastiCache + Step Functions for async handling Cost increase from model calls High Medium Bedrock usage control, SageMaker autoscaling CI/CD misconfigurations Medium Low CloudFormation rollback policies Security vulnerabilities High Medium WAF, GuardDuty, PrivateLink, IAM least privilege Third-party API dependency Medium Medium Bedrock fallback to S3-stored inference results 8. Expected Outcomes Technical Outcomes: Complete serverless AI image generation workflow with secure CI/CD. Modular orchestration enabling rapid MLOps integration. Improved latency and reliability via caching and async workflows. Long-Term Value: A foundation for AI as a Service (AIaaS) platform expansion. Ready-to-scale MLOps framework with automated retraining. Reusable cloud infrastructure for future AI products. "},{"uri":"https://taindAI19.github.io/aws-internship-report/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":" Week 10 Objectives: Part 1 – Building a backend Document Management System (DMS) with Lambda and DynamoDB Understand the operational architecture of a document management system. Design DynamoDB tables based on the actual access needs of the application. Write Lambda Functions to create new, get lists, query details and delete documents. Configure IAM Role for Lambda according to the principle of minimal rights. Conduct testing through test events and monitor with CloudWatch Logs. Prepare the backend to integrate API Gateway and Amplify next week. Part 2 – Understand how DMS will be extended in the coming weeks Understand the role of Cognito, Amplify Storage in the system. Know how DynamoDB Streams are used to synchronize data to OpenSearch for search. Visualize how the entire backend will be deployed using AWS SAM. Understand the expected CI/CD flow running on AWS CodePipeline. Work done this week Day Implementation content Start Complete Source 2 Analyze the entire DMS architecture\n• Clarify the goals: upload, view, download, delete and search documents.\n• Identify the main services: Lambda, DynamoDB, S3, Cognito, API Gateway.\n• Analyze the processing flow from uploading to saving metadata and serving search.\n• Build a list of APIs needed for the backend. 10/11/2025 10/11/2025 https://cloudjourney.awsstudygroup.com/ 3 Designing a DynamoDB table for document metadata\n• Create a Documents table (PK: userId, SK: documentId).\n• Define attributes such as filename, fileType, size, tags, createdAt, updatedAt.\n• Analyze access patterns: query by user, get details by documentId, filter by tag.\n• Prepare sample data and test using Console/CLI. 11/11/2025 11/11/2025 https://cloudjourney.awsstudygroup.com/ 4 Create IAM Role for Lambda\n• Initialize the role DMSLambdaRole.\n• Grant DynamoDB CRUD permissions (GetItem, Query, PutItem, DeleteItem).\n• Enable logging to CloudWatch Logs.\n• Review the trust policy for Lambda.\n• Confirm the role works correctly by testing directly in Lambda. 11/12/2025 11/12/2025 https://cloudjourney.awsstudygroup.com/ 5 Write Lambda CreateDocument\n• Check user input.\n• Generate documentId as UUID.\n• Save metadata to table via put_item.\n• Log for monitoring.\n• Test via test event and CloudWatch Logs.\n• Handle missing input, DynamoDB errors or unusual errors. 11/13/2025 11/13/2025 https://cloudjourney.awsstudygroup.com/ 6 Write Lambda GetDocuments and GetDocumentById\n• Write logic to query all documents by userId.\n• Add pagination capability using LastEvaluatedKey.\n• Write function to get details of a document by documentId.\n• Normalize the returned JSON to match the front-end.\n• Test with real data in DynamoDB. 11/14/2025 11/14/2025 https://cloudjourney.awsstudygroup.com/ 7 Write Lambda DeleteDocument and cleanup data\n• Check the existence of the document before deleting.\n• Delete metadata using delete_item.\n• Prepare logic to delete S3 file (will do next week).\n• Log details of the deletion process.\n• Clean up unused test records. 11/15/2025 11/15/2025 https://cloudjourney.awsstudygroup.com/ Results achieved in week 10 1. Complete the DynamoDB table with the optimal model Build the Documents table to fully meet the metadata storage needs of DMS. Design a partition/sort key suitable for querying by user and getting document details. Data structure is easy to extend to DynamoDB Streams or integrate OpenSearch. 2. Complete the Lambda CRUD set for the system Create new documents using CreateDocument. Get list of documents by userId using GetDocuments. Query details using GetDocumentById. Delete documents using DeleteDocument. All logic is serverless, logs are clear and reprocesses when errors occur. 3. Implement best practices for security and monitoring -IAM Role is configured according to the principle of “least privilege”.\nCloudWatch Logs serve to monitor the entire processing process.\nError handler is clear enough: input error, DynamoDB error or non-existent record.\n4. Fully understand the overall architecture of the DMS system Understand the connection between Authentication – API – Storage – Metadata.\nKnow how Lambda will be integrated into API Gateway next week.\nHave a technical foundation to continue with Amplify Authentication and Storage next week.\nSummary of knowledge in the week AWS DynamoDB Design tables according to access patterns.\nUse Partition/Sort Key correctly.\nMaster PutItem, GetItem, Query, DeleteItem (Boto3) operations.\nAWS Lambda Write serverless functions for CRUD.\nCreate IAM Roles with appropriate policies.\nGet familiar with logging and monitoring with CloudWatch.\nHandle errors according to production standards.\nDMS Architecture Metadata is in DynamoDB. Actual files are stored on S3. User authentication with Cognito. API routing via API Gateway and Lambda. "},{"uri":"https://taindAI19.github.io/aws-internship-report/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":" Week 11 Objectives: Part 1 – Serverless with Amplify Authentication and Storage Learn how Amplify Libraries work, especially Amplify Auth and Storage. Connect a web application to Cognito to manage users. Build a frontend upload function and save files directly to S3 via Amplify Storage. Understand the three file access modes: public, protected, and private. Understand the communication mechanism between Cognito – S3 – Amplify in a serverless application. Part 2 – Serverless: Build a Frontend that communicates with an API Gateway Understand how to create an API Gateway and map requests to Lambda.\nWrite frontend (JavaScript/React) to call API Gateway → Lambda → DynamoDB.\nTest API using Postman and from frontend itself.\nUnderstand the entire data path process: Frontend → API Gateway → Lambda → DynamoDB.\nWork done this week Day Contents Start Finish Source 2 Starting Amplify and Preparing the Environment\n• Learn about Amplify Libraries, CLI, and UI Components.\n• Initialize an Amplify integration project.\n• Install aws-amplify and @aws-amplify/ui.\n• Configure AWS locally. 11/17/2025 11/17/2025 https://cloudjourney.awsstudygroup.com/ 3 Authenticate users with Amplify Auth\n• Create a User Pool and Identity Pool.\n• Configure Amplify Auth in the frontend.\n• Use Amplify UI Components to create a login interface.\n• Test the flow of creating an account, logging in, confirming the code, and changing the password. 11/18/2025 11/18/2025 https://cloudjourney.awsstudygroup.com/ 4 Store files via Amplify Storage\n• Create an S3 bucket using Amplify.\n• Set up public/protected/private access rights.\n• Write an upload function from the frontend to S3.\n• Test again on the S3 console.\n• Write logic to list files and get the URL. 11/19/2025 11/19/2025 https://cloudjourney.awsstudygroup.com/ 5 Create an API Gateway to connect Lambda\n• Build a REST API.\n• Attach CRUD routes to Lambda.\n• Enable CORS for the frontend.\n• Update new ARNs for Lambda functions. 20/11/2025 20/11/2025 https://cloudjourney.awsstudygroup.com/ 6 Test API with Postman\n• Send requests with GET/POST/DELETE methods.\n• Attach Cognito token in header.\n• Monitor logs with CloudWatch.\n• Fix CORS errors and IAM permissions. 21/11/2025 21/11/2025 https://cloudjourney.awsstudygroup.com/ 7 Integrate frontend with API Gateway\n• Write JS service to call API Gateway.\n• Automatically attach Cognito token.\n• Display data returned from Lambda/DynamoDB.\n• Complete end-to-end process: Frontend → API → Lambda → DynamoDB.\n• Clean up test data. 11/22/2025 11/22/2025 https://cloudjourney.awsstudygroup.com/ Results achieved in week 11 1. Successfully set up Authentication using Amplify + Cognito Complete all flows: login, registration, code authentication and password reset.\nFrontend gets JWT token to use when calling API Gateway.\nUnderstand how Cognito manages identity through User Pool and Identity Pool.\n2. Store and manage files using Amplify Storage + S3 Upload files directly from frontend to S3 successfully.\nS3 bucket is automatically created by Amplify.\nFully test 3 types of access rights.\nComplete file processing functions: upload, list, get URL, and delete option.\n3. Build API Gateway to serve DMS system Create complete REST API with CRUD routes.\nConnect directly to Lambda from last week.\nConfigure standard CORS for frontend.\nSend request with JWT token and test everything with Postman.\n4. Integrate frontend with API Gateway Write frontend logic to send request with Cognito token.\nDisplay metadata list from DynamoDB.\nComplete the overall flow:\nSummary of the week Amplify Amplify Auth uses Cognito as the authentication system.\nAmplify Storage supports simple S3 operations from the frontend.\nUI component set helps create a quick login page.\nCognito (Authentication) User Pool: account management. Identity Pool: grants access to AWS resources. JWT token used when sending requests to API Gateway. S3 (Storage) Support file permissions according to public / protected / private. Upload directly from the client via Amplify. API Gateway Coordinate requests to Lambda.\nCORS is important for the frontend to work.\nCan require authentication using Cognito tokens.\nLambda + DynamoDB Lambda executes CRUD functions. DynamoDB stores document metadata. Frontend only communicates via API Gateway. "},{"uri":"https://taindAI19.github.io/aws-internship-report/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Objectives for the week Part 1 – Introducing Amazon Bedrock \u0026amp; Foundation Models\nUnderstand how Amazon Bedrock is designed and the main operating components. Learn how to access and use foundation models through Bedrock. Survey popular models such as Claude, Llama, Mistral, Amazon Titan… Understand the difference between text generation models, embedding models, and multimodal models. Part 2 – Building AI Agents with Bedrock Agent\nUnderstand the concept of AI Agents and how they work.\nUnderstand the components:\nAction groups\nKnowledge bases\nOrchestration\nIntegrate Lambda functions\nSet up an AI Agent that can call and use tools through Lambda.\nPart 3 – Using Knowledge Base for Enterprise Search\nCreate a Knowledge Base using vector embeddings.\nLink Bedrock KB to data stored in S3.\nTest semantic search capabilities and RAG (Retrieval Augmented Generation) mechanism.\nComplete KB integration into Bedrock Agent.\nPart 4 – Integrate Front-end with Bedrock Agent\nBuild a chat interface to interact directly with the Agent.\nImplement streaming data to display real-time responses.\nDescribe the processing flow: User input → Agent → (KB, Lambda) → Returned results.\nBring the front-end to the Amplify Hosting environment.\nDeployment tasks of the week Deployment Tasks of the Week Day Task Details Start Date Completion Date Resources 2 Understanding the Bedrock Platform\n• Explore the model list.\n• Try Claude / Llama 3 in Playground.\n• Compare costs and performance.\n• Understand the basic Bedrock API. 2025-11-24 2025-11-24 https://cloudjourney.awsstudygroup.com/ 3 Develop a simple Text Generation application\n• Write a Lambda that calls the InvokeModel API.\n• Test temperature and max tokens.\n• Create a small CLI using the SDK. 2025-11-25 2025-11-25 https://cloudjourney.awsstudygroup.com/ 4 Set up Bedrock Agent\n• Configure agent.\n• Create instructions and guardrails.\n• Create Action Group using Lambda.\n• Connect agent to sample DynamoDB. 2025-11-26 2025-11-26 https://cloudjourney.awsstudygroup.com/ 5 Build Knowledge Base (RAG) system\n• Create S3 bucket to store documents.\n• Set up embeddings (Titan Embeddings G1).\n• Test Q\u0026amp;A with PDF/CSV.\n• Integrate KB into Agent. 2025-11-27 2025-11-27 https://cloudjourney.awsstudygroup.com/ 6 Test the entire Agent\n• Test the chain of operations: Agent → KB → Lambda.\n• Fix IAM, KMS errors.\n• Test multiple conversations.\n• Add error handling and fallback. 2025-11-28 2025-11-28 https://cloudjourney.awsstudygroup.com/ 7 Develop Chat interface using Amplify\n• Create React Chat UI.\n• Connect to Agent via API Gateway.\n• Test streaming output.\n• Deploy using Amplify Hosting. 2025-11-29 2025-11-29 https://cloudjourney.awsstudygroup.com/ Results achieved in week 12 1. Knowledge of Amazon Bedrock \u0026amp; Foundation Models\nKnow how to call models via Bedrock SDK.\nPractice with different model groups.\nUnderstand application scenarios: text generation, summarization, code generation, embeddings.\n2. Complete a real-world AI Agent\nAgent recognizes user intent.\nUse Action Groups to trigger Lambda.\nSupport multi-step inference.\nQuery DynamoDB via Lambda.\n3. Build a Knowledge Base for RAG\nCreate a vector repository from data in S3. Perform semantic search successfully. Agent improves response quality when integrating KB. 4. Create a chat interface that works with Bedrock\nBuild a chat UI using React. Connect via API Gateway for security. Work well with streaming responses. Full flow: UI → Agent → KB/Lambda → Response. Summary of knowledge learned Amazon Bedrock\nProvides access to many powerful models. Fully managed and enterprise-grade security system. Supports document generation, embeddings, and multi-modal models. Bedrock Agents\nCapable of maintaining conversations, processing logic, and calling tools. Action Groups enable real-world backend connections. Guardrails help secure content control. Knowledge Bases\nEmbeddings enhance semantic search capabilities. Combine KB and Agent to form a complete RAG system. Supports automatic document updates from S3. Integration \u0026amp; Deployment\nLambda executes backend logic.\nAPI Gateway ensures secure connections for the front-end.\nAmplify supports rapid interface deployment.\n"},{"uri":"https://taindAI19.github.io/aws-internship-report/5-workshop/5.3-architecture/5.3.3-chunking/","title":"Chunking &amp; Embedding","tags":[],"description":"","content":" Why Chunking Is Needed Documents or FAQs are often long; to compute embeddings effectively and optimize similarity search, large text must be split into smaller segments (chunks).\nBenefits Reduces context loss during embedding More accurate retrieval with vector similarity Better performance for search Compatible with token limits of embedding models Chunking Strategy The code uses RecursiveCharacterTextSplitter:\nsplitter = RecursiveCharacterTextSplitter( chunk_size=500, chunk_overlap=0 ) chunks = splitter.split_documents(docs) Chunking Parameters Parameter Value Meaning chunk_size 500 Size of each chunk (characters) chunk_overlap 0 No overlap between chunks Optimization Tips Recommended chunk size: 500–1000 tokens (depends on embedding model)\nChunk overlap: Use 50–100 characters if continuous context is needed\nTrade-off:\nSmaller chunks → higher accuracy but more vectors Larger chunks → fewer vectors but may lose context Creating Embeddings and Vector Store Initialize Embedding Model emb = HuggingFaceEmbeddings( model_name=\u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; ) Selected model: all-MiniLM-L6-v2\nLightweight and fast Good for English Embedding dimension: 384 Create FAISS Vector Store faq_store = FAISS.from_documents(chunks, emb) FAISS (Facebook AI Similarity Search) provides:\nHigh-speed vector search Efficient handling of large datasets Multiple index algorithms Query the Vector Store results = faq_store.similarity_search(query, k=3) Parameters:\nquery: User question k=3: Returns top 3 most relevant chunks Important Notes Updating Data If documents change (add/update):\nRe-embed all data, or Apply incremental updates to the vector store Choosing an Embedding Model Trade-off comparison:\nCriteria Lightweight Model Heavy Model Speed Fast Slow Accuracy Good Very good Cost Low High Use cases FAQ, chatbot Research, legal Vietnamese Models For better Vietnamese performance:\nkeepitreal/vietnamese-sbert sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 Full Code Example from langchain_text_splitters import RecursiveCharacterTextSplitter from langchain_huggingface import HuggingFaceEmbeddings from langchain_community.vectorstores import FAISS # Load documents docs = load_faq_csv() # Chunking splitter = RecursiveCharacterTextSplitter( chunk_size=500, chunk_overlap=0 ) chunks = splitter.split_documents(docs) # Embedding emb = HuggingFaceEmbeddings( model_name=\u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; ) # Vector Store faq_store = FAISS.from_documents(chunks, emb) # Query query = \u0026#34;How do I change my password?\u0026#34; results = faq_store.similarity_search(query, k=3) Deployment Checklist Prepare documents (CSV, JSON, text files) Select appropriate chunk_size (test 500, 750, 1000) Choose embedding model (English or multilingual) Build and save FAISS index Test retrieval with sample queries Monitor and tune the k value for similarity search "},{"uri":"https://taindAI19.github.io/aws-internship-report/5-workshop/5.3-architecture/","title":"RAG Architecture Deployed on AWS AgentCore","tags":[],"description":"","content":" Using the Gateway Endpoint In this section, we will explore how to integrate Groq to call OpenAI-compatible models and how to perform data chunking for RAG.\n"},{"uri":"https://taindAI19.github.io/aws-internship-report/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - AWS named as a Leader in 2025 Gartner Magic Quadrant for Cloud-Native Application Platforms and Container Management This blog announces that AWS has been recognized by Gartner as a Leader in the 2025 Magic Quadrant for both Cloud-Native Application Platforms and Container Management. The post highlights AWS’s strong execution and comprehensive vision in helping enterprises build, deploy, and manage cloud-native applications. AWS offers a rich portfolio of services such as AWS Lambda, App Runner, Amazon ECS, Amazon EKS, and AWS Fargate, enabling customers to develop applications that are flexible, scalable, and efficient across cloud and hybrid environments. The blog also reaffirms AWS’s commitment to continued innovation—delivering modern AI/ML tools and container solutions that meet the diverse needs of customers worldwide.\nBlog 2 - Automate and orchestrate Amazon EMR jobs using AWS Step Functions and Amazon EventBridge This blog demonstrates how to build a fully automated and cost-efficient Apache Spark data processing pipeline on Amazon EMR using AWS Step Functions and Amazon EventBridge. The solution automatically provisions temporary EMR clusters on EC2, runs Spark jobs, stores results in Amazon S3, and terminates clusters upon completion—reducing operational costs and eliminating manual intervention. The post walks through an example using public COVID-19 data to calculate monthly hospital bed and ICU utilization metrics by state. It also provides detailed guidance on deploying the infrastructure with AWS CloudFormation, configuring EventBridge schedules, monitoring workflows via Step Functions, reviewing CloudWatch logs, and cleaning up resources after execution. This architecture is especially well-suited for periodic workloads such as ETL, batch analytics, and compliance reporting—where fine-grained infrastructure control, high security, and cost optimization are critical.\nBlog 3 - Streamline Spark application development on Amazon EMR with the Data Solutions Framework on AWS This blog introduces how to simplify the entire Apache Spark application development lifecycle on Amazon EMR using the AWS Cloud Development Kit (CDK), Data Solutions Framework (DSF), and Amazon EMR Toolkit for VS Code. You’ll learn how to set up a consistent local development environment aligned with production, deploy serverless Spark infrastructure as code (IaC), and build automated CI/CD pipelines for testing and multi-environment deployments. The post walks through packaging a PySpark application as a deployable artifact for EMR Serverless, integrating automated tests with Pytest, and using self-mutating CDK Pipelines to streamline updates. This solution empowers developers with full control over both code and infrastructure, reduces team dependencies, accelerates development cycles, and optimizes Spark workload performance on AWS.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"},{"uri":"https://taindAI19.github.io/aws-internship-report/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim for your report, including this warning.\nIn this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event’s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://taindAI19.github.io/aws-internship-report/5-workshop/5.4-agent-core-run/","title":"Run Agent Core","tags":[],"description":"","content":"In this section, you will learn how to deploy and invoke AWS Agent Core directly from your local machine. Why you should use AWS CLI Using the AWS CLI allows you to easily access, configure, and manage Agent Core from your own environment. It provides flexibility, convenience, and full control when working locally.\n"},{"uri":"https://taindAI19.github.io/aws-internship-report/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Building a RAG Agent with Groq API and AgentCore Memory Overview In this workshop, we will build a complete RAG (Retrieval-Augmented Generation) Agent with the following capabilities:\nCalling the Groq API to use high-performance LLM models Chunking \u0026amp; Embedding documents for optimized vector search AgentCore Memory to maintain long-term context across chat sessions Tool Integration so the agent can automatically search FAQs and reformulate queries AgentCore provides a framework for building AI agents with memory persistence, middleware hooks, and tool orchestration — enabling the agent to “remember” conversation history and personalize responses.\nContents Workshop Overview\nPrerequisites\nArchitecture\n5.3.1. Calling Groq API 5.3.2. Chunking \u0026amp; Embedding 5.3.3. AgentCore Code Handler Running AgentCore\nTech Stack Component Technology LLM Provider Groq API (OpenAI models) Embedding Model HuggingFace (all-MiniLM-L6-v2) Vector Store FAISS Agent Framework LangChain + AgentCore Memory Backend AgentCore Memory Store Text Splitting RecursiveCharacterTextSplitter Prerequisites Python 3.8+ Groq API Key Basic understanding of RAG and LLMs Familiarity with vector embeddings Expected Outcome By the end of the workshop, you will have:\nAn agent capable of answering FAQs via vector search A memory system that remembers user preferences and context Tool orchestration allowing the agent to decide when to use tools Production-ready code with logging and error handling Get Started: 5.1. Workshop Overview\n"},{"uri":"https://taindAI19.github.io/aws-internship-report/6-self-evaluation/","title":"Self-assessment","tags":[],"description":"","content":"During my internship at [AWS Vietnam] from [08/09/2025] to [09/12/2025], I had the opportunity to learn, practice and apply the knowledge I have acquired at school to the actual working environment.\nI participated in [APT Magic project], thereby improving my skills in [AWS, programming, system design, analysis, report writing, communication, teamwork \u0026hellip;].\nIn terms of style, I always try to complete my tasks well, comply with the rules, and actively discuss with colleagues to improve work efficiency.\nTo objectively reflect the internship process, I would like to self-assess myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge and skills Understanding of the industry, applying knowledge to practice, skills in using tools, quality of work ☐ ✅ ☐ 2 Learning ability Acquiring new knowledge, learning quickly ☐ ✅ ☐ 3 Proactive Self-study, take on tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Complete work on time, ensure quality ✅ ☐ ☐ 5 Discipline Comply with working hours, rules, and procedures ✅ ☐ ☐ 6 Progressiveness Willingness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Present ideas, report work clearly ✅ ☐ ☐ 8 Teamwork Work effectively with colleagues, participate in groups ✅ ☐ ☐ 9 Professional behavior Respect colleagues, partners, working environment ✅ ☐ ☐ 10 Problem-solving mindset Identify problems, propose solutions, be creative ☐ ✅ ☐ 11 Contribute to projects/organizations Work efficiency, innovation initiatives, recognition from the team ✅ ☐ ☐ 12 Overall Overall assessment of the entire internship process ✅ ☐ ☐ Needs improvement Improve professional knowledge and skills more Be proactive in learning from company members and be more creative in work Improve concentration at work, be more diligent and disciplined with yourself "},{"uri":"https://taindAI19.github.io/aws-internship-report/7-feedback/","title":"Sharing, contributing ideas","tags":[],"description":"","content":"Overall assessment 1. Working environment The working atmosphere at FCJ is friendly and easy to integrate. Everyone supports each other enthusiastically, even when I need help outside of working hours. The working space is clean and comfortable, creating conditions for good concentration. I think the company can organize more internal exchanges or group activities to increase connection between members.\n2. Support from mentor / admin team Mentor explains in detail, is patient and always encourages me to ask questions when I am not clear about the problem. The admin team quickly supports the necessary procedures and documents, helping me get into the rhythm of work smoothly. What I like most is that the mentor lets me experiment and solve problems myself before instructing.\n3. The level of suitability between work and major The assigned tasks are quite close to the knowledge I learned in school, and at the same time help me approach new content. Thanks to that, I not only consolidate my basic knowledge but also learn practical skills that have not been mentioned in the curriculum.\n4. Opportunities to learn and develop skills During the internship, I was trained in many skills such as team coordination, task management, using professional working tools and how to communicate in the office environment. The mentor also shared very useful practical experiences to orient my future career.\n5. Culture \u0026amp; Teamwork Spirit I feel the company culture is positive: respect each other, support each other wholeheartedly and work in a comfortable but still professional atmosphere. When there is an urgent project, everyone cooperates to complete the common goal, regardless of position or role. This makes me feel welcome even though I am just an intern.\n6. Internship policy/benefits The company has an internship allowance and is quite flexible in terms of time when interns need to adjust. In addition, being able to participate in internal training sessions is also a big plus.\nSome other questions During the internship, what made you most satisfied? I am most satisfied with the good support environment and the very dedicated mentor. What do you think the company can improve to better support interns? Assign work according to professional capacity so that students have the opportunity to handle it themselves, thereby learning many practical skills. Many missing documents need to be supplemented. If you have friends who are interested, would you recommend them to intern here? Why? Yes. Because of the friendly environment, enthusiastic mentor, work that suits the major and helps to learn quickly in practice. Suggestions \u0026amp; wishes Do you have any ideas to improve the internship experience? There should be a separate onboarding session for new interns to get acquainted with the process and tools faster. Do you want to continue participating in this program if you have the opportunity? Yes. Because the program brings many practical experiences and development opportunities. Additional suggestions (feel free to share): I appreciate the current working environment. If there are more periodic in-depth workshops, the experience will be even better. "},{"uri":"https://taindAI19.github.io/aws-internship-report/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://taindAI19.github.io/aws-internship-report/tags/","title":"Tags","tags":[],"description":"","content":""}]